"use strict";(self.webpackChunktgrall_blog=self.webpackChunktgrall_blog||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2025/10/21/modernizing-legacy-code-with-github-copilot-a-documentation-first-approach","metadata":{"permalink":"/blog/2025/10/21/modernizing-legacy-code-with-github-copilot-a-documentation-first-approach","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2025-10-21-modernizing-legacy-code-with-github-copilot-a-documentation-first-approach.md","source":"@site/blog/2025-10-21-modernizing-legacy-code-with-github-copilot-a-documentation-first-approach.md","title":"Modernizing Legacy Code with GitHub Copilot: A Documentation-First Approach","description":"Learn how to use GitHub Copilot to tackle legacy code modernization through a documentation-first approach. This guide walks you through setting up instruction files, custom chat modes, and reusable prompts to automatically generate comprehensive documentation from your existing codebase\u2014creating the perfect foundation for understanding, maintaining, or rewriting legacy applications in any programming language.","date":"2025-10-21T03:12:00.000Z","formattedDate":"October 21, 2025","tags":[{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"},{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"documentation","permalink":"/blog/tags/documentation"},{"label":"legacy","permalink":"/blog/tags/legacy"}],"readingTime":3.685,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Modernizing Legacy Code with GitHub Copilot: A Documentation-First Approach","description":"Learn how to use GitHub Copilot to tackle legacy code modernization through a documentation-first approach. This guide walks you through setting up instruction files, custom chat modes, and reusable prompts to automatically generate comprehensive documentation from your existing codebase\u2014creating the perfect foundation for understanding, maintaining, or rewriting legacy applications in any programming language.","tags":["coding with copilot","github","copilot","documentation","legacy"],"keywords":["coding with copilot","github","copilot","documentation","legacy","progress","openedge"],"image":"/images/posts/2025-10-21/header.png","date":"2025-10-21T03:12"},"nextItem":{"title":"Effortlessly Generate JSON Arrays from Raw Data with GitHub Copilot","permalink":"/blog/2024/03/03/github-copilot-from-raw-data-to-json-array"}},"content":"A common question I hear from developers and development teams is: \\"Can Copilot help me with my legacy code?\\" Whether it\'s understanding it, maintaining it, or rewriting and modernizing the application in a new stack, the answer is absolutely yes\u2014but you need to follow the right process.\\n\\nYou can find a video -in French- walkthrough of this approach here:\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/DYZNNsB-krc?si=eh_WlvAm1ErjF-KU\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n\\n## Don\'t Rush to Rewrite\\n\\nThe key mistake many make is jumping straight from legacy code (like Progress Software in my example) and asking Copilot to \\"write me a Java app\\" or \\"convert this to TypeScript.\\" Instead, the proper approach is to **start with documentation**. We need to begin by creating reverse documentation from the existing code, which then serves as a solid foundation for rewriting. This ensures all functionality is well-documented and provides the right context for generating new code.\\n\\nYou can see a complete example of this retro-documentation process applied to the [Sports App](https://github.com/tug-on-dev/progress-sports-app/tree/main/documentation). _(Originally in French then translated to English using Copilot)_\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Setting Up Your Context\\n\\nI\'m working with a VS Code project using a Progress Software example application\u2014specifically the Sports app, which includes a complete database and architecture. The first step is establishing context for Copilot.\\n\\n### 1. Create Instruction Files\\n\\nAsk GitHub Copilot to create [an instruction file for the agent](https://github.com/tug-on-dev/progress-sports-app/blob/main/.github/copilot-instructions.md). Copilot analyzes the code and workspace to identify:\\n- Architecture\\n- Key components  \\n- Critical development information (build, deploy, cloud deployment)\\n\\nYou can extend this file with your own rules and references. In my case, I added instructions for Copilot to respond in French and use technical terminology.\\n\\nTake a look to the [instructions](https://github.com/github/awesome-copilot/tree/main/instructions) folder in the [GitHub Awesome Copilot repository](https://github.com/github/awesome-copilot/).\\n\\n### 2. Configure Chat Modes\\n\\nCreate [custom chat modes](https://github.com/tug-on-dev/progress-sports-app/blob/main/.github/chatmodes/doc-writer.chatmode.md) to control Copilot\'s behavior. I created a \\"doc writer\\" mode\u2014essentially a system prompt that defines how the agent should behave when writing documentation. You can have multiple chat modes for different team members and tasks.\\n\\nTake a look to the [Chat Modes](https://github.com/github/awesome-copilot/tree/main/chatmodes) folder in the [GitHub Awesome Copilot repository](https://github.com/github/awesome-copilot/).\\n\\n\\n### 3. Build Reusable Prompts\\n\\nInstead of typing requests each time, [create a prompt file](https://github.com/tug-on-dev/progress-sports-app/blob/main/.github/prompts/retro-doc.prompt.md). I created a \\"[Retrodoc](https://github.com/tug-on-dev/progress-sports-app/blob/main/.github/prompts/retro-doc.prompt.md)\\" prompt that I can reuse across projects. This prompt instructs Copilot to:\\n- Inspect the entire codebase\\n- Generate documentation\\n- Create diagrams with Mermaid for UML and databases\\n- Extract architecture, functional requirements, and more\\n\\nTake a look to the [Prompts](https://github.com/github/awesome-copilot/tree/main/prompts) folder in the [GitHub Awesome Copilot repository](https://github.com/github/awesome-copilot/).\\n\\n**Important**: For large codebases, work iteratively by module or service rather than trying to document everything at once\u2014you won\'t be able to review hundreds of pages of documentation in one sitting anyway, so it\'s better to divide the work into manageable steps.\\n\\n## The Documentation Process\\n\\nWith these files prepared, I\'m ready to work. After initializing a Git repository to track changes, I start a new chat in doc writer mode and trigger my Retrodoc prompt.\\n\\nGitHub Copilot then:\\n- Reads through all the files (like any developer would)\\n- Creates directory structures (documentation, architecture, API, deployment, requirements)\\n- Generates comprehensive technical documentation\\n\\nThe process takes a few minutes, but you can watch the documentation being created in real-time. The output includes:\\n- High-level architecture overview (web layer, Nginx server, Progress WebSpeed middleware, replicated OpenEdge database)\\n- Data flow and user navigation\\n- Database structure and formats\\n- Deployment options\\n\\n## Refining the Output\\n\\nAs documentation is generated, review it and make corrections. For example, when I noticed display issues with Mermaid diagrams, I selected the code and asked Copilot to fix it. There\'s always a need for human oversight\u2014both functional and technical.\\n\\n**Tip**: If you see recurring issues, add instructions to your prompt or instruction file to prevent them in future runs.\\n\\n\\n## Summary: The Three-Step Approach\\n\\n1. **Instruction File**: Create context about your application to guide Copilot\\n2. **Chat Mode**: Define agent behavior for specific tasks (like documentation writing)\\n3. **Prompt File**: Build reusable, precise prompts for consistent results\\n\\nThis approach works with any programming language or framework\u2014whether cutting-edge or legacy systems like OpenEdge and Progress Software. The documentation you generate becomes your roadmap for understanding, maintaining, and eventually modernizing your legacy applications.\\n\\nI encourage you to try this approach with your own applications. Take the time upfront to configure your project properly, and you\'ll see significantly better results from GitHub Copilot."},{"id":"/2024/03/03/github-copilot-from-raw-data-to-json-array","metadata":{"permalink":"/blog/2024/03/03/github-copilot-from-raw-data-to-json-array","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2024-03-03-github-copilot-from-raw-data-to-json-array.md","source":"@site/blog/2024-03-03-github-copilot-from-raw-data-to-json-array.md","title":"Effortlessly Generate JSON Arrays from Raw Data with GitHub Copilot","description":"Discover how GitHub Copilot simplifies the creation of JSON arrays from raw data, streamlining the process of updating product catalogs","date":"2024-03-03T06:12:00.000Z","formattedDate":"March 3, 2024","tags":[{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"},{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"json","permalink":"/blog/tags/json"},{"label":"data","permalink":"/blog/tags/data"}],"readingTime":0.84,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Effortlessly Generate JSON Arrays from Raw Data with GitHub Copilot","description":"Discover how GitHub Copilot simplifies the creation of JSON arrays from raw data, streamlining the process of updating product catalogs","tags":["coding with copilot","github","copilot","json","data"],"keywords":["coding with copilot","github","copilot","json","data"],"image":"/images/posts/2024-03-03/header.png","date":"2024-03-03T06:12"},"prevItem":{"title":"Modernizing Legacy Code with GitHub Copilot: A Documentation-First Approach","permalink":"/blog/2025/10/21/modernizing-legacy-code-with-github-copilot-a-documentation-first-approach"},"nextItem":{"title":"Efficiently Format Data in My IDE with GitHub Copilot","permalink":"/blog/2024/02/16/github-copilot-formatting-data-in-my-ide"}},"content":"When developing my WindR application, I frequently encounter the task of adding new entries to the product catalog, a process that hasn\'t yet been automated. Consequently, I find myself creating JSON documents for the MongoDB database.\\n\\nProducts such as boards, sails, fins, and foils possess diverse specifications that vary depending on their type. Extracting these values from official product websites can be laborious, particularly when field names differ across brands, as illustrated by the example of two sails from different manufacturers:\\n\\n![Patrik](/images/posts/2024-03-03/patrik.png)\\n\\n![Duotone](/images/posts/2024-03-03/duotone.png)\\n\\nIn the accompanying video, I demonstrate how I leverage Copilot Chat to extract data from copied website content. By providing a simple prompt outlining the array-to-JSON attribute mapping, I swiftly convert the data:\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/WrfpGJz-2B4?si=vuMaeAGf-DuSCA7g\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nThis approach streamlines the addition of new products to my catalog. Looking ahead, I aim to automate this process using a combination of a small script and GenAI programming. However, that\'s a tale for another time."},{"id":"/2024/02/16/github-copilot-formatting-data-in-my-ide","metadata":{"permalink":"/blog/2024/02/16/github-copilot-formatting-data-in-my-ide","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2024-02-16-github-copilot-formatting-data-in-my-ide.md","source":"@site/blog/2024-02-16-github-copilot-formatting-data-in-my-ide.md","title":"Efficiently Format Data in My IDE with GitHub Copilot","description":"Discover how GitHub Copilot streamlines the formatting of data extracted from GitHub Issues","date":"2024-02-16T06:12:00.000Z","formattedDate":"February 16, 2024","tags":[{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"},{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"issue","permalink":"/blog/tags/issue"}],"readingTime":0.585,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Efficiently Format Data in My IDE with GitHub Copilot","description":"Discover how GitHub Copilot streamlines the formatting of data extracted from GitHub Issues","tags":["coding with copilot","github","copilot","issue"],"keywords":["coding with copilot","github","copilot","issue"],"image":"/images/posts/2024-02-16/header.png","date":"2024-02-16T06:12"},"prevItem":{"title":"Effortlessly Generate JSON Arrays from Raw Data with GitHub Copilot","permalink":"/blog/2024/03/03/github-copilot-from-raw-data-to-json-array"},"nextItem":{"title":"Creating a CSV from SQL statements with GitHub Copilot","permalink":"/blog/2024/02/10/github-copilot-create-csv-from-sql"}},"content":"In this brief demonstration, I showcase how GitHub Copilot help me to format data sourced from a web page and pasted into my IDE. By simply copying information from a webpage and pasting it into my coding environment, I initiate the process.\\n\\nThen, with a straightforward inquiry directed to Copilot Inline Chat, I effortlessly extract specific details \u2014such as GitHub Handles\u2014 from the text and neatly organize them into a CSV format:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/xAC6_gjQOeA?si=Ej10BtTeUjwjeu-y\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nAlthough this isn\'t a task I tackle daily, especially with GitHub Handles, I wanted to illustrate another practical application of Copilot that can significantly assist in your coding endeavors."},{"id":"/2024/02/10/github-copilot-create-csv-from-sql","metadata":{"permalink":"/blog/2024/02/10/github-copilot-create-csv-from-sql","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2024-02-10-github-copilot-create-csv-from-sql.md","source":"@site/blog/2024-02-10-github-copilot-create-csv-from-sql.md","title":"Creating a CSV from SQL statements with GitHub Copilot","description":"GitHub Copilot assists in creating a CSV from an SQL statements.","date":"2024-02-10T06:12:00.000Z","formattedDate":"February 10, 2024","tags":[{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"},{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"python","permalink":"/blog/tags/python"},{"label":"images","permalink":"/blog/tags/images"}],"readingTime":0.65,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Creating a CSV from SQL statements with GitHub Copilot","description":"GitHub Copilot assists in creating a CSV from an SQL statements.","tags":["coding with copilot","github","copilot","python","images"],"keywords":["coding with copilot","github","copilot","sql","csv"],"image":"/images/posts/2024-02-10/header.png","date":"2024-02-10T06:12"},"prevItem":{"title":"Efficiently Format Data in My IDE with GitHub Copilot","permalink":"/blog/2024/02/16/github-copilot-formatting-data-in-my-ide"},"nextItem":{"title":"Rapid Prototyping with GitHub Copilot: A Quick Journey into Image Processing","permalink":"/blog/2024/01/13/github-copilot-implementing-an-ide-in-mn"}},"content":"During one of my test I had to create some CSV files, and needed some data. I could have used Copilot directly to generate random data, but I wanted to test something else. \\n\\nIn my SpringBoot PetClinic application, I have a SQL file that contains some data, and I wanted to use this data to create a CSV file, directly from the insert statements without any code.\\n\\nThe Copilot then effortlessly executed the specified tasks, as demonstrated in the accompanying video:\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube-nocookie.com/embed/_YFS7Wbzurc?si=Hy7sKxxPNpzIjS4b\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" fullscreen></iframe>\\n\\nNote, I could have also use Copilot to do it directly from the database, but just wanted to see if Copilot could extract the data from the SQL file, and it did it!.."},{"id":"/2024/01/13/github-copilot-implementing-an-ide-in-mn","metadata":{"permalink":"/blog/2024/01/13/github-copilot-implementing-an-ide-in-mn","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2024-01-13-github-copilot-implementing-an-ide-in-mn.md","source":"@site/blog/2024-01-13-github-copilot-implementing-an-ide-in-mn.md","title":"Rapid Prototyping with GitHub Copilot: A Quick Journey into Image Processing","description":"GitHub Copilot expedites the creation of an image processing prototype for WindR.org\'s transition to Next.js, seamlessly converting a described script into functional code within minutes, showcasing its transformative potential for rapid prototyping.","date":"2024-01-13T06:12:00.000Z","formattedDate":"January 13, 2024","tags":[{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"},{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"python","permalink":"/blog/tags/python"},{"label":"images","permalink":"/blog/tags/images"}],"readingTime":0.93,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Rapid Prototyping with GitHub Copilot: A Quick Journey into Image Processing","description":"GitHub Copilot expedites the creation of an image processing prototype for WindR.org\'s transition to Next.js, seamlessly converting a described script into functional code within minutes, showcasing its transformative potential for rapid prototyping.","tags":["coding with copilot","github","copilot","python","images"],"keywords":["coding with copilot","github","copilot","python","images"],"image":"/images/posts/2024-01-13/header.png","date":"2024-01-13T06:12"},"prevItem":{"title":"Creating a CSV from SQL statements with GitHub Copilot","permalink":"/blog/2024/02/10/github-copilot-create-csv-from-sql"},"nextItem":{"title":"TDD/Test Failures: Resolving Issues with GitHub Copilot","permalink":"/blog/2023/12/29/github-copilot-fixing-test-failure"}},"content":"As I transition WindR.org to Next.js, the necessity arises to clean up the product catalog\'s images, which come in various sizes and formats; hence, I opted to write a small Python script using Pillow for resizing and standardizing the images. Despite not being a Python developer, I\'m equipped with basic knowledge, making it easy to read and utilize the language\'s numerous libraries.\\n\\nUsing Visual Studio Code, I initiated a new script to fulfill the image cleanup requirements, specifying the objective: \\n- _download an image from a given URL, save it locally as \'original.png,\' resize it into a square with a transparent background, and save copies at 1000x1000 pixels and 256x256 pixels in greyscale_.\\n\\nThe Copilot then effortlessly executed the specified tasks, as demonstrated in the accompanying video:\\n\\nWatch the video [here](https://www.youtube.com/embed/doFGEFdfp24?si=gX7J35CALGfSNJTi)\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/doFGEFdfp24?si=gX7J35CALGfSNJTi\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWhile this script serves as a fundamental prototype, the next step involves incorporating this logic into a complete application. GitHub Copilot\'s assistance not only jumpstarted the development process but also facilitated the creation of a functional prototype within minutes."},{"id":"/2023/12/29/github-copilot-fixing-test-failure","metadata":{"permalink":"/blog/2023/12/29/github-copilot-fixing-test-failure","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-29-github-copilot-fixing-test-failure.md","source":"@site/blog/2023-12-29-github-copilot-fixing-test-failure.md","title":"TDD/Test Failures: Resolving Issues with GitHub Copilot","description":"See how Copilot has helped me implement business logic to fix a test failure","date":"2023-12-29T06:12:00.000Z","formattedDate":"December 29, 2023","tags":[{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"},{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"rest","permalink":"/blog/tags/rest"},{"label":"quarkus","permalink":"/blog/tags/quarkus"},{"label":"panache","permalink":"/blog/tags/panache"},{"label":"java","permalink":"/blog/tags/java"},{"label":"tdd","permalink":"/blog/tags/tdd"},{"label":"test","permalink":"/blog/tags/test"}],"readingTime":2,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"TDD/Test Failures: Resolving Issues with GitHub Copilot","description":"See how Copilot has helped me implement business logic to fix a test failure","tags":["coding with copilot","github","copilot","rest","quarkus","panache","java","tdd","test"],"keywords":["coding with copilot","github","copilot","tdd","test","testing","quarkus"],"image":"/images/posts/2023-12-29/header.png","date":"2023-12-29T06:12"},"prevItem":{"title":"Rapid Prototyping with GitHub Copilot: A Quick Journey into Image Processing","permalink":"/blog/2024/01/13/github-copilot-implementing-an-ide-in-mn"},"nextItem":{"title":"Resize multiple Next.js components with GitHub Copilot","permalink":"/blog/2023/12/28/github-copilot-resize-nextjs-components"}},"content":"In my Quarkus application, I encountered a hiccup with the Panache ORM while implementing and testing a REST service. Specifically, I faced a test failure related to the automatic update of a specifications list after deleting an element from the JSON/Entity. In this blog post, I\'ll walk you through the issue and demonstrate how GitHub Copilot came to the rescue, streamlining the implementation of a solution.\\n\\n**The Challenge:**\\n\\nThe JSON schema for my REST service looked like this:\\n\\n```json\\n{\\n    \\"id\\": 1,\\n    \\"name\\": \\"Fanatic Falcon\\",\\n    \\"description\\": \\"Slalom board\\",\\n    \\"specifications\\" : [\\n        {\\"id\\": 10, \\"name\\": \\"Falcom 100\\", \\"volume\\": 100},\\n        {\\"id\\": 11, \\"name\\": \\"Falcom 110\\", \\"volume\\": 110},\\n        {\\"id\\": 12, \\"name\\": \\"Falcom 120\\", \\"volume\\": 120} \\n    ]\\n    ```\\n}\\n```\\n\\nI had a test verifying the number of specifications after the deletion of one of them. However, the Panache ORM didn\'t automatically update the specifications list after deletion, leading to a failing test.\\n\\n\\n**The Solution:**\\n\\nTo address this issue, I needed to implement business logic to delete specifications in the database that were not present in the JSON payload. I documented the logic in a comment and collaborated with GitHub Copilot to generate the code.\\n\\nHere\'s a snippet of the code:\\n\\n```java\\n...\\n\\n// if the number of specifications in the existing board is different than the number of specifications in the updated board\\n// it means that some specifications have been removed, so we need to delete them\\n// for this we meed to loop on existing specifications and see if they are in the updated board\\n// if they are not add them to a list of specs to delete\\n// then use removeAll on existing board specifications\\nList<BoardSpecification> specsToDelete = new ArrayList<>();\\nfor (BoardSpecification existingSpec : existingBoard.specifications) {\\n    boolean found = false;\\n    for (BoardSpecification spec : board.specifications) {\\n        if (existingSpec.id.equals(spec.id)) {\\n            found = true;\\n        }\\n    }\\n    if (!found) {\\n        specsToDelete.add(existingSpec);\\n    }\\n}\\nexistingBoard.specifications.removeAll(specsToDelete);\\nexistingBoard.persist();\\n\\n...\\n```\\n\\nCheck out the video below to witness how I leveraged GitHub Copilot to easily implement the business logic.\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/KXD_Kv6mvz0?si=1wRTLLzF2IUbTnSB\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nOnce again, GitHub Copilot has proven to be an invaluable coding companion, significantly enhancing my efficiency and helping me overcome challenges in my coding journey. With its assistance, I navigated through the intricacies of the Panache ORM and successfully resolved the test failure, ensuring the seamless functionality of my Quarkus application."},{"id":"/2023/12/28/github-copilot-resize-nextjs-components","metadata":{"permalink":"/blog/2023/12/28/github-copilot-resize-nextjs-components","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-28-github-copilot-resize-nextjs-components.md","source":"@site/blog/2023-12-28-github-copilot-resize-nextjs-components.md","title":"Resize multiple Next.js components with GitHub Copilot","description":"See how Copilot has helped me resize multiple Next.js components in a matter of seconds","date":"2023-12-28T07:12:00.000Z","formattedDate":"December 28, 2023","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"typescript","permalink":"/blog/tags/typescript"},{"label":"next","permalink":"/blog/tags/next"},{"label":"react","permalink":"/blog/tags/react"},{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"}],"readingTime":0.5,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Resize multiple Next.js components with GitHub Copilot","description":"See how Copilot has helped me resize multiple Next.js components in a matter of seconds","tags":["github","copilot","javascript","typescript","next","react","coding with copilot"],"keywords":["github","copilot","javascript","typescript"],"image":"/images/posts/2023-12-28/header.png","date":"2023-12-28T07:12"},"prevItem":{"title":"TDD/Test Failures: Resolving Issues with GitHub Copilot","permalink":"/blog/2023/12/29/github-copilot-fixing-test-failure"},"nextItem":{"title":"Seamless React State Updates: Mastering Lists with GitHub Copilot","permalink":"/blog/2023/12/27/github-copilot-update-react-state"}},"content":"Today, I tackled the creation of a new NextJS form for my application, opting for a sleek horizontal arrangement of components. Striving for a 100% width within the parent container proved trickier than expected. Copilot came to the rescue, aiding with the necessary math to resize the components accurately.\\n\\nCatch a glimpse of the journey in this brief video:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/imt6Ttn8Njw?si=naKXRVzUfJfAhp3C\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nYet again, GitHub Copilot proves to be the hero of my coding journey \u2013 making my day, one efficient line of code at a time!"},{"id":"/2023/12/27/github-copilot-update-react-state","metadata":{"permalink":"/blog/2023/12/27/github-copilot-update-react-state","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-27-github-copilot-update-react-state.md","source":"@site/blog/2023-12-27-github-copilot-update-react-state.md","title":"Seamless React State Updates: Mastering Lists with GitHub Copilot","description":"See how Copilot has helped me to extract a value from a list of objects and update the state of a React component.","date":"2023-12-27T10:12:00.000Z","formattedDate":"December 27, 2023","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"typescript","permalink":"/blog/tags/typescript"},{"label":"next","permalink":"/blog/tags/next"},{"label":"react","permalink":"/blog/tags/react"},{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"}],"readingTime":0.705,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Seamless React State Updates: Mastering Lists with GitHub Copilot","description":"See how Copilot has helped me to extract a value from a list of objects and update the state of a React component.","tags":["github","copilot","javascript","typescript","next","react","coding with copilot"],"keywords":["github","copilot","javascript","typescript"],"image":"/images/posts/2023-12-27/header.png","date":"2023-12-27T10:12"},"prevItem":{"title":"Resize multiple Next.js components with GitHub Copilot","permalink":"/blog/2023/12/28/github-copilot-resize-nextjs-components"},"nextItem":{"title":"Quarkus: Simplifying Cloud File Uploads","permalink":"/blog/2023/12/24/quarkus-uploading-image-to-the-cloud"}},"content":"In my Next.js project, I encountered a challenge while working on a form that involved managing a list of values. Being a beginner, I found myself unsure about the logic required to extract the selected value from the list and update the state of the component.\\n\\nFortunately, as I delved into coding, GitHub Copilot came to my aid with a suggested code snippet:\\n\\n```javascript\\nif (field === \\"brand.id\\") {\\n    let brand = brands.find(brand => brand.id === value);\\n    setBoard({\\n        ...board,\\n        brand: brand\\n    });\\n    return;\\n}\\n```\\n\\nExperience the efficiency and precision of Copilot in action by watching the video demonstration below:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/mctuEXSbQsg?si=hv953HevedK3aQWY\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nYet again, GitHub Copilot proves to be the hero of my coding journey \u2013 making my day, one efficient line of code at a time!"},{"id":"/2023/12/24/quarkus-uploading-image-to-the-cloud","metadata":{"permalink":"/blog/2023/12/24/quarkus-uploading-image-to-the-cloud","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-24-quarkus-uploading-image-to-the-cloud.md","source":"@site/blog/2023-12-24-quarkus-uploading-image-to-the-cloud.md","title":"Quarkus: Simplifying Cloud File Uploads","description":"Learn how to create a REST service to upload image to the Cloud.","date":"2023-12-24T10:00:00.000Z","formattedDate":"December 24, 2023","tags":[{"label":"quarkus","permalink":"/blog/tags/quarkus"},{"label":"panache","permalink":"/blog/tags/panache"},{"label":"rest","permalink":"/blog/tags/rest"},{"label":"google","permalink":"/blog/tags/google"},{"label":"aws","permalink":"/blog/tags/aws"},{"label":"azure","permalink":"/blog/tags/azure"},{"label":"cloud","permalink":"/blog/tags/cloud"},{"label":"s3","permalink":"/blog/tags/s-3"},{"label":"learning quarkus","permalink":"/blog/tags/learning-quarkus"}],"readingTime":6.26,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Quarkus: Simplifying Cloud File Uploads","description":"Learn how to create a REST service to upload image to the Cloud.","tags":["quarkus","panache","rest","google","aws","azure","cloud","s3","learning quarkus"],"keywords":["quarkus","panache","rest","google","aws","azure","cloud","s3"],"image":"/images/posts/2023-12-24-quarkus-uploading-image-to-the-cloud/header.png","date":"2023-12-24T10:00:00.000Z"},"prevItem":{"title":"Seamless React State Updates: Mastering Lists with GitHub Copilot","permalink":"/blog/2023/12/27/github-copilot-update-react-state"},"nextItem":{"title":"Java: Using MessageFormat to Generate JSON","permalink":"/blog/2023/12/21/java-generating-json-with-messageformat"}},"content":"![Quarkus: Simplifying Cloud File Uploads](/images/posts/2023-12-24-quarkus-uploading-image-to-the-cloud/header.png)\\n\\n\\nIn many projects, facilitating user uploads to cloud services is a common requirement. In my current project, I find myself inviting users to seamlessly upload various files, such as profile pictures, GPS track files, or session photos, to the WindR.org site. To enhance my understanding, I\'ve opted to employ multiple cloud providers \u2014[Microsoft Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Amazon S3](https://aws.amazon.com/pm/serv-s3/), [Google Cloud Storage](https://cloud.google.com/storage)\u2014 allowing me to test and compare their functionalities.\\n\\nThis article guides you through the process of:\\n\\n1. Creating a REST service for file uploads using RESTEasy Reactive in Quarkus.\\n2. Uploading files to the cloud from a Quarkus application.\\n\\nTo complement this discussion, a complete code example is available on GitHub. This resource serves as a practical reference for learning and experimentation.\\n\\n[GitHub Repository: Quarkus: Uploading Image to the Cloud](https://github.com/tgrall/learning-quarkus/tree/main/upload-image)\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n## The REST Service\\n\\nFor this example, let\'s create a service focused on uploading windsurfing board pictures. The corresponding endpoint URL will be `/api/v1/boards/picture`.\\n\\nThe REST endpoint will accept a `multipart/form-data` request, returning the URL of the uploaded file. The request comprises two parts:\\n\\nBoard metadata (JSON): Includes id, brand, year, and slug (used as the filename).\\nBoard picture (binary)\\n\\n\\n#### The Board Metadata class\\n\\nThe board metadata is a simple class with the following attributes:\\n\\n```java\\n    public static class BoardMetadata {\\n        long id;\\n        String brand;\\n        int year;\\n        String slug;\\n    }\\n```\\n\\n#### The POST Endpoint\\n\\n\\nThe `upload()` method, annotated with `@POST` and` @Path(\\"/picture\\")`, defines the HTTP method and URL. It is also annotated with `@Consumes(MediaType.MULTIPART_FORM_DATA)` to specify the content type of the request.\\n\\n\\n```java\\n@POST\\n@Path(\\"/picture\\")\\n@Consumes(MediaType.MULTIPART_FORM_DATA)\\n@Produces(MediaType.APPLICATION_JSON)\\npublic Response upload(\\n        @RestForm @PartType(MediaType.APPLICATION_JSON) BoardMetadata board,\\n        @RestForm(\\"picture\\") FileUpload picture\\n) {\\n    String json = MessageFormat.format(\\"\'{\'\\\\\\"slug\\\\\\":\\\\\\"{0}\\\\\\",\\\\\\"name\\\\\\":\\\\\\"{1}\\\\\\",\\\\\\"size\\\\\\":{2,number,#}\'}\'\\",\\n            board.slug, picture.fileName(), picture.size());\\n    return Response.ok(json).status(CREATED).build();\\n}\\n```\\n\\nThe `upload()` method takes two parameters:\\n\\n- `board`: Board metadata, a JSON object extracted from the request body.\\n- `picture`: Board picture, a binary object extracted from the request body.\\n\\nThe `@RestForm` annotation is used to define the parameter as a [Form Parameter](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST).\\n\\n>**Tip**: do not forget to a JSON provider to your project, in my case I am using Jackson, with the following extension:\\n>```\\n>./mvnw quarkus:add-extension -Dextensions=\'quarkus-resteasy-reactive-jackson\'\\n>```\\n\\n\\n## Uploading to Cloud Storage\\n\\nQuarkus simplifies integration with various cloud providers. For this example, we\'ll use the following extensions:\\n\\n- [Azure Storage](https://quarkus.io/extensions/io.quarkiverse.azureservices/quarkus-azure-storage-blob/)\\n- [Google Cloud Storage](https://quarkus.io/extensions/io.quarkiverse.googlecloudservices/quarkus-google-cloud-storage/)\\n- [Amazon S3](https://quarkus.io/extensions/io.quarkiverse.amazonservices/quarkus-amazon-s3/)\\n\\nThe cloud provider is selected through a new parameter in the endpoint path: `/picture/{cloud : (azure|aws|gcp)?}`.\\n\\n```java\\n@POST\\n@Path(\\"/picture/{cloud : (azure|aws|gcp)?}\\")\\n@Consumes(MediaType.MULTIPART_FORM_DATA)\\n@Produces(MediaType.APPLICATION_JSON)\\npublic Response upload(@RestForm @PartType(MediaType.APPLICATION_JSON) BoardMetadata board,\\n        @RestForm(\\"picture\\") FileUpload picture, @PathParam(\\"cloud\\") String cloud) {\\n...\\n}\\n```\\n\\nFor each cloud provider, a specific method is created (e.g., `uploadToAzure()`, `uploadToAWS()`, `uploadToGCP()`), abstracting the details of configuration. \\n\\nEach of these methods will use:\\n- the bucket define in the `application.properties` file\\n- upload the image using the SDK, configuring the content type and public access\\n- return the URL of the uploaded file.\\n\\nIn the project you can find [Terraform scripts](https://github.com/tgrall/learning-quarkus/tree/upload-file/upload-image/terraform) to create the buckets in each cloud provider.\\n\\n### Azure Storage Implementation\\n\\n\\n```java\\n...\\n@Inject\\ncom.azure.storage.blob.BlobServiceClient azureBlobServiceClient;\\n...\\n\\nprivate String uploadToAzure(FileUpload picture, BoardMetadata board) {\\n    String container = \\"catalog\\";\\n    String blobName = getBlobName(picture.fileName(), board);\\n    try {\\n        Map<String, String> metadata = Collections.singletonMap(\\"metadata\\", \\"value\\");\\n        BlobContainerCreateOptions options = new BlobContainerCreateOptions()\\n            .setMetadata(metadata)\\n            .setPublicAccessType(PublicAccessType.BLOB);\\n        BlobContainerClient blobContainerClient = azureBlobServiceClient\\n                .createBlobContainerIfNotExistsWithResponse(container, options, Context.NONE).getValue();\\n        BlobHttpHeaders headers = new BlobHttpHeaders();\\n        headers.setContentType(picture.contentType());\\n        BlobClient blobClient = blobContainerClient.getBlobClient(blobName);\\n        blobClient.uploadFromFile(picture.uploadedFile().toAbsolutePath().toString(), true);\\n        blobClient.setHttpHeaders(headers);\\n        return blobClient.getBlobUrl();\\n    } catch (BlobStorageException e) {\\n        Log.error(\\"An error occurred while uploading the file to Google Blob Storage: \\" + e.getMessage());\\n        throw new RuntimeException(\\"An error occurred while uploading the file to Azure Blob Storage: \\"\\n                + e.getErrorCode() + \\" - \\" + e.getStatusCode());\\n    }\\n}\\n```\\n\\nThe `BlobServiceClient` is injected using `@Inject`. The method uses this client to create a new container if needed and then uploads the file. In case of an error, a `RuntimeException` is thrown, and the REST endpoint handles the HTTP status code from the Azure SDK.\\n\\n> Note: if you do not put any connection information in the `application.properties` file, the application will connect to the Azure Storage Data Service, running on your local machine. This is very useful for development and testing.\\n\\n### Google Cloud Storage Implementation\\n\\n```java\\n@Inject\\ncom.google.cloud.storage.Storage googleStorage; \\n...\\n\\nprivate String uploadToGCP(FileUpload picture, BoardMetadata board) {\\n    String extension = getExtensionByStringHandling(picture.fileName()).orElse(\\"jpg\\");\\n    String blobName = \\"catalog/\\"+ board.brand + \\"/\\" + board.year + \\"/\\" + board.slug + \\".\\" + extension;\\n\\n    try {\\n        BlobInfo blobInfo = BlobInfo\\n            .newBuilder(BUCKET_NAME, blobName)\\n            .setContentType(picture.contentType())\\n            .setAcl(Collections.singletonList(Acl.of(Acl.User.ofAllUsers(), Acl.Role.READER)))\\n            .build();\\n        Blob blob = googleStorage.createFrom(blobInfo, picture.uploadedFile().toAbsolutePath());\\n        String url = \\"https://storage.googleapis.com/\\" + BUCKET_NAME + \\"/\\" + blobInfo.getBlobId().getName();\\n        return url;\\n    } catch (IOException e) {\\n        throw new RuntimeException(e);\\n    }\\n}\\n```\\n\\nGoogle Cloud Storage integration is similar. A `Storage` object is injected, and the file is uploaded after creating a `BlobInfo` object with the necessary metadata.\\n\\n> Note: Google storage does not have a data service running locally, so you need to have a valid GCP project to run the application.\\n\\n### Amazon S3 Implementation\\n\\n```java\\n@Inject\\nsoftware.amazon.awssdk.services.s3.S3Client s3Client;\\n...\\nprivate String uploadToAWS(FileUpload picture, BoardMetadata board) {\\n    Log.info(\\"Uploading to AWS: \\" + board.slug +\\" - in bucket \\" + BUCKET_NAME);\\n    String blobName = \\"catalog/\\"+ getBlobName(picture.fileName(), board);\\n\\n    try {\\n        PutObjectRequest putRequest = PutObjectRequest.builder()\\n            .bucket(BUCKET_NAME)\\n            .key(blobName)\\n            .contentType(picture.contentType())\\n            .acl(ObjectCannedACL.PUBLIC_READ)\\n            .build();\\n        PutObjectResponse putResponse =  s3Client.putObject(\\n            putRequest,\\n            picture.uploadedFile().toAbsolutePath());\\n        return s3Client.utilities().getUrl(builder -> builder.bucket(BUCKET_NAME).key(blobName)).toString();\\n    } catch (S3Exception e) {\\n        Log.error(\\"An error occurred while uploading the file to AWS Blob Storage: \\" + e.getMessage());\\n        throw new RuntimeException(\\"An error occurred while uploading the file to AWS Blob Storage: \\"\\n                + e.getMessage());\\n    }\\n}\\n```\\n\\nThe Amazon S3 integration is similar to the Azure Storage integration. The `S3Client` is injected, and the file is uploaded after creating a `PutObjectRequest` object with the necessary metadata.\\n\\nThe Quarkus S3 extension is using the AWS SDK for Java V2, and the `url-connection-client` is needed to make it work. You need to add the following dependency in the `pom.xml` file:\\n\\n```xml\\n    <dependency>\\n        <groupId>software.amazon.awssdk</groupId>\\n        <artifactId>url-connection-client</artifactId>\\n    </dependency>\\n```\\n\\n> Note: The Quarkus S3 extension uses the local data service based on the `localstack/localstack` container image, and you have to configure the `application.properties` with the name of the bucket, for example:\\n> ```properties\\n> bucket.name=quarkuscloudstorage\\n> quarkus.s3.devservices.buckets=${bucket.name}\\n> ```\\n>\\n>If you want to disable the use of the local data service, you can use the following configuration:\\n>\\n>```properties\\n>quarkus.s3.devservices.enabled=false\\n>```\\n\\n\\n\\n## Testing the application\\n\\n### Using the Web UI\\n\\nOnce the Quarkus application is running, navigate to [http://localhost:8080](http://localhost:8080) in your browser. Enter values, select a cloud storage service, and upload a picture.\\n\\n### Using `curl`\\n\\nYou can use `curl` commands to upload a picture to different cloud storage providers. Here are examples for AWS S3, Azure Blob Storage, and Google Cloud Storage.\\n\\n**AWS S3**\\n\\n```bash\\ncurl -i -X POST http://localhost:8080/api/v1/boards/picture/aws \\\\\\n  -H \'Content-Type: multipart/form-data\' \\\\\\n  -F \'picture=@./src/test/resources/test-board.png\' \\\\\\n  -F \'board={\\"id\\":5 , \\"year\\":2024, \\"brand\\":\\"jp-australia\\", \\"slug\\":\\"jp-australia-2024-ultimate-wave\\"}\'\\n```\\n\\n**Azure Blob Storage**\\n\\n```bash\\ncurl -i -X POST http://localhost:8080/api/v1/boards/picture/azure \\\\\\n  -H \'Content-Type: multipart/form-data\' \\\\\\n  -F \'picture=@./src/test/resources/test-board.png\' \\\\\\n  -F \'board={\\"id\\":5 , \\"year\\":2024, \\"brand\\":\\"jp-australia\\", \\"slug\\":\\"jp-australia-2024-ultimate-wave\\"}\'\\n```\\n\\n**Google Cloud Storage**\\n\\n```bash\\ncurl -i -X POST http://localhost:8080/api/v1/boards/picture/gcp \\\\\\n  -H \'Content-Type: multipart/form-data\' \\\\\\n  -F \'picture=@./src/test/resources/test-board.png\' \\\\\\n  -F \'board={\\"id\\":5 , \\"year\\":2024, \\"brand\\":\\"jp-australia\\", \\"slug\\":\\"jp-australia-2024-ultimate-wave\\"}\'\\n```\\n\\n## Conclusion\\n\\nIn this post, we explored the seamless integration of Quarkus Cloud Storage extensions for uploading files to Azure, Google, and AWS. The code remains consistent across the three providers, with differences limited to configuration nuances. Although deployment specifics are not covered here, configuring credentials for cloud provider access is necessary during deployment.\\n\\nFeel free to check out [the complete code on GitHub](https://github.com/tgrall/learning-quarkus/tree/main/upload-image), experiment with the examples, and stay tuned for more Quarkus insights in future posts!"},{"id":"/2023/12/21/java-generating-json-with-messageformat","metadata":{"permalink":"/blog/2023/12/21/java-generating-json-with-messageformat","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-21-java-generating-json-with-messageformat.md","source":"@site/blog/2023-12-21-java-generating-json-with-messageformat.md","title":"Java: Using MessageFormat to Generate JSON","description":"How to use MessageFormat to generate JSON","date":"2023-12-21T12:34:56.000Z","formattedDate":"December 21, 2023","tags":[{"label":"java","permalink":"/blog/tags/java"}],"readingTime":1.275,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Java: Using MessageFormat to Generate JSON","description":"How to use MessageFormat to generate JSON","tags":["java"],"keywords":["java"],"image":"https://tgrall.github.io/images/posts/2023-12-21-java-generating-json-with-messageformat/header.png","date":"2023-12-21T12:34:56.000Z"},"prevItem":{"title":"Quarkus: Simplifying Cloud File Uploads","permalink":"/blog/2023/12/24/quarkus-uploading-image-to-the-cloud"},"nextItem":{"title":"Creating a Typescript Class from a JSON file","permalink":"/blog/2023/12/18/github-copilot-typescript-class-from-json"}},"content":"![Java: Using MessageFormat to Generate JSON](/images/posts/2023-12-21-java-generating-json-with-messageformat/header.png)\\n\\nAs developers, we often encounter situations where we need to generate a JSON string for debugging purposes, especially when dealing with REST services. While frameworks like Spring Boot or Quarkus typically handle this task seamlessly, there are instances where manual intervention is required.\\n\\nIn a recent scenario, I found myself faced with this challenge. Traditionally, I had relied on string concatenation for such tasks. However, eager to explore more efficient alternatives, I turned to Java\'s java.text.MessageFormat to simplify the process.\\n\\n\x3c!-- truncate --\x3e\\n\\n### The Problem\\n\\nOne specific hurdle I encountered during this exploration was the need to escape the curly braces (`{` and `}`) within the JSON string. Despite the simplicity of the task, it temporarily stumped me. However, after some experimentation, I discovered a straightforward solution: using single quotes (`\'{\'`, `\'}\'`) around the problematic curly braces.\\n\\n### The Solution\\n\\nLet\'s delve into the code that resolved this issue:\\n\\n```java\\nimport java.text.MessageFormat;\\n\\npublic class Main {\\n\\n    public static void main(String[] args) {\\n        String json = MessageFormat.format(\\"\'{\'\\\\\\"name\\\\\\":\\\\\\"{0}\\\\\\",\\\\\\"age\\\\\\":{1}\'}\'\\", \\"Thomas\\", 50);\\n        System.out.println(json);\\n    }\\n}\\n```\\n\\nThe Result:\\n\\nExecuting this code yields the desired JSON output:\\n\\n```json\\n{\\"name\\":\\"Thomas\\",\\"age\\":50}\\n```\\n\\n## Conclusion\\n\\nIn the quest for an optimal solution to manual JSON string generation, experimenting with different approaches can lead to valuable insights. In this case, leveraging Java\'s MessageFormat provided a cleaner and more readable alternative to string concatenation. By sharing this experience, I hope to assist fellow developers facing similar challenges and encourage the exploration of diverse tools within the Java ecosystem."},{"id":"/2023/12/18/github-copilot-typescript-class-from-json","metadata":{"permalink":"/blog/2023/12/18/github-copilot-typescript-class-from-json","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-18-github-copilot-typescript-class-from-json.md","source":"@site/blog/2023-12-18-github-copilot-typescript-class-from-json.md","title":"Creating a Typescript Class from a JSON file","description":"How GitHub Copilot helped me create a Typescript class from a JSON file","date":"2023-12-18T21:12:00.000Z","formattedDate":"December 18, 2023","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"typescript","permalink":"/blog/tags/typescript"},{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"}],"readingTime":0.53,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Creating a Typescript Class from a JSON file","description":"How GitHub Copilot helped me create a Typescript class from a JSON file","tags":["github","copilot","javascript","typescript","coding with copilot"],"keywords":["github","copilot","javascript","typescript"],"image":"https://tgrall.github.io/images/posts/2023-12-18/header.png","date":"2023-12-18T21:12"},"prevItem":{"title":"Java: Using MessageFormat to Generate JSON","permalink":"/blog/2023/12/21/java-generating-json-with-messageformat"},"nextItem":{"title":"Quarkus: Database Projection with Panache","permalink":"/blog/2023/12/16/quarkus-database-projection-with-panache"}},"content":"When working with NextJS and Typescript, I often find myself creating classes to represent data structures. In the course of doing so, I often need to create the class from a REST API response that is a JSON object.\\n\\nTo expedite this crucial task, I turned to GitHub Copilot for assistance. Witness the efficiency and precision of Copilot in action by watching the following video:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/aQa3hjxkhzo?si=s1OSBQOcLbJLrRjq\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nYet again, GitHub Copilot proves to be the hero of my coding journey \u2013 making my day, one efficient line of code at a time!"},{"id":"/2023/12/16/quarkus-database-projection-with-panache","metadata":{"permalink":"/blog/2023/12/16/quarkus-database-projection-with-panache","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-16-quarkus-database-projection-with-panache.md","source":"@site/blog/2023-12-16-quarkus-database-projection-with-panache.md","title":"Quarkus: Database Projection with Panache","description":"Learn how to use Database Projection with Panache.","date":"2023-12-16T00:00:00.000Z","formattedDate":"December 16, 2023","tags":[{"label":"quarkus","permalink":"/blog/tags/quarkus"},{"label":"panache","permalink":"/blog/tags/panache"},{"label":"java","permalink":"/blog/tags/java"},{"label":"orm","permalink":"/blog/tags/orm"},{"label":"learning quarkus","permalink":"/blog/tags/learning-quarkus"}],"readingTime":4.79,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Quarkus: Database Projection with Panache","description":"Learn how to use Database Projection with Panache.","tags":["quarkus","panache","java","orm","learning quarkus"],"keywords":["quarkus","panache","java","orm"],"image":"/images/posts/2023-12-16-quarkus-database-projection-with-panache/header.png"},"prevItem":{"title":"Creating a Typescript Class from a JSON file","permalink":"/blog/2023/12/18/github-copilot-typescript-class-from-json"},"nextItem":{"title":"Seamless Internationalization with GitHub Copilot","permalink":"/blog/2023/12/15/github-copilot-internationalisation"}},"content":"![Quarkus: Database Projection with Panache](/images/posts/2023-12-16-quarkus-database-projection-with-panache/header.png)\\n\\n\\nWelcome back to the second installment of our exploration into Quarkus and Panache! In the previous blog post, we delved into [setting default values for Panache entity fields](/blog/2023/12/09/quarkus-entity-default-value). Now, as we continue refining the WindR.org website with Quarkus integration, our primary focus shifts to implementing **Database Projection with Panache**.\\n\\nCode Example on GitHub:\\n\\nTo accompany this discussion, I\'ve published the complete code example on GitHub, providing you with a hands-on reference for learning and experimentation.\\n\\n[GitHub Repository: Learning Quarkus: Database Projection with Panache](https://github.com/tgrall/learning-quarkus/tree/main/projection-with-panache)\\n\\n\\n###  Understanding the data model\\n\\nFor this illustrative example, we\'ll work with a straightforward data model consisting of two tables: \'boards\' and \'brands.\' The \'boards\' table contains a list of windsurfing boards, while the \'brands\' table serves as a reference, linked to the \'boards\' table through a foreign key relationship.\\n\\n![Quarkus: Default Values for Panache Entity Fields](/images/posts/2023-12-16-quarkus-database-projection-with-panache/database-model.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n#### Panache Entities:\\n\\nLet\'s take a closer look at the Panache entities representing these tables.\\n\\n**Board.java**\\n\\n```java\\n@Entity\\n@Table (\\n    name = \\"boards\\"\\n)\\npublic class Board extends PanacheEntity {\\n\\n        @ManyToOne( fetch = FetchType.LAZY, optional = false)\\n        @JoinColumn(name = \\"brand_id\\", nullable = false, foreignKey = @ForeignKey(name = \\"fk_board_brand_id\\"))\\n        public Brand brand;\\n\\n        public String name;\\n        public String description;\\n        public String image;\\n        public String program;\\n}\\n```\\n\\n**Brand.java**\\n\\n```java\\n@Entity\\n@Table(\\n    name = \\"brands\\"\\n)\\npublic class Brand extends PanacheEntity {\\n\\n        public String name;\\n        public String description;\\n        public String logo;\\n        public String website;\\n        public String address;\\n\\n}\\n```\\n\\n#### Database Schema\\n\\nThese entities are mapped to the following database tables, reflecting the relational structure:\\n\\n\\n\\n```sql\\ncreate table boards (\\n    id bigint not null,\\n    description varchar(255),\\n    image varchar(255),\\n    name varchar(255),\\n    program varchar(255),\\n    brand_id bigint not null,\\n    primary key (id)\\n);\\n\\ncreate table brands (\\n    id bigint not null,\\n    address varchar(255),\\n    description varchar(255),\\n    logo varchar(255),\\n    name varchar(255),\\n    website varchar(255),\\n    primary key (id)\\n);\\n\\nalter table if exists boards \\n    add constraint fk_board_brand_id \\n    foreign key (brand_id) \\n    references brands;`\\n\\n```\\n\\n\\n### Using the Panache Entities\\n\\nNow, let\'s create a new REST endpoint that efficiently returns the list of boards and board details, thanks to the simplicity provided by Quarkus.\\n\\n\\n**BoardResource.java**\\n\\n```java\\n@Path(\\"/api/v1/boards\\")\\n@Produces(MediaType.APPLICATION_JSON)\\npublic class BoardResource {\\n\\n    @GET\\n    public List<Board> allBoards() {\\n        return Board.listAll();\\n    }\\n\\n    @GET\\n    @Path(\\"{id}\\")\\n    public Board getBoard(Long id) {\\n        return Board.findById(id);\\n    }\\n}\\n```\\n\\n####  Testing the REST Endpoint\\nSo we can test the endpoint with the following command:\\n\\n```bash\\ncurl http://localhost:8080/api/v1/boards/0\\n```\\n\\nthat returns the following JSON:\\n\\n```json\\n{\\n  \\"id\\": 0,\\n  \\"name\\": \\"Fanatic Skate\\",\\n  \\"brand\\": {\\n    \\"id\\": 0,\\n    \\"name\\": \\"Fanatic\\",\\n    \\"description\\": \\"Fanatic Description\\",\\n    \\"logo\\": \\"logo.svg\\",\\n    \\"website\\": \\"https://www.fanatic.com/\\",\\n    \\"address\\": null\\n  },\\n  \\"description\\": \\"Fanatic Skate Description\\",\\n  \\"image\\": \\"image.png\\",\\n  \\"program\\": \\"freestyle\\"\\n}\\n```\\n\\nAs you can see the JSON contains the full details of the board and brand; and all the columns are loaded from the database, as you can see in the following SQL query:\\n\\n```sql\\n    select\\n        b1_0.id,\\n        b1_0.brand_id,\\n        b2_0.id,\\n        b2_0.address,\\n        b2_0.description,\\n        b2_0.logo,\\n        b2_0.name,\\n        b2_0.website,\\n        b1_0.description,\\n        b1_0.image,\\n        b1_0.name,\\n        b1_0.program \\n    from\\n        boards b1_0 \\n    join\\n        brands b2_0 \\n            on b2_0.id=b1_0.brand_id \\n    where\\n        b1_0.id=?\\n```\\n\\nSo, from the client perspective, it is not very efficient because you are loading the full Brand object from the database, and you are sending all the columns to the client.\\n\\n#### A quick & dirty solution\\n\\nWait! Before we do that, if the reference table has a very small number of columns, you can use the `@JsonIgnoreProperties` annotation to exclude some columns from the Brand JSON serialization. For example, if you do not want to return the `description`, `logo`, `website`, and `address` columns, you can use the following annotation:\\n\\n```java\\n    @ManyToOne( fetch = FetchType.EAGER, optional = false)\\n    @JoinColumn(name = \\"brand_id\\", nullable = false, foreignKey = @ForeignKey(name = \\"fk_board_brand_id\\"))\\n    @JsonIgnoreProperties({\\"description\\", \\"logo\\", \\"website\\", \\"address\\"})\\n    public Brand brand;\\n```\\n\\nNow when you call the endpoint, you will get the following JSON:\\n\\n```json\\n{\\n  \\"id\\": 0,\\n  \\"name\\": \\"Fanatic Skate\\",\\n  \\"brand\\": {\\n    \\"id\\": 0,\\n    \\"name\\": \\"Fanatic\\"\\n  },\\n  \\"description\\": \\"Fanatic Skate Description\\",\\n  \\"image\\": \\"image.png\\",\\n  \\"program\\": \\"freestyle\\"\\n}\\n```\\n\\nSo from the client perspective, it is better, but from the server-side, it is not very efficient because you are still loading the full Brand & Boards objects from the database.\\n\\n\\n## Efficient Data Retrieval with Database Projection:\\n\\n\\nTo enhance efficiency, we introduce the concept of a projection\u2014a query returning a subset of columns from the target entity.\\n\\nI am using Java 21, so lets create a new record named `BoardProjection`:\\n\\n```java\\nrecord BoardProjection(\\n    Long id,\\n    String name,\\n    String image,\\n    String program,\\n    @ProjectedFieldName(\\"brand.name\\") String brandName\\n) { }\\n```\\nThe process is straightforward: specify the fields you wish to include, and when referencing a field from a related table, employ the [`@ProjectedFieldName`](https://javadoc.io/doc/io.quarkus/quarkus-hibernate-orm-panache-common/latest/io/quarkus/hibernate/orm/panache/common/ProjectedFieldName.html) annotation.\\n\\nTo implement this projection, utilize a [`PanacheQuery`](https://javadoc.io/doc/io.quarkus/quarkus-hibernate-orm-panache/latest/io/quarkus/hibernate/orm/panache/PanacheQuery.html) and employ the project method to define the projection.\\n\\n\\n```java\\n    @GET\\n    public List<BoardProjection> allBoards() {\\n        PanacheQuery<Board> boards = Board.findAll();\\n        return boards.project(BoardProjection.class).list();\\n    }\\n\\n    @GET\\n    @Path(\\"{id}\\")\\n    public BoardProjection getBoard(Long id) {\\n\\n        PanacheQuery<BoardProjection> boardQuery =\\n                Board.find(\\"id\\", id).project(BoardProjection.class);\\n        return boardQuery.firstResult();\\n    }\\n```\\n\\nPretty neat !\\n\\nLet\'s look at the API call and SQL query:\\n\\n```bash \\ncurl http://localhost:8080/api/v1/boards/0\\n```\\nJSON Result\\n```json\\n{\\n  \\"id\\": 0,\\n  \\"name\\": \\"Skate\\",\\n  \\"image\\": \\"image.png\\",\\n  \\"program\\": \\"freestyle\\",\\n  \\"brandName\\": \\"Fanatic\\"\\n}\\n```\\n\\nand the SQL query:\\n\\n```sql\\n    select\\n        b1_0.id,\\n        b1_0.name,\\n        b1_0.image,\\n        b1_0.program,\\n        b2_0.name \\n    from\\n        boards b1_0 \\n    join\\n        brands b2_0 \\n            on b2_0.id=b1_0.brand_id \\n    where\\n        b1_0.id=? \\n    fetch\\n        first ? rows only\\n\\n```\\n\\nBy utilizing Panache\'s simple projection definition with a Java record, our API calls now retrieve only the essential data, optimizing both client and server-side performance.\\n\\n\\n## Conclusion\\n\\nIn this blog post, we\'ve explored the power of Database Projection with Panache in a Quarkus environment. By efficiently selecting specific columns through a projection, we strike a balance between data completeness and performance, ensuring a streamlined experience for both developers and end-users alike.\\n\\nFeel free to check out [the complete code on GitHub](https://github.com/tgrall/learning-quarkus/tree/main/projection-with-panache), experiment with the examples, and stay tuned for more Quarkus insights in future posts!"},{"id":"/2023/12/15/github-copilot-internationalisation","metadata":{"permalink":"/blog/2023/12/15/github-copilot-internationalisation","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-15-github-copilot-internationalisation.md","source":"@site/blog/2023-12-15-github-copilot-internationalisation.md","title":"Seamless Internationalization with GitHub Copilot","description":"How GitHub Copilot helped me translate my website in seconds","date":"2023-12-15T00:00:00.000Z","formattedDate":"December 15, 2023","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"typescript","permalink":"/blog/tags/typescript"},{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"}],"readingTime":0.985,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Seamless Internationalization with GitHub Copilot","description":"How GitHub Copilot helped me translate my website in seconds","tags":["github","copilot","javascript","typescript","coding with copilot"],"keywords":["github","copilot","javascript","typescript"],"image":"/images/posts/2023-12-15/header.png"},"prevItem":{"title":"Quarkus: Database Projection with Panache","permalink":"/blog/2023/12/16/quarkus-database-projection-with-panache"},"nextItem":{"title":"Simplifying Data Generation with GitHub Copilot","permalink":"/blog/2023/12/13/github-copilot-assist-generate-test-data"}},"content":"In the course of developing using NextJS, the integration of internationalization became a pivotal requirement on my checklist. The aim was to enable seamless translation of the application into multiple languages, leveraging the native i18n features provided by NextJS, as outlined in the [official documentation](https://nextjs.org/docs/app/building-your-application/routing/internationalization).\\n\\nTo facilitate this, I organized a set of JSON files, each containing translations for the application in a specific language. For simplicity, I adopted a one-file-per-language approach, ensuring uniformity by maintaining identical keys across all files. The translation process involves replacing the values associated with each key.\\n\\nTo expedite this crucial task, I turned to GitHub Copilot for assistance. Witness the efficiency and precision of Copilot in action by watching the following video:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/C-UfSQrmI_8?si=NVZepFaOa-9yl1Jb\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nYet again, GitHub Copilot proves to be the hero of my coding journey \u2013 making my day, one efficient line of code at a time!\\n\\nPS: In this video I am translating the full file, but you can also translate a single key, or a set of keys, as you type them in the IDE, GitHub Copilot completion will propose you the translation."},{"id":"/2023/12/13/github-copilot-assist-generate-test-data","metadata":{"permalink":"/blog/2023/12/13/github-copilot-assist-generate-test-data","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-13-github-copilot-assist-generate-test-data.md","source":"@site/blog/2023-12-13-github-copilot-assist-generate-test-data.md","title":"Simplifying Data Generation with GitHub Copilot","description":"How GitHub Copilot is helping me to generate sample test data.","date":"2023-12-13T00:00:00.000Z","formattedDate":"December 13, 2023","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"java","permalink":"/blog/tags/java"},{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"}],"readingTime":1.025,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Simplifying Data Generation with GitHub Copilot","description":"How GitHub Copilot is helping me to generate sample test data.","tags":["github","copilot","java","coding with copilot"],"keywords":["github","copilot","java"],"image":"/images/posts/2023-12-13-copilot-sample-data/header.png"},"prevItem":{"title":"Seamless Internationalization with GitHub Copilot","permalink":"/blog/2023/12/15/github-copilot-internationalisation"},"nextItem":{"title":"Accelerating REST Development with GitHub Copilot","permalink":"/blog/2023/12/11/github-copilot-assist-writing-a-rest-client"}},"content":"In the process of crafting an updated version of the product catalog for WindR.org, the need to generate sample data arises. Leveraging the power of Quarkus and Panache, I find myself tasked with creating entities that embody various technical specifications for \\"windsurfing boards\\" \u2013 encompassing attributes like size, volume, width, and more.\\n\\nRecognizing the potential tedium associated with manually creating this data, I turned to GitHub Copilot for assistance. The approach I took involved visiting a public website housing a comprehensive list of windsurfing boards. Here, I extracted the specifications of a specific board and seamlessly fed them into the GitHub Copilot Chat window. I then prompted Copilot to not only generate sample Java entities but also produce the corresponding SQL script for creating database rows.\\n\\nThe efficiency and effectiveness of GitHub Copilot in this scenario are showcased in the accompanying video. Witness firsthand how this tool streamlines the often laborious task of data generation, saving valuable time and effort in the development process:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/XEE7mi3ZGJU?si=6zPKgmxgS3NqwuID\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nYet again, GitHub Copilot proves to be the hero of my coding journey \u2013 making my day, one efficient line of code at a time!"},{"id":"/2023/12/11/github-copilot-assist-writing-a-rest-client","metadata":{"permalink":"/blog/2023/12/11/github-copilot-assist-writing-a-rest-client","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-11-github-copilot-assist-writing-a-rest-client.md","source":"@site/blog/2023-12-11-github-copilot-assist-writing-a-rest-client.md","title":"Accelerating REST Development with GitHub Copilot","description":"How GitHub Copilot helped me write a REST Client in seconds","date":"2023-12-11T00:00:00.000Z","formattedDate":"December 11, 2023","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"copilot","permalink":"/blog/tags/copilot"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"typescript","permalink":"/blog/tags/typescript"},{"label":"coding with copilot","permalink":"/blog/tags/coding-with-copilot"}],"readingTime":0.665,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Accelerating REST Development with GitHub Copilot","description":"How GitHub Copilot helped me write a REST Client in seconds","tags":["github","copilot","javascript","typescript","coding with copilot"],"keywords":["github","copilot","javascript","typescript"],"image":"/images/posts/2023-12-11-github-copilot-assist-writing-a-rest-client/header.png"},"prevItem":{"title":"Simplifying Data Generation with GitHub Copilot","permalink":"/blog/2023/12/13/github-copilot-assist-generate-test-data"},"nextItem":{"title":"Quarkus: Default Values for Panache Entity Fields","permalink":"/blog/2023/12/09/quarkus-entity-default-value"}},"content":"In the process of constructing a product catalog within NextJS, I found myself in need of seamlessly interfacing with my backend using REST APIs, each adorned with a variety of URIs and parameters.\\n\\nThe typical structure of these URLs, such as `/api/v1/products?category=shoes&color=red&size=10`, prompted me to harness the power of GitHub Copilot to expedite the creation of functions with the appropriate parameters. Copilot not only assists in generating the function but also crafts the necessary fetch code for calling the API with precision.\\n\\nWatch this video to witness the rapid development in action:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/-WWwTHudP4I?si=-TrVAxgW_OjtF2WB\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nYet again, GitHub Copilot proves to be the hero of my coding journey \u2013 making my day, one efficient line of code at a time!"},{"id":"/2023/12/09/quarkus-entity-default-value","metadata":{"permalink":"/blog/2023/12/09/quarkus-entity-default-value","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-12-09-quarkus-entity-default-value.md","source":"@site/blog/2023-12-09-quarkus-entity-default-value.md","title":"Quarkus: Default Values for Panache Entity Fields","description":"Learn how to set default values for Panache entity fields in Quarkus.","date":"2023-12-09T00:00:00.000Z","formattedDate":"December 9, 2023","tags":[{"label":"quarkus","permalink":"/blog/tags/quarkus"},{"label":"panache","permalink":"/blog/tags/panache"},{"label":"java","permalink":"/blog/tags/java"},{"label":"orm","permalink":"/blog/tags/orm"},{"label":"learning quarkus","permalink":"/blog/tags/learning-quarkus"}],"readingTime":3.3,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Quarkus: Default Values for Panache Entity Fields","description":"Learn how to set default values for Panache entity fields in Quarkus.","tags":["quarkus","panache","java","orm","learning quarkus"],"keywords":["quarkus","panache","java","orm"]},"prevItem":{"title":"Accelerating REST Development with GitHub Copilot","permalink":"/blog/2023/12/11/github-copilot-assist-writing-a-rest-client"},"nextItem":{"title":"Revoke Deploy Keys with GitHub CLI","permalink":"/blog/2023/03/31/revoke-github-deploy-key-org-level"}},"content":"![Quarkus: Default Values for Panache Entity Fields](/images/posts/2023-12-09/header.png)\\n\\nIn the ever-evolving landscape of database technologies, my journey led me away from Java ORM projects for a decade, exploring the realms of NoSQL databases like MongoDB, Couchbase, Redis, and even HBase.\\n\\nRecently, my focus shifted back to Java and, specifically, Quarkus. In this blog post, I\'ll share my experience migrating part of my site, windr.org, from MongoDB to PostgreSQL with Quarkus, highlighting how I tackled my first small challenge of **setting default values for Panache entity fields**.\\n\\n\\nChoosing Quarkus and Panache:\\n\\nHaving dabbled with Quarkus during my time at Red Hat in 2019, I decided to delve deeper into it for my personal projects. While familiar with using MongoDB directly with Node.js, working with Quarkus and RDBMS prompted me to opt for Hibernate ORM with Panache, a Quarkus extension offering a [simplified and user-friendly API for Hibernate ORM](https://quarkus.io/guides/hibernate-orm-panache).\\n\\n\\nI have published the code of this example on GitHub:\\n- [Learning Quarkus: Default Values for Panache Entity Fields](https://github.com/tgrall/learning-quarkus/tree/main/panache-default-value)\\n\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n## Panache Entities and Active Record Pattern\\n\\nUtilizing the [active record pattern](https://martinfowler.com/eaaCatalog/activeRecord.html), I extended the PanacheEntity class, leveraging its built-in methods for persisting, finding, updating, and deleting entities. Taking an example, let\'s look at the `Brand` entity:\\n\\n```java\\n@Entity\\n@Table(name = \\"brands\\")\\npublic class Brand extends PanacheEntity {\\n\\n    public String name;\\n\\n    public String description;\\n\\n    public String logo;\\n\\n}\\n``` \\n\\nAs mentioned in the [Quarkus Documentation](https://quarkus.io/guides/hibernate-orm-panache#panache-entity), the `PanacheEntity` class provides the methods to persist, find, update and delete entities.\\n\\nThe question that I had was: \\n- *how to set default values for the fields of the entity?*\\n\\nIn my case I wanted to set the `logo` field to a default value if it was not set.\\n\\n\\n## Setting Default Values:\\n\\nThe primary question that arose during this migration was how to set default values for Panache entity fields, specifically the \'logo\' field in my case. Two approaches were explored.\\n\\n### Using Entity Field Declaration\\n\\nThe simplest method involved assigning a default value directly in the entity field declaration. For instance:\\n\\n```java\\n@Entity\\n@Table(name = \\"brands\\")\\npublic class Brand extends PanacheEntity {\\n\\n    public String name;\\n\\n    public String description;\\n\\n    public String logo = \\"default-logo.png\\";\\n\\n}\\n```\\n\\nThis method worked seamlessly, providing the desired default value when creating new entities, as you can see with the following test:\\n\\n```java\\n        Brand fanaticWithoutLogo = new Brand();\\n        fanaticWithoutLogo.name = \\"Fanatic\\";\\n        fanaticWithoutLogo.description = \\"Fanatic Description\\";\\n\\n        String fanaticWithoutLogoString = objectMapper.writeValueAsString(fanaticWithoutLogo);\\n        given()\\n                .body(fanaticWithoutLogoString)\\n                .contentType(ContentType.JSON)\\n                .accept(ContentType.JSON)\\n                .when().post(\\"/api/v1/brands\\")\\n                .then()\\n                .statusCode(201)\\n                .body(\\n                        \\"name\\", is(fanaticWithoutLogo.name),\\n                        \\"description\\", is(fanaticWithoutLogo.description),\\n                        \\"logo\\", is(\\"default-logo.png\\")\\n                );\\n```\\n\\nI wanted also to enforce a default value when I data are inserted directly in the database. For example, when I am using the `psql` command line tool to insert data in the database, or other external tools.\\n\\n\\n### Using Column Definition\\n\\nA more complex but schema-focused approach utilized the `@Column` annotation to set the default value:\\n\\n```java\\n@Entity\\n@Table(name = \\"brands\\")\\npublic class Brand extends PanacheEntity {\\n\\n    public String name;\\n\\n    public String description;\\n\\n    @Column(columnDefinition = \\"varchar(255) default \'default-logo.png\'\\")\\n    public String logo = \\"default-logo.png\\";\\n\\n}\\n```\\n\\nThis modify the database schema and add the default value:\\n\\n```sql\\n    create table brands (\\n    id bigint not null,\\n    description varchar(255),\\n    logo varchar(255) default \'default-logo.png\',\\n    name varchar(255),\\n    primary key (id)\\n);\\n);\\n```\\n\\n## Caveats and Conclusion\\n\\nWhile effective, this approach involved hard-coding the default value in both the code and the database schema, presenting a minor inconvenience.\\n\\nDespite a brief disappointment with the redundancy of the second solution, it proved effective for my current needs. Although `@PrePersist` annotation wasn\'t explored, it\'s likely to exhibit similar behavior. If a more elegant solution emerges, I\'ll be sure to update this post.\\n\\nIn sharing this journey, I hope to assist developers navigating similar challenges and contribute to the collective knowledge of the Quarkus community. While these concepts might be second nature for Jakarta Persistence users, they present insightful perspectives for individuals, much like myself, who are reacquainting themselves with Java ORM after a decade immersed in the world of NoSQL development."},{"id":"/2023/03/31/revoke-github-deploy-key-org-level","metadata":{"permalink":"/blog/2023/03/31/revoke-github-deploy-key-org-level","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-03-31-revoke-github-deploy-key-org-level.md","source":"@site/blog/2023-03-31-revoke-github-deploy-key-org-level.md","title":"Revoke Deploy Keys with GitHub CLI","description":"In this post, I will show you how to use GitHub CLI to invalidate deploy keys at the organization level.","date":"2023-03-31T00:00:00.000Z","formattedDate":"March 31, 2023","tags":[{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"CLI","permalink":"/blog/tags/cli"},{"label":"API","permalink":"/blog/tags/api"},{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"DevOps","permalink":"/blog/tags/dev-ops"},{"label":"SSH","permalink":"/blog/tags/ssh"}],"readingTime":1.665,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Revoke Deploy Keys with GitHub CLI","description":"In this post, I will show you how to use GitHub CLI to invalidate deploy keys at the organization level.","tags":["GitHub","CLI","API","CI/CD","DevOps","SSH"],"keywords":["GitHub","CLI","API","CI/CD","DevOps","SSH"]},"prevItem":{"title":"Quarkus: Default Values for Panache Entity Fields","permalink":"/blog/2023/12/09/quarkus-entity-default-value"},"nextItem":{"title":"GitHub Copilot: How I use it, and Why I love it","permalink":"/blog/2023/02/21/copilot-how-i-use-it-why-i-love-it"}},"content":"![Use GitHub CLI to Invalide Deploy Keys at the Organization Level](/images/posts/2023-03-31-revoke-github-deploy-key-org-level/header.png)\\n\\nWhen working with Github, you may need to manage [deploy keys](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/managing-deploy-keys) to allow automated deployments from external services. Deploy keys are SSH keys that grant read-only or read-write access to a single repository. You can add deploy keys to your repository from the settings page, but what if you need to revoke access for many repositories at once?\\n\\nToday, there\'s no direct way to do this from the Github UI, but you can easily accomplish it with the APIs and the [GitHub CLI `gh`](https://cli.github.com/).\\n\\nI have created the following script to achieve this:\\n\\n```bash\\n#!/bin/bash\\n\\nORG=$1\\n\\nif [ -z \\"$ORG\\" ]\\nthen\\n  echo \\"No organization provided\\"\\n  exit 1\\nfi\\n\\nREPOS=$(gh repo list $ORG -L 200 --json name --jq \'.[].name\')\\n\\nfor repo in $REPOS\\ndo\\n  echo \\"Deployment keys for $repo:\\"\\n  KEYS=$(gh api repos/$ORG/$repo/keys | jq -r \'.[] | [.id] | @tsv\')\\n  if [ -z \\"$KEYS\\" ]\\n  then\\n    echo -e \'\\\\t No Deployment Key\'\\n  else\\n    for key in $KEYS\\n    do\\n    gh repo deploy-key delete -R $ORG/$repo $key\\n    done\\n  fi\\ndone\\n\\n```\\n\\nYou can use it like this:\\n\\n```bash\\n./revoke-deploy-keys.sh <org-name>\\n```\\n\\nLet\'s take a closer look at the script itself. It starts by setting a variable named \\"`ORG`\\" to the value passed as a parameter when the script is run. The script then uses the `gh` CLI to retrieve a list of repositories for the organization and stores them in a variable named \\"`REPOS`\\".\\n\\nNext, the script loops through each repository and retrieves a list of deploy keys using the GH API. If there are no deploy keys, the script simply prints a message saying so. Otherwise, the script loops through each key and uses the `gh` CLI to delete it using the command [`gh repo deploy-key delete`](https://cli.github.com/manual/gh_repo_deploy-key_delete).\\n\\nOverall, the script is a quick and easy way to revoke deploy keys for an entire organization\'s repositories. By combining the `gh` CLI and Github API, you can automate this process and save time and effort."},{"id":"/2023/02/21/copilot-how-i-use-it-why-i-love-it","metadata":{"permalink":"/blog/2023/02/21/copilot-how-i-use-it-why-i-love-it","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2023-02-21-copilot-how-i-use-it-why-i-love-it.md","source":"@site/blog/2023-02-21-copilot-how-i-use-it-why-i-love-it.md","title":"GitHub Copilot: How I use it, and Why I love it","description":"How do I use GitHub Copilot during my development, why I love it!.","date":"2023-02-21T00:00:00.000Z","formattedDate":"February 21, 2023","tags":[{"label":"Github","permalink":"/blog/tags/github"},{"label":"development","permalink":"/blog/tags/development"},{"label":"Copilot","permalink":"/blog/tags/copilot"},{"label":"AI","permalink":"/blog/tags/ai"},{"label":"IDE","permalink":"/blog/tags/ide"}],"readingTime":9.895,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"GitHub Copilot: How I use it, and Why I love it","description":"How do I use GitHub Copilot during my development, why I love it!.","tags":["Github","development","Copilot","AI","IDE"],"keywords":["Github","development","Copilot","AI","IDE"]},"prevItem":{"title":"Revoke Deploy Keys with GitHub CLI","permalink":"/blog/2023/03/31/revoke-github-deploy-key-org-level"},"nextItem":{"title":"GitHub Self-Hosted Runner Autoscaling with Kubernetes","permalink":"/blog/2022/10/16/github-self-hosted-runner-autoscaling-with-kubernetes"}},"content":"![GitHub Copilot: How I use it, and Why I love it](/images/posts/2023-02-04-a-year-with-copilot/header.jpeg)\\n\\nSince its launch, GitHub Copilot has been a hot topic in the developer community. Some people love it, some people hate it, and some people are just curious about it.  I am part of the first group, and I have been using it for more than a year now. I will not go into the details of how Copilot works, but I will share with you my experience with it, and how it has changed my life as a developer.\\n\\nCopilot will help developers differently depending on the way they work and the phase of the project they are in. I will try to cover as many use cases as possible, but I will not be able to cover all of them. Here are the main uses cases I have for Copilot\\n\\n- Write code faster\\n- Create new features much faster\\n- Learn new things in context\\n\\n\x3c!--truncate--\x3e\\n\\nSo let me start by explaining what I do as a developer.\\n\\nAs you probably know, I am a Solutions Engineer at GitHub, so I am doing different things:\\n- explaining and showing how GitHub works during customers meetings or developer events\\n- building demonstrations and tutorials around GitHub\\n\\nOn a more personal level, I am also a developer, and I am working on a Web site for Windsurfers, Kitesurfers and Wingfoilers: [**https://windr.org**](https://windr.org). This project has two main goals:\\n- continue to develop my skills as a developer, and test new things\\n- build a \\"real\\" online service to let riders log session and compete in Virtual GPS Competitions.\\n\\nTo make it short, WindR mixes many technologies: JavaScript, TypeScript, Java, Python, Vue, React / ReactNative, Node, Spring, MongoDB, Redis, and Containers/Docker all these are managed in GitHub repositories and deployed on various Cloud Services from [Azure](https://azure.microsoft.com/) ([Container Apps](https://azure.microsoft.com/en-us/products/container-apps), [Azure Functions](https://azure.microsoft.com/en-us/products/functions/), [Storage](https://azure.microsoft.com/en-us/products/storage/blobs/), Database), GCP ([Storage](https://cloud.google.com/storage), [Cloud Run](https://cloud.google.com/run)) and [Clever Cloud](https://www.clever-cloud.com/) (Web, Database, Storage). _I have not tried AWS yet, but I will soon_.\\n\\n\\n\\n### Write more tests... faster\\n\\nAs part of the WindR project, I have to write many file parsers (GPX, OAO) to extract data from files and do some calculations: top 5 500m, 10s, max speed and many more. \\n\\nWriting tests for these parsers is tedious, and I have to do it for each new parser I write. I have to create a file with the data I want to test, and then I have to write the test, and most of these tasks are repetitive.\\n\\nThe following video shows, how I Copilot is using me in IntelliJ to write a test for my [OAO Parser](https://www.motion-gps.com/motion/documentation/oao-file-format.html):\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/IRLMLEfcXEI\\" title=\\"Write test with Copilot\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\nCopilot is using the context of your IDE -- current file, opened tabs, project file--, this is why when I work I keep many files opened in my IDE. In this example I only have 2 files opened:\\n- the list of values used in the tests\\n- the tests file\\n\\nAs you can see, the suggestions are made based on the context of the file I am working on. It was very helpful to have the suggestions giving me 20s, 30s, .. then 100m, 250m 500m based on the context and the values located in the different files.\\n\\nIn the video, I show the Test suggestions in Java, I have also used Copilot in Javascript for the main application and the [product catalog](https://windr.org/catalog) and its API, and also some TypeScript for a new service I am creating.\\n\\nCopilot helped me to have a good test coverage for my parser:\\n\\n![OAO Parser Test Coverage](/images/posts/2023-02-04-a-year-with-copilot/001-code-coverage.png)\\n\\n\\n_As a developer, I have no excuse to not write tests anymore!_\\n\\n\\n### Write code... faster\\n\\nYou can find many demonstrations of Copilot where developers are creating applications from scratch, it is pretty cool and impressive. This is more or less what I am showing in the next section. However, most of the time when you are working on a project, you are not creating a new application from scratch, you are working on an existing project, and you are adding new features, or fixing bugs.\\n\\nIn this case, Copilot is helping me a lot, providing me with suggestions in context and using the coding style of the project. \\n\\nThe following video shows how I use Copilot to add new UI components in an HTML page:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/k0uVz3OmJGw\\" title=\\"Write code faster\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nHere, I am using Copilot in VS Code, and I am working on a project that uses EJS templates. I have a page that displays the session details, and I need to add new data in different sections of the page. Each section is using the session data differently:\\n- take the best in the header\\n- take the top 5 in the detailed view\\n\\nBy adding new comments in the HTML file, Copilot can suggest the code I need to add to the page, and the context of the page is used to use the proper HTML elements, a single value for the header, and a list for the detailed view. And as you may have noticed, the suggested code is using the coding style of the project, so I do not have to change the way I code.\\n\\nWhen I am using Copilot this way, I am rarely using the Copilot Window (`ctrl+enter`), I am using the suggestions directly in the editor. So this is why I can say that you do not have to change the way you code when using Copilot, just code as you usually do, and Copilot will help with some portions of the code almost \\"transparently\\".\\n\\n_So you can see now why I am saying that Copilot is helping me to write code faster._\\n\\n\\n### Create new features... faster\\n\\nMost of the time when I work with Copilot I am using the suggestion directly in the editor based on the context of the files I am working on, so using some comments, or simply typing some code. Nevertheless, sometimes I want to create a new feature, and I use Copilot to help me with the design of the skeleton of the feature.\\n\\nFor example on my Windsurfing site, users should be able to create an image of the session and share it on social networks ([like this one](https://windr.org/traces/63af1d96b3335105b2b433b6.png)). I opened VS Code, created a new Python file and typed a comment that describes my feature, something like:\\n\\n```python\\n# using selenium and chromedriver as headless browser\\n# go to https://windr.org/traces/63af1d96b3335105b2b433b6/image\\n# windown size 800x800\\n# sleep for 3 seconds\\n# save screenshot as image.png \\n```\\n\\nThen I use the Copilot Window (`ctrl+enter`) to get some suggestions. I have to say that I am not a big fan of the Copilot Window, I prefer to use the suggestions directly in the editor, but I have to admit that it is very helpful when you want to create a new feature. \\n\\nThe following video shows how I use Copilot to create a new feature:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/-JBHy5w94Us\\" title=\\"Build new features in minutes\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\n_You can see why I am saying that Copilot is helping me to create new features faster_, few comments and I have a first implementation. As you can guess I am not using the code as it is, but it is a very good starting point that will allow me to go faster to the \\"meat\\" of the feature, and allow me to deploy it faster, evaluate it, and iterate on it. (I am right now reworking this feature, see below)\\n\\n### Learn new things... faster\\n\\nCopilot is quite handy when you want to learn new programming languages and/or new libraries. For example, I have used it to help me to write some Python, Terraform, and TypeScript code.\\n\\nThere, I am using a mix of inline suggestions and the Copilot Window, to discover new things. \\n\\nOne thing I am learning is [Playwright](https://playwright.dev/), to write end-to-end tests; and I want to use it to create screenshots in the feature I have described above - and move from Python to TypeScript.\\n\\n### And now... ChatGPT\\n\\nWith the arrival of [ChatGPT](https://chat.openai.com/), I often have questions about how do I compare Copilot and ChatGPT, and I have a short answer:\\n\\n- _I do not compare Copilot and ChatGPT, I use both of them depending on the context._\\n\\nAs a developer, I spend most of my time writing code inside an IDE (_IntelliJ, VSCode & Codespaces_), and I want to stay focused on my IDE, I do not want to switch to a web browser to use ChatGPT. Based on the uses cases I have described above, I do not \\"need\\" an additional tool, Copilot is there and helping me a lot already.\\n\\nHowever, I have started to use ChatGPT to help me \\"design\\" some features, a little bit like a conversation with a teammate in a meeting room with a whiteboard, for example asking the following questions:\\n\\n- _Which logic I should use to extract the best 500 meters speed in a GPX file?_\\n- _What is the best way to save an HTML page as image programmatically using TypeScript?_\\n\\nChatGPT can answer the question, suggest some ideas, and I can use it to help me to implement the feature. So far I have not used the code generated by ChatGPT, but I have used the ideas to implement the feature, in my IDEs with the help of Copilot.\\n\\n\\n### What\'s next?\\n\\n**Copilot Labs extension for VS Code**\\n\\nIn this article, I have not talked about the [Copilot Labs extension for VS Code](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-labs), which has interesting features: code explanation, code translation, IDE Brushes, test generation. I am showing all these features when I am doing a live demo of Copilot, but I have not used them yet in my \\"daily developer work\\". \\n\\nMostly because, as I explained earlier, I am using Copilot to help me with the code I am writing and do not want to move to another tab in the IDE while I am coding. That said, I have already a few use cases that I want to try with the Copilot Labs: generating tests, and code translation to help me to move some code from JavaScript to TypeScript.\\n\\n\\n**GitHub Next**\\n\\nCopilot Labs, and previously Copilot itself, are part of the \\"[GitHub Next](https://githubnext.com/)\\" initative that [investigates the future of software development](https://githubnext.com/). I am inviting you to look at this Web site and see what is coming next for GitHub, and for developers. Some of the features using AI are very interesting, and I am looking forward to seeing them in action for example:\\n- [GitHub Copilot CLI](https://githubnext.com/projects/copilot-cli/) will help me to use the shell in a more productive way\\n- [AI for Pull Requests](https://githubnext.com/projects/ai-for-pull-requests/) that will help developers to work on their pull requests (refactoring, reviews, tests, ...)\\n\\nI will let you discover the others features and do not hesitate to join the various beta & waitlists.\\n\\n\\n## Conclusion\\n\\nAs described in the various use cases, I wanted to share with you how Copilot is helping me when I am wearing my developer hat. I have heard sometimes people that are saying that Copilot/AI is replacing developers, and I do not agree with this statement. \\n\\nToday, Copilot is helping me to write code faster, or to learn new things, or to create new features, but I am still writing the code, and I am still the one that is responsible for what is implemented. \\n\\nI won\'t say that Copilot is making me a better developer, but it helps me to be more productive, and probably that it will help me to be more creative in the future. Copilot is just an additional tool in my toolbox.\\n\\n**And you what is your experience with Copilot?**"},{"id":"/2022/10/16/github-self-hosted-runner-autoscaling-with-kubernetes","metadata":{"permalink":"/blog/2022/10/16/github-self-hosted-runner-autoscaling-with-kubernetes","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2022-10-16-github-self-hosted-runner-autoscaling-with-kubernetes.md","source":"@site/blog/2022-10-16-github-self-hosted-runner-autoscaling-with-kubernetes.md","title":"GitHub Self-Hosted Runner Autoscaling with Kubernetes","description":"GitHub Self-Hosted Runner Autoscaling with Kubernetes","date":"2022-10-16T00:00:00.000Z","formattedDate":"October 16, 2022","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"actions","permalink":"/blog/tags/actions"},{"label":"automation","permalink":"/blog/tags/automation"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"label":"cicd","permalink":"/blog/tags/cicd"},{"label":"devops","permalink":"/blog/tags/devops"}],"readingTime":6.84,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"GitHub Self-Hosted Runner Autoscaling with Kubernetes","tags":["github","actions","automation","kubernetes","cicd","devops"]},"prevItem":{"title":"GitHub Copilot: How I use it, and Why I love it","permalink":"/blog/2023/02/21/copilot-how-i-use-it-why-i-love-it"},"nextItem":{"title":"Getting Started with GitHub Releases","permalink":"/blog/2022/02/08/getting-started-with-github-releases"}},"content":"![GitHub Self-Hosted Runner Autoscaling with Kubernetes](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/title.png)\\n\\nIn this article, you will learn how to use Kubernetes to run GitHub Actions self-hosted runners on-demand. This enables you to auto-scale your runners based on the number of queued jobs, and quickly respond to spikes in workflow activity.\\n\\nI will focus on GitHub Enterprise Server, but the same concepts apply to GitHub.com, and this post is based on the work of the [Actions Runner Controller](https://github.com/actions-runner-controller/actions-runner-controller) team and [Natalie Somersall](https://github.com/some-natalie)\\n\\n\\nGitHub Actions is a great way to run your CI/CD workflows. However, it can be complex to run a large number of self-hosted runners. Using Kubernetes to auto-scale your runners you can simplify the management of your runners. The [`actions-runner-controller`](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/docs/detailed-docs.md) is a Kubernetes controller that manages self-hosted.\\n\\nYou can find a video of this article on YouTube:\\n\\n- [\ud83c\udfa5 GitHub Self-Hosted Runner Autoscaling with Kubernetes](https://www.youtube.com/watch?v=6Z4p-qjnKCQ)\\n\\n\x3c!--truncate--\x3e\\n\\n##### Prerequisites\\n\\n- [GitHub Enterprise Server](https://docs.github.com/en/enterprise-server@latest/admin/overview/about-github-enterprise-server) with an existing organization. I am using version 3.6.1, and my organization is called `demo`\\n- A Kubernetes cluster, I am using a [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine) cluster  _(v1.22.12-gke.2300 no autopilot)_, not a preference, just that the team I am working with is using GKE. \\n- [Helm](https://helm.sh/), I am using v3.10.1\\n\\n\\nIn the following section you will:\\n- create a GitHub App to authenticate the `actions-runner-controller`\\n- deploy and configure the `actions-runner-controller`\\n- Test the newly created runners\\n\\n### GitHub Enterprise Server Authentication\\n\\nThe Actions Runner Controller must authenticate with GitHub Enterprise Server. \\n\\nTo authenticate the runner controller against GitHub Enterprise Server you have two options:\\n-  PAT (Personal Access Token)\\n-  GitHub Apps\\n\\nNote that the runners can be registered at different levels:\\n- [Enterprise](https://docs.github.com/en/enterprise-server@latest/actions/hosting-your-own-runners/adding-self-hosted-runners#adding-a-self-hosted-runner-to-an-enterprise)\\n- [Organization](https://docs.github.com/en/enterprise-server@latest/actions/hosting-your-own-runners/adding-self-hosted-runners#adding-a-self-hosted-runner-to-an-organization)\\n- [Repository](https://docs.github.com/en/enterprise-server@latest/actions/hosting-your-own-runners/adding-self-hosted-runners#adding-a-self-hosted-runner-to-a-repository)\\n\\n\\nIn the next steps, you will create a GitHub App to authenticate and use it at the organization level.\\n\\n\\n\\n### Create a GitHub App\\n\\nTo create a GitHub App, you need to be an admin of the GitHub Enterprise Server instance. You can find more information about GitHub Apps [here](https://docs.github.com/en/enterprise-server@latest/developers/apps/about-apps).\\n\\n\\n1. Navigate to your Organization settings.\\n    \\n   For this post, you will create a GitHub App owned by an organization, in the upper-right corner of any page, click your profile photo, then click **Your organizations**. \\n   \\n   ![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/gh-apps-001.png)\\n\\n   Then, to the right of the organization, click **Settings**.\\n   \\n   ![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/gh-apps-002.png)\\n\\n    \\n 1. In the left sidebar, click  ** < >Developer settings**.  \\n\\n 1. Click **New GitHub App**.\\n\\n 1. In \\"GitHub App name\\", type the name of your app.\\n  \\n  ![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/gh-apps-003.png)\\n\\n 1. Optionally, in \\"Description\\", type a description of your app that users will see.\\n\\n 1. In \\"Homepage URL\\", type the URL where users can learn more about your app, you can point to the Actions Runner Controller GitHub repository.\\n\\n 1. In \\"Wehbook\\", uncheck **Active**.\\n\\n\\n 1. Then configure the permissions for your app. For this post, you will need to select the following permissions: (we will use Organization Runners)\\n    * Repository Permissions\\n        * Actions (read)\\n        * Metadata (read)\\n    * Organization Permissions\\n        * Self-hosted runners (read / write)\\n    \\n    If you want to register the runners at the Repository level, you will need different permissions as documented [here](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/docs/detailed-docs.md#deploying-using-github-app-authentication).\\n\\n \\n 1. Keep the default values for the other options, and click **Create GitHub App**.\\n\\n\\n 1. In the next screen, Generate a Private key, and download it. You will need it later. (file `arc-private-private-key.pem` in the following steps)\\n\\n\\n#### Install the newly created GitHub App\\n\\nIn the application settings,\\n \\n 1. Find the **Install App** button. Click on it.\\n 2. Select the organization where you want to install the app. Then click **Install**.\\n 3. Select \\"All repositories\\" and click **Install**.\\n\\n  ![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/gh-apps-004.png)\\n\\n\\n\\n### Deploy GitHub Actions Runner Controller\\n\\nYou are not ready to deploy the Actions Runner Controller and register it to your GitHub Enterprise Server instance, for this we need:\\n\\n\\n### 1. Install the Cert Manager using Helm\\n\\n```bash\\n\\n> kubectl create namespace cert-manager\\n\\n> helm repo add jetstack https://charts.jetstack.io\\n\\n> helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1 --set installCRDs=true\\n\\n\\n```\\n\\n### 2. Install the Actions Runner Controller using Helm\\n\\n- The GitHub Server URL ( `$GITHUB_SERVER_URL` in the following steps)\\n\\n\\n```bash\\n\\n> helm repo add actions-runner-controller https://actions-runner-controller.github.io/actions-runner-controller\\n\\n> kubectl create namespace actions-runner-system\\n\\n> helm install -n actions-runner-system actions-runner-controller actions-runner-controller/actions-runner-controller --version=0.21.0\\n\\n> kubectl set env deploy actions-runner-controller -c manager GITHUB_ENTERPRISE_URL=\\"$GITHUB_SERVER_URL\\" --namespace actions-runner-system\\n\\n```\\n\\n\\n### 3. Configure the GitHub App Authentication\\n\\n- The GitHub App data (that you can get from the GitHub App settings page):\\n    - The App ID ( `$GITHUB_APP_ID`)\\n    - The App Private Key file ( `$GITHUB_APP_PRIVATE_KEY_FILEPATH` )\\n    - The APP Installation ID ( `$GITHUB_APP_INSTALLATION_ID` ). To get the Application Installation ID, go to the Application Settings Page, and click on the **Install App** button. Then click on the **Configure** button. You will find the Installation ID in the URL of the page. (e.g. `https://github.tugdualgrall.com/organizations/{ORG}/settings/installations/{INSTALL_ID}` )\\n\\n\\n\\n```bash\\n\\n$ kubectl create secret generic controller-manager \\\\\\n    -n actions-runner-system \\\\\\n    --from-literal=github_app_id=${GITHUB_APP_ID} \\\\\\n    --from-literal=github_app_installation_id=${GITHUB_APP_INSTALLATION_ID} \\\\\\n    --from-file=github_app_private_key=${GITHUB_APP_PRIVATE_KEY_FILEPATH}\\n\\n```\\n\\n\\n### 4. Deploy and Test your first Runner\\n\\nYou are now ready to register your runners to your organization. For this, you need to create a `RunnerDeployment` resource.\\n\\nI like to put all my runners in a separate namespace, so let\'s create it with the following command:\\n\\n```bash\\n> kubectl create namespace runners\\n\\n```\\n\\nLet\'s create a new file `runner-001.yml` with the following content:\\n\\n```yaml\\n\\napiVersion: actions.summerwind.dev/v1alpha1\\nkind: RunnerDeployment\\nmetadata:\\n  name: runner-001\\n  namespace: runners\\nspec:\\n  replicas: 1\\n  template:\\n    spec:\\n      organization: demo\\n      labels:\\n        - arc\\n        - kubernetes\\n        - gke\\n      group: Default\\n    \\n```\\n\\n\\n```bash\\n> kubectl apply -f runner-001.yml\\n\\n```\\n\\nThis command will deploy a runner in the `runners` namespace, and register it to the `demo` organization. .\\n\\n\\nYou can check the status of the runner with the following command:\\n\\n```bash\\n> kubectl get runner -n runners\\n```\\n\\n```bash\\nNAME                     ENTERPRISE   ORGANIZATION   REPOSITORY   GROUP     LABELS                       STATUS    MESSAGE   AGE\\nrunner-001-hp7jc-zhs66                demo                        Default   [\\"arc\\",\\"kubernetes\\",\\"gke\\"]   Pending             3s\\n```\\n\\n\\nOnce the runner is ready, you can check the organization settings page, and you will see the new runner:\\n\\n![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/org-001.png)\\n\\n\\nYou can now use your runner, for this, create a new workflow file `.github/workflows/test.yml` in one of your repositories, and add the following content:\\n\\n\\n```yaml\\nname: \\"Hello World\\"\\non:\\n    workflow_dispatch:\\n\\njobs:\\n  build:\\n    runs-on: [ arc, kubernetes]\\n    steps:\\n      - name: Hello world action step\\n        run: echo Hello world!\\n```\\n\\nOnce you have manually triggered the workflow, you will see the runner active on the organization settings page:\\n\\n![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/org-003.png)\\n\\n\\n\\nYou can increase the number of runners by updating the `replicas` value in the `RunnerDeployment` resource: `replicas: 2`, once you have updated the resource, you will see the new runner in the organization settings page.\\n\\n![](/images/posts/github-self-hosted-runner-autoscaling-with-kubernetes/org-002.png)\\n\\n\\n\\nThese runners are ephemeral, which means that they are waiting for a Job, and once the Jon is done the Pod used by the runner is deleted and recreated.\\n\\n\\n### 5- About the Runner Images\\n\\nThe Actions Runner Controller provides three Runner container images, available on Docker Hub and [GitHub Container Registry](https://github.com/orgs/actions-runner-controller/packages?repo_name=actions-runner-controller):\\n\\n- [actions-runner](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/runner/actions-runner.dockerfile)\\n- [actions-runner-dind](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/runner/actions-runner-dind.dockerfile)\\n- [actions-runner-dind-rootless](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/runner/actions-runner-dind-rootless.dockerfile)\\n\\nThe `actions-runner` image is the default image used by the controller, it is a minimal image with the runner installed. The other images are based on the `actions-runner` image, and add `Docker in Docker` capabilities.\\n\\nNote: \\n\\nAs you may have seen the runner pods have 2 containers, the `runner` container, and the `dind` container. The `dind` container to use docker.\\n \\n```bash\\nkubectl get pods -n runners -o jsonpath=\\"{.items[*].spec.containers[*].image}\\" |\\\\\\n  tr -s \'[[:space:]]\' \'\\\\n\' |\\\\\\n  sort |\\\\\\n  uniq -c\\n```\\n\\n\\nResults:\\n\\n```bash\\n   2 docker:dind\\n   2 summerwind/actions-runner:latest\\n```\\n\\nWhen using Docker in Docker \\"DinD, you need privileged access to the host; so if you are running in an environment where it is not possible, you can run a separate VM to have a custom runner dedicated to Docket tasks and actions. You can also test the [`actions-runner-dind-rootless` image](https://github.com/actions-runner-controller/actions-runner-controller/pkgs/container/actions-runner-controller%2Factions-runner-dind-rootless), see [documentation](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/docs/detailed-docs.md#runner-with-rootless-dind) - I have not tested this image yet.\\n\\n\\n## Conclusion\\n\\nIn this blog post, we have seen how to deploy a self-hosted runner on Kubernetes, and how to use it to run GitHub Actions workflows:\\n- install the Actions Runner Controller\\n- configure the GitHub App Authentication\\n- deploy and test your first runner\\n\\nThe [Actions Runner Controller documentation](https://github.com/actions-runner-controller/actions-runner-controller/blob/master/docs/detailed-docs.md) contains additional information about the various resources and options available.\\n\\n\\n\\n### \ud83c\udfa5 Video\\n\\nSee the Actions Runner Controller in action in this video:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/y4BKdLjKEvs\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2022/02/08/getting-started-with-github-releases","metadata":{"permalink":"/blog/2022/02/08/getting-started-with-github-releases","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2022-02-08-getting-started-with-github-releases.md","source":"@site/blog/2022-02-08-getting-started-with-github-releases.md","title":"Getting Started with GitHub Releases","description":"In the following video, I show how to create a new product release with GitHub. This example shows how to:","date":"2022-02-08T00:00:00.000Z","formattedDate":"February 8, 2022","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"actions","permalink":"/blog/tags/actions"},{"label":"automation","permalink":"/blog/tags/automation"},{"label":"release","permalink":"/blog/tags/release"},{"label":"packages","permalink":"/blog/tags/packages"},{"label":"documentation","permalink":"/blog/tags/documentation"}],"readingTime":0.36,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Getting Started with GitHub Releases","tags":["github","actions","automation","release","packages","documentation"]},"prevItem":{"title":"GitHub Self-Hosted Runner Autoscaling with Kubernetes","permalink":"/blog/2022/10/16/github-self-hosted-runner-autoscaling-with-kubernetes"},"nextItem":{"title":"Use Private Actions with your Team","permalink":"/blog/2022/01/24/share-private-actions-enterprise"}},"content":"In the following [video](https://www.youtube.com/embed/Gw2vB18X3sQ), I show how to create a new product release with GitHub. This example shows how to:\\n- [Create a new release](https://docs.github.com/en/repositories/releasing-projects-on-github), and generate [release notes](https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes) from PRs\\n- Upload a Maven artifact into GitHub Packages with [Actions](https://docs.github.com/en/actions/publishing-packages/publishing-java-packages-with-maven)\\n- Generate new documentation site with [GitHub Pages](https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages)\\n- Use GitHub [Discussions](https://docs.github.com/en/discussions) to involve your community.\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/Gw2vB18X3sQ\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2022/01/24/share-private-actions-enterprise","metadata":{"permalink":"/blog/2022/01/24/share-private-actions-enterprise","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2022-01-24-share-private-actions-enterprise.md","source":"@site/blog/2022-01-24-share-private-actions-enterprise.md","title":"Use Private Actions with your Team","description":"A long-awaited feature is now available on GitHub: share actions in your workflows in your enterprise from Internal Repositories \ud83e\udd16 !","date":"2022-01-24T00:00:00.000Z","formattedDate":"January 24, 2022","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"actions","permalink":"/blog/tags/actions"},{"label":"automation","permalink":"/blog/tags/automation"},{"label":"javascript","permalink":"/blog/tags/javascript"}],"readingTime":0.735,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Use Private Actions with your Team","tags":["github","actions","automation","javascript"]},"prevItem":{"title":"Getting Started with GitHub Releases","permalink":"/blog/2022/02/08/getting-started-with-github-releases"},"nextItem":{"title":"Write GitHub Action: Checks and Annotations API (Part 2)","permalink":"/blog/2021/11/07/how-to-write-a-github-action-annotation-api"}},"content":"A long-awaited feature is now available on GitHub: share **actions** in your workflows in your enterprise from Internal Repositories \ud83e\udd16 !\\n\\nWhen a developer does not find a suitable GitHub Action in the [Marketplace](http://github.com/marketplace), they can create their own, something that I have documented in the \\"[How to Write Custom GitHub Action](/blog/2021/10/30/how-to-write-a-github-action)\\" post.\\n\\nUntil today, Actions had to be stored in a public repository, to allow other repositories to use them in workflows *(or take a complex approach)*. This has changed \ud83c\udf89.\\n\\nNow you can put your actions in an [internal](https://docs.github.com/en/enterprise-cloud@latest/repositories/creating-and-managing-repositories/about-repositories#about-internal-repositories) repository; repositories visible to your whole enterprise.\\n\\nOnce your repository visibility is set to \\"Internal\\", in the **Settings** pages, click in the **Actions** section and select the proper **Access** configuration:\\n\\n![GitHub Actions Access](/images/posts/share-private-actions-enterprise/01-action-access.png)\\n\\nThe following video shows this feature in action:\\n\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/IC58NsaiOuI\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2021/11/07/how-to-write-a-github-action-annotation-api","metadata":{"permalink":"/blog/2021/11/07/how-to-write-a-github-action-annotation-api","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2021-11-07-how-to-write-a-github-action-annotation-api.md","source":"@site/blog/2021-11-07-how-to-write-a-github-action-annotation-api.md","title":"Write GitHub Action: Checks and Annotations API (Part 2)","description":"In the first blog post of this series, I have explained how to create a custom GitHub Action that is interesting when you do not find the action that you need on the GitHub Marketplace.","date":"2021-11-07T00:00:00.000Z","formattedDate":"November 7, 2021","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"actions","permalink":"/blog/tags/actions"},{"label":"automation","permalink":"/blog/tags/automation"},{"label":"javascript","permalink":"/blog/tags/javascript"}],"readingTime":4.33,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Write GitHub Action: Checks and Annotations API (Part 2)","tags":["github","actions","automation","javascript"]},"prevItem":{"title":"Use Private Actions with your Team","permalink":"/blog/2022/01/24/share-private-actions-enterprise"},"nextItem":{"title":"How to Write Custom GitHub Action (Part 1)","permalink":"/blog/2021/10/30/how-to-write-a-github-action"}},"content":"In the [first blog post](/blog/2021/10/30/how-to-write-a-github-action) of this series, I have explained how to create a custom GitHub Action that is interesting when you do not find the action that you need on the [GitHub Marketplace](https://github.com/marketplace).\\n\\nI will now focus on some interesting API that you can use when building an action: [Checks](https://docs.github.com/en/rest/reference/checks#create-a-check-run) & [Annotations](https://docs.github.com/en/rest/reference/checks#update-a-check-run).\\n\\n\\nIt is import, when a workflow is running to provide visual feedback to the user. This is where the checks and annotation API is handy as it allows you for example indicates to the user that a specific step has failed ( \u274c ) or was successfully ( \u2705 ) executed; and using the API it is also possible to create a detailed annotation that points to a specific line of code; this helping the user to understand what is going on in the workflow.\\n\\nThe following screenshot shows the annotation API in action:\\n\\n![Workflow Annotations](/images/posts/how-to-write-a-github-action/004-annotations.png)\\n\\n\\n\\n\ud83d\udcd7 In this second post, you will learn how to:\\n\\n1. Create custom Checks\\n2. Add some detailed annotation with reference to source code (lines) with error\\n3. Deploy the action\\n\\nIf you prefer the video version of this post go to [Github Actions: Create custom Checks and Annotations\\n](https://www.youtube.com/watch?v=NF7kqYowzLA).\\n\\nIt is time now to dive into the example!\\n\\n\x3c!--truncate--\x3e\\n\\n\\nIn this new example, you will simply add few lines of code to the action that you have created in the previous post. If you have not yet created the action, please refer to the previous post or take the source from the [v1.0.0 release of the `tgrall/check-files-action`](https://github.com/tgrall/check-files-action/tree/v1.0.0).\\n\\nIn the first version, the action simply use `core.info()` to log a message or `core.setFailed()` to raise an annotation. \\n\\nThe goal now, is to check if the `README.md` file starts with a title, and if not, raise create an annotation that point to the first line of the file. *(As mentioned in the previous post, I am not trying to create an action that you can reuse directly but use this to discover and learn the API related to Actions & Workflows)*\\n\\n\\n## Check if the `README.md` file starts with a title\\n\\n\\nSo we need to add a new function in our action that check if the file starts with a title *(a markdown header using the `#` character)*.\\n\\n### Creating a new function to test the content of the `README.md` file\\n\\n\\nI have used GitHub Copilot to create a new function that read the file and chec if it starts with a title, inclusing a small regular expression to remove all empty lines. \\n\\n```js {7,9} \\n// create a function that checks if the file starts with a markdown header\\nasync function checkFileStartsWithHeader(filePath) {\\n    return fs.promises.readFile(filePath, \'utf8\')\\n    .then(fileContent => {\\n\\n        // remove all empty lines ad the beginning of the file\\n        fileContent = fileContent.replace(/^\\\\s*\\\\n/gm, \'\');\\n\\n        if (fileContent.startsWith(\'#\')) {\\n            core.info(`File ${filePath} starts with a header`);\\n            return true;\\n        } else {\\n            core.setFailed(`File ${filePath} does not start with a header`);\\n            return false;\\n        }\\n    });\\n}\\n\\n```\\n\\nThis function receive the path to the file, open and read the content, then:\\n\\n- on line 7, remove all the empty lines of the file using a regular expression generated by [GitHub Copilot](https://copilot.github.com). *(See the video to view it in action; also I remove all the empty line for simplicity)*\\n- on line 9, the function checks if the string start with a `#` character. If it does, it returns true, otherwise it returns false.\\n\\nI kept the comments in the snippet to let you test [GitHub Copilot](https://copilot.github.com) if you have access to technical preview.\\n\\n### Calling the new function\\n\\nBefore testing the new version of the action, you must call this method from the main method of the action.\\n\\n```js\\n...\\n    checkFileStartsWithHeader(\\"README.md\\");\\n...\\n```\\n\\n\\n## Creating a Check with Annotation\\n\\nNow that the aciton has been tested, I will create a new check and annotation. \\n\\n```js {3,6-7,11-33}\\n    ...\\n        if (\\n            ! await checkFileStartsWithHeader(\\"README.md\\")\\n        ) {\\n            // get token for octokit\\n            const token = core.getInput(\'repo-token\');\\n            const octokit = new github.getOctokit(token);\\n\\n\\n            // call octokit to create a check with annotation and details\\n            const check = await octokit.rest.checks.create({\\n                owner: github.context.repo.owner,\\n                repo: github.context.repo.repo,\\n                name: \'Readme Validator\',\\n                head_sha: github.context.sha,\\n                status: \'completed\',\\n                conclusion: \'failure\',\\n                output: {\\n                    title: \'README.md must start with a title\',\\n                    summary: \'Please use markdown syntax to create a title\',\\n                    annotations: [\\n                        {\\n                            path: \'README.md\',\\n                            start_line: 1,\\n                            end_line: 1,\\n                            annotation_level: \'failure\',\\n                            message: \'README.md must start with a header\',\\n                            start_column: 1,\\n                            end_column: 1\\n                        }\\n                    ]\\n                }\\n            });\\n        }\\n    ...\\n           \\n```\\n\\n- line 3: call the function to check if the file starts with a title, and if not, raise an annotation\\n- lines 6-7: get the token for octokit, this will allow the action to call the GitHub REST API (see line 11)\\n-  line 11: call the GitHub REST API to create a check with annotation and details.\\n\\nThis brief call is creating a new Check with an Annotation to point the user to the proper file and lines, as you can see below, the user is informed that the line 1 in `README.md` should be a title.\\n\\n![Workflow Annotations](/images/posts/how-to-write-a-github-action/005-annotations-result.png)\\n\\n\\n\\nThe annotation objects are defined in the [GitHub API documentation](https://docs.github.com/en/rest/reference/checks#update-a-check-run--parameters).\\n\\n\\n\\n## Video\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/NF7kqYowzLA\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2021/10/30/how-to-write-a-github-action","metadata":{"permalink":"/blog/2021/10/30/how-to-write-a-github-action","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2021-10-30-how-to-write-a-github-action.md","source":"@site/blog/2021-10-30-how-to-write-a-github-action.md","title":"How to Write Custom GitHub Action (Part 1)","description":"Automation is a key element of modern software development and deployment. GitHub with GitHub Actions allows you to automate many tasks, starting with your continuous integration and continuous deployment... but GitHub Actions a lot more than a CI/CD, you can use it for provisioning your environments, automating some project management tasks. However, it is not the purpose of this post, where I want to focus on the development of your own Github Action!","date":"2021-10-30T00:00:00.000Z","formattedDate":"October 30, 2021","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"actions","permalink":"/blog/tags/actions"},{"label":"automation","permalink":"/blog/tags/automation"},{"label":"javascript","permalink":"/blog/tags/javascript"}],"readingTime":9.055,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"How to Write Custom GitHub Action (Part 1)","tags":["github","actions","automation","javascript"]},"prevItem":{"title":"Write GitHub Action: Checks and Annotations API (Part 2)","permalink":"/blog/2021/11/07/how-to-write-a-github-action-annotation-api"},"nextItem":{"title":"Simple caching service with Redis","permalink":"/blog/2020/05/16/simple-caching-service-with-redis"}},"content":"Automation is a key element of modern software development and deployment. GitHub with GitHub Actions allows you to automate many tasks, starting with your **continuous integration** and **continuous deployment**... but GitHub Actions a lot more than a CI/CD, you can use it for provisioning your environments, automating some project management tasks. However, it is not the purpose of this post, where I want to focus on the development of your own Github Action!\\n\\nAn \\"Action\\" is the reusable component of a workflow, and when you create your automation you will start by searching the [GitHub Marketplace](https://github.com/marketplace?type=actions) to look for actions to achieve a specific task. In addition to the thousands of actions available on the marketplace, and available in open source communities, you can create your own actions. \\n\\nThis blog post will guide you using a concrete example, through the steps to create your actions, and this is just \\"my version\\" of the official [Creating Actions](https://docs.github.com/en/actions/creating-actions) documentation chapter.\\n\\n\\nLet\'s say for example that you want to enforce the fact that your repositories have always a `README.md `and a `LICENSE` file. And when the repository is not compliant with these rules the workflow should fail and provide clear information to the user.\\n\\nThe following screenshot shows messages and alerts raised by the actions during an integration workflow:\\n\\n\\n![Workflow Checks](/images/posts/how-to-write-a-github-action/001-checks.png)\\n\\n\ud83d\udcd5 In this first post, you will learn how to:\\n\\n1. Create an action\\n2. Publish the action\\n3. Use the action in a workflow\\n4. Add some logic to control the workflow\'s success or failure.\\n\\n\\n\ud83d\udcd7 In a second post, you will learn how to:\\n\\n1. Create custom Checks\\n2. Add some detailed annotation with reference to source code (lines) with error\\n3. Deploy the action\\n\\nIf you prefer a video version of it, take a look to \\"[Build Your Own Action](https://www.youtube.com/embed/kKbIEPsLj88)\\" on YouTube.\\n\\n\\nIt is time now to dive into the example!\\n\\n\x3c!--truncate--\x3e\\n\\n> *Note: the goal is not to create a real action that you can use as it is in your workflows, but to focus on the development an Action and use GitHub APIs.*\\n\\n\\n## Create a Javascript Action\\n\\nYou can write an action in Javascript/TypeScript, or using a Docker Container. In this example, you will learn how do it in  Javascript.\\n\\n\\n### 1- Add a new Github Repository\\n\\nThe action will be located in a public repository to be consumed by the workflows, so let\'s create a new repository.\\n\\nI am using the https://cli.github.com/, if you do not have it, you can create the repository from GitHub Website and clone the repository locally.\\n\\n```sh\\n\\ngh repo create <YOUR-ORG>/check-files-action \\\\\\n               --public \\\\\\n               --gitignore \\"Node\\" \\\\\\n               --license \\"ISC\\" \\\\\\n               --enable-wiki=false \\\\\\n               --description \\"My first GitHub Action\\" \\\\\\n               -y\\n\\n\\n```\\n\\nThis the [`gh repo create`](https://cli.github.com/manual/gh_repo_create) command, with these parameters, creates the repository and clone it in one step.\\n\\nYou can now open the `check-files-action` folder with your favorite editor, in my case VSCode\\n\\n```sh\\n\\ncode check-files-action\\n\\n```\\n\\nIf you are interested to see the sample code you can look at this repository:\\n- https://github.com/tgrall/check-files-action\\n\\n### 2- Initialize the Node project & add dependencies\\n\\n2.1 - In the project directory, run the `npm` command to initialize the application\\n\\n```sh\\nnpm init -y\\n```\\n\\n2.2 - Add Dependencies\\n\\nGitHub Actions comes with a toolkit to interact with the GitHub and Workflows APIs:\\n\\n- [`@actions/core`](https://www.npmjs.com/package/@actions/core) : Core functions for setting results, logging, registering secrets, and exporting variables across actions\\n- [`@actions/github`](https://www.npmjs.com/package/@actions/github) : returns an authenticated Octokit client. See https://octokit.github.io/rest.js for the API.\\n\\nInstall these dependencies with the following commands:\\n\\n```sh\\n\\nnpm install @actions/core @actions/github\\n\\n```\\n\\n### 3- Update the .gitignore file\\n\\nThe Workflow runners will use your project repository to get the code and execute it directly. So all information needed for the execution of the action must be present.\\n\\nThis is why you must keep the `node_modules` and related directories (`dist`).\\n\\n\ud83d\udea8 We need to be sure that the dependencies are part of the project, so for this, we need to remove the `node_modules` and `dist` entries from the `.gitignore` file.\\n\\nSo edit the .gitignore file and comment the node_modules line:\\n\\n```yaml\\n...\\n# Nuxt.js build / generate output\\n# dist\\n...\\n\\n# Dependency directories\\n# node_modules/\\n\\n...\\n```\\n\\n### 4- Add the metadata and main Javascript files\\n\\nThe action is define by an `action.yml` located in the root directory of your project.\\n\\n```yaml\\nname: \'File Check Action\'\\ndescription: \'An action that checks if the LICENSE and README.md files exists\'\\nruns:\\n  using: \'node12\'\\n  main: \'index.js\'\\n```\\n\\nIn the next blog post, I will go into more detail about this configuration file. For now, let\'s keep it as simple as possible.\\n\\nAs defined in the `action.yml`, you need to create an `index.js` file. This js file is the entry point for the logic of your action.\\n\\n```js\\nconst core = require(\'@actions/core\');\\nconst github = require(\'@actions/github\');\\n\\n(async () => {\\n    try {\\n        core.notice(\'Check File Action called!!!\');\\n    } catch (error) {\\n        core.setFailed(error.message);\\n    }\\n})();\\n```\\n\\nThis first version of the Action is simply printing a message when called by a workflow.\\n\\n### 5- \\"Deploy\\" the action\\n\\nAs mentioned before, a workflow references Action using the Github repository location. This means that when I talk about deployment, it is simply pushing the code in the repository.\\n\\n```sh\\n\\ngit add .\\n\\ngit commit -m \\"first action test\\"\\n\\ngit push origin main\\n\\n```\\n\\n\\n## Use the action in a workflow\\n\\nEven if the action is not that exciting, we can already use it in a workflow and learn more about it.\\nWe could use the same repository, but I like it to be more representative of a real-life use-case, to create a new repository with a simple workflow.\\n\\n### 1- Create a simple repository\\n\\nSame as earlier, let\'s create a new repository, this time private, and we will not add any license file.\\n\\n```\\n\\ngh repo create <YOUR-ORG>/to-test-my-action \\\\\\n               --private \\\\\\n               --description \\"To test my actions\\" \\\\\\n               -y\\n\\n```\\n\\n\\n### 2- Create a simple workflow\\n\\n**\ud83d\udca1 Tip: An easy way to add a workflow:**\\n\\nYou can open the newly created repository using the GitHub CLI:\\n\\n```sh\\ngh repo view --web <YOUR-ORG>/to-test-my-action \\n```\\n\\n2.1 - Click on the **Actions** tab, and click on the **Simple workflow** suggestion\\n\\nThis will generate the following content:\\n\\n```yml\\n\\nname: Simple Workflow\\n\\n# Controls when the workflow will run\\non:\\n  # Triggers the workflow on push or pull request events but only for the main branch\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\n  # Allows you to run this workflow manually from the Actions tab\\n  workflow_dispatch:\\n\\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\\njobs:\\n  # This workflow contains a single job called \\"build\\"\\n  build:\\n    # The type of runner that the job will run on\\n    runs-on: ubuntu-latest\\n\\n    # Steps represent a sequence of tasks that will be executed as part of the job\\n    steps:\\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\\n      - uses: actions/checkout@v2\\n\\n      # Runs a single command using the runners shell\\n      - name: Run a one-line script\\n        run: echo Hello, world!\\n\\n      # Runs a set of commands using the runners shell\\n      - name: Run a multi-line script\\n        run: |\\n          echo Add other actions to build,\\n          echo test, and deploy your project.\\n\\n\\n```\\n\\n\\n2.2 -  **Adding our action to the Workflow**\\n\\nBefore saving the file, add the following entry to add our newly created action to the workflow:\\n\\n```\\n      # Call my powerful action!\\n      - name: Check Files\\n        uses: <YOUR-ORG>/check-files-action@main\\n\\n```\\n\\nAs you can see it is very simple, just add \\n- `uses: <YOUR-ORG>/check-files-action@main`\\n\\nTo point to the action with the proper tag (in our case for now we are using `main` branch).\\n\\n### 3- Call the workflow\\n\\nTo call the workflow, you can simply commit/push the file to the repository. \\n\\nThe workflow should be automatically executed since it is triggered by any push on the `main` branch and related pull-request.\\n\\nSee the top of the workflow file:\\n\\n```yml\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n```\\n\\n### 4- Check the execution\\n\\n\\nThe workflow has been executed, so you should be able to see the result of it in the **Actions** tab, click on the workflow instance and you will see an annotation, like the following screenshot.\\n\\n\\n![Workflow Checks](/images/posts/how-to-write-a-github-action/002-workflow.png)\\n\\n\\n\ud83c\udf89 - Congratulations, you have created your first GitHub Action!\\n\\nLet\'s now add some logic to it.\\n\\n\\n## Add some logic to the action\\n\\nNow that we have the \\"core\\" of our action we need to add some logic to checl if the files exists and check the content of the README.file.\\n\\nThen if the file are not correct we should create some checks that will be visible to the developer.\\n\\n\\n### 1- Edit the index.js to add some functions\\n\\nIn the index.js file let\'s add a function that test is a file is present:\\n\\n- if presents just log the information\\n- if not present, the job fails and an annotations is added\\n\\n\\n```js {9,13} \\nconst core = require(\'@actions/core\');\\nconst github = require(\'@actions/github\');\\nconst fs = require(\\"fs\\");\\nconst { connected } = require(\'process\');\\n\\nasync function checkFileExistence(path) {\\n    return fs.promises.access(path, fs.constants.F_OK)\\n    .then(() => {\\n        core.info(`${path} exists`);\\n        return true;\\n    })\\n    .catch(() => {\\n        core.setFailed(`${path} does not exist`);\\n        return false;\\n    });\\n  }\\n\\n(async () => {\\n    try {\\n   \\n        checkFileExistence(\\"README.md\\");\\n        checkFileExistence(\\"LICENSE\\");\\n        \\n    } catch (error) {\\n        core.setFailed(error.message);\\n    }\\n})();\\n\\n```\\n\\nThe highlighted lines shows how to use the GitHub Action core API to log some information in the console, and more \\"importantly\\" how to fail with an error message, that will be visible\\n\\n- in the console \\n- as an annotation attached to the workflow execution.\\n\\n### 2- Test the new code\\n\\nOnce you have modify `the index.js`, commit and push your action code to main.\\n\\nThen go to the workflow that you have used to test, and re-run it, or invoke it manually.\\n\\nYou will see, depending of your repository that the workflow fails with some messages. In the following image, the `README.md` is present but not the `LICENSE` one, so the workflow fails.\\n\\n![Workflow Checks](/images/posts/how-to-write-a-github-action/003-workflow.png)\\n\\n\ud83c\udf89 - Congratulations, your Action is checking the validity of the repository.\\n\\n\\n\\nI invite you to do some test, for example add a README.md file, run the workflow, add a LICENSE, ...\\n\\nYou will see that when the two files are present, the wofklow execution is successul.\\n\\n## Conclusion\\n\\nIn this first article you have learned how to:\\n\\n- Create a Javascript action\\n- Deploy and use it in a workflow\\n- Add some user feedback using GitHub Action core API.\\n\\nIn the next article we will add more logic to check the content of the `README.md` file and learn how to create detailed checks and annotations.\\n\\n\\nIf you have questions, commemnts about this article, you can use the following [GitHub Discussion](https://github.com/tgrall/tgrall.github.io/discussions/7).\\n\\n\\n## Video\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/kKbIEPsLj88\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2020/05/16/simple-caching-service-with-redis","metadata":{"permalink":"/blog/2020/05/16/simple-caching-service-with-redis","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2020-05-16-simple-caching-service-with-redis.md","source":"@site/blog/2020-05-16-simple-caching-service-with-redis.md","title":"Simple caching service with Redis","description":"One of the most common use cases for Redis is to use it the database as a caching layer for your data, but Redis can do a lot more (I will publish new articles later)!","date":"2020-05-16T00:00:00.000Z","formattedDate":"May 16, 2020","tags":[{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"redis","permalink":"/blog/tags/redis"},{"label":"microservices","permalink":"/blog/tags/microservices"}],"readingTime":6.795,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Simple caching service with Redis","tags":["nosql","redis","microservices"]},"prevItem":{"title":"How to Write Custom GitHub Action (Part 1)","permalink":"/blog/2021/10/30/how-to-write-a-github-action"},"nextItem":{"title":"How to use SSL/TLS with Redis Enterprise","permalink":"/blog/2020/01/02/how-to-use-ssl-slash-tls-with-redis-enterprise"}},"content":"![](/images/posts/simple-caching-with-redis/001-ws-caching.png)\\n\\n\\nOne of the most common use cases for Redis is to use it the database as a caching layer for your data, but Redis can do a lot more *(I will publish new articles later)*!\\n\\nIn this article, you will learn using a straightforward service, how to cache the result on some REST API calls to accelerate the data access, and also reduce the number of calls to external services.\\n\\nFor this example, I am using the \\"Redis Movie Database\\" application, a microservice-based application that I created to showcase and explain various features of Redis and Redis Enterprise. \\n\\n\x3c!--truncate--\x3e\\n\\n\\nYou can see the caching service in action in this video:\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/2X6hmXGbLbg\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n\\n### Architecture Overview\\n\\nThe application uses a third party API provided by the \\"[OMDb API](http://www.omdbapi.com/)\\" to retrieve the ratings of the movie using its IMDb identifier. The frontend application call the `/caching/rating/` service to get the rating information from OMDB.\\n\\n\\nThis service is doing the following:\\n\\n1. Check if the rating data is already cached retrieve from the cache\\n2. If the information is not cached, the system calls the OMDB API with the proper key and Movie ID\\n3. The result is cached in Redis with a time to live of 120 seconds\\n4. The ratings are returned to the client.\\n\\n\\n**Prerequisites**\\n\\n* Redis 5.x or later.\\n* Java 8 or later\\n* Apache Maven 3.6\\n* Git\\n\\n### Implementation\\n\\nIn the microservice demonstration project, you can find the caching service in the project below:\\n\\n* [Caching Service](https://github.com/tgrall/redis-microservices-demo/tree/master/caching-service)\\n\\n\\nThe Spring Boot application exposes a REST endpoint [RestStatusController.java](https://github.com/tgrall/redis-microservices-demo/blob/master/caching-service/src/main/java/io/redis/demos/services/caching/RestStatusController.java) with the following key features:\\n\\n* `/api/1.0/caching/configuration/omdb_api` : to save the OMDb API key in Redis.\\n* `/api/1.0/caching/ratings/{id}` : to retrieve the IMDB Rating information.\\n\\n#### Data Structure\\n\\nThe Caching Service is pretty simple and using two sets of keys:\\n\\n* `ms:config`: is a Redis Hash that will be used to store all global configuration; for the caching service, the entry `OMDB_API_KEY` will contain the OMDb API key.\\n* `ms:cache:ws:*` : one entry for each movie viewed by a user, where the IMDb movie id replaces the `*`. The hash contains the ratings, and each of the Movie Rating sites is a key in the hash (\\"Internet Movie Database\\",  \\"Rotten Tomatoes\\", \\"Metacritic\\"), and the value is the rating itself.\\n\\n\\n#### Code\\n\\nThe implementation of the caching layer is simple and located in the class below:\\n\\n* [`WebServiceCachingService`](https://github.com/tgrall/redis-microservices-demo/blob/master/caching-service/src/main/java/io/redis/demos/services/caching/service/WebServiceCachingService.java)\\n\\n**Getting the Redis Connection**\\n\\nIn this project, I have not used Redis/Spring integration *by choice*, I am only using Spring injection and Spring Boot Web features.\\n\\n* the `afterConstruct()` method creates the JedisPool from the Spring Configuration\\n* then each time a connection is needed, the application calls `Jedis jedis = jedisPool.getResource()` to get a connection from the pool.\\n\\n\\n#### Setting & Getting the OMDb API Key\\n\\nAs you have seen earlier, the OMDb API Key is stored in a hash associated with the Redis key `ms:config`.\\n\\n* The method `saveOMDBAPIKey` is used to store the configuring with a  `hset` call.\\n\\n```java\\n    jedis = jedisPool.getResource();\\n    jedis.hset(KEY_CONFIG, OMDB_API_KEY, key);\\n    omdbAPIKEY = key;\\n```\\n\\n* line 1: get the connection from the pool\\n* line 2: set the key in the hash *(as you can see the application uses static variables (`KEY_CONFIG` and `OMDB_API_KEY`)\\n* line 3: the key received as a method parameter is set to a class member `omdbAPIKEY` to avoid calling the hash each time.\\n\\n\\n#### Calling OMDb API and Caching (or not) the result\\n\\nThe method `getRatings` receives two parameters:\\n\\n* `imdbId` the IMDb id\\n* `withCache` a boolean value to use or not Redis. The goal here is to show as a demonstration of the benefits of using Redis.\\n\\n\\nLet\'s now look at the code:\\n\\n```java\\n String restCallKey = KEY_PREFIX + imdbId;\\n\\n        try (Jedis jedis = jedisPool.getResource()) {\\n\\n            // Look in the map to see if the value has been cached\\n            if (withCache) {\\n                returnValue = jedis.hgetAll(restCallKey);\\n            }\\n\\n            if (returnValue.isEmpty()) {\\n                returnValue.put(\\"imdb_id\\", imdbId);\\n                CloseableHttpClient httpClient = HttpClientBuilder.create().build();\\n                HttpGet getRequest = new HttpGet(url);\\n                getRequest.addHeader(\\"accept\\", \\"application/json\\");\\n                ResponseHandler<String> responseHandler = new BasicResponseHandler();\\n\\n                String WsCall = httpClient.execute(getRequest, responseHandler);\\n\\n                Map<String, Object> map = jsonMapper.readValue(WsCall, Map.class);\\n                List<Map<String, String>> ratings = (List<Map<String, String>>) map.get(\\"Ratings\\");\\n\\n                Map<String, String> ratingAsMap = new HashMap<>();\\n                for (Map<String, String> it : ratings) {\\n                    ratingAsMap.put(it.get(\\"Source\\"), it.get(\\"Value\\"));\\n                }\\n\\n                returnValue.putAll(ratingAsMap);\\n\\n                jedis.hset(restCallKey, returnValue);\\n                jedis.expire(restCallKey, TTL);\\n            }\\n\\n        } catch(HttpResponseException e){\\n            // Small hack to keep it simple\\n            returnValue.put(\\"Metacritic\\", \\"<p style=\'color:red\'>Error: OMDBAPI Key is invalid -- see services page</p>\\");\\n            omdbAPIKEY = null;\\n\\n        } catch (IOException e) {\\n            e.printStackTrace();\\n        }\\n    } else {\\n        // Small hack to keep it simple\\n        returnValue.put(\\"Metacritic\\", \\"<p style=\'color:red\'>Error: OMDBAPI Key is not set, please configure it -- see services page</p>\\");\\n    }\\n    long end = System.currentTimeMillis();\\n    returnValue.put(\\"elapsedTimeMs\\", Long.toString(end - start) );\\n    return returnValue;\\n```\\n\\nSo if you look in the code carefully you see that only a few lines are related to the cache itself:\\n* Line 1, the key is created from a prefix and the IMDb identifier.\\n* Line 3, the application retrieves a connection from the Jedis Pool.\\n* Line 7, if the cache is enabled,  the connection to get the value from Redis `returnValue = jedis.hgetAll(restCallKey)`\\n* If a value is present in the cache, the value is returned to the caller\\n* If `returnValue` is empty,  the OMDB REST service must be called (lines 11 to 25)\\n* The result of the Web service call is stored in the `returnValue` variable, and save into Redis with a time to live (TTL) of 120 seconds (Line 27 to 29)\\n* Finally, the value is returned to the caller (Line 47).\\n\\nQuite simple, no?\\n\\nIt is possible to optimize a little bit the application/code with few additions:\\n\\n* Make the TTL configurable by adding a new entry in the `ms:config` cache\\n* Use [pipelining](https://redis.io/topics/pipelining) to reduce the round trip time (RTT)\\n\\n\\n\\n### Running the application\\n\\nIn the project, the service connects to a `local` instance of Redis on port `6379`. If you want to use a different instance or configure a password/user, you have to edit the `/redis-microservices-demo/caching-service/application.properties`.\\n\\n#### Cloning and Building\\n\\n```shell\\n> git clone https://github.com/tgrall/redis-microservices-demo.git\\n\\n> cd redis-microservices-demo/caching-service\\n\\n> mvn clean package\\n\\n```\\n\\n#### Running the application\\n\\nThe application is a Spring Boot application, run the following command to start it:\\n\\n```shell\\n> mvn spring-boot:run\\n```\\n\\nThen you should save your [OMDB API key](https://www.patreon.com/join/omdb) in Redis using the following call:\\n\\n```shell\\n> curl -X POST http://localhost:8084/api/1.0/caching/configuration/omdb_api\\\\?key\\\\=[YOUR_KEY]\\n```\\n\\n\\nNow you can call the service itself to retrieve the ratings of the movie \\"WarGames\\"\\n\\n```shell\\n> curl -X GET http://localhost:8084/api/1.0/caching/ratings/tt0086567\\n```\\n\\nCall it multiple times, and you will see that the first call is *slow\\" (100ms or more). Then subsequent requests will be a lot faster, as the data are coming out of Redis. After tow minutes, the data is removed from the cache automatically (expiration), and the OMDB service will be called again.\\n\\nYou can also force the service to no use the cache using the following call:\\n\\n```shell\\n> curl -X GET http://localhost:8084/api/1.0/caching/ratings/tt0086567?cache=0\\n```\\n\\n\\n### Conclusion\\n\\nThe pattern used here is called \\"Cache-Aside\\"; and usually pretty easy to implement. It is interesting to notice that many libraries such as Spring provide built-in features to implement such caches.\\n\\nThat said, this is not a silver bullet, you still have to look at the following points when you are implementing such caching service:\\n\\n* Loading the cache: in the example, the cache is populated when the service is called. This lazy loading approach is excellent since the cache is only filled with data that are used by the application. However, the first call is paying the price of higher latency, so on your application, you may require to load the data at startup to avoid any hit miss.\\n* Cache Invalidation and Lifetime: When caching data, it is essential to look at the invalidation strategy, when and how I can update the data in the cache, but also how long the data will stay in the cache. In the example above, each data will remain for two minutes. \\n\\nSo now you are all set to implement a simple cache and have consistent fast access to your application data, independently of the backend."},{"id":"/2020/01/02/how-to-use-ssl-slash-tls-with-redis-enterprise","metadata":{"permalink":"/blog/2020/01/02/how-to-use-ssl-slash-tls-with-redis-enterprise","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2020-01-02-how-to-use-ssl-slash-tls-with-redis-enterprise.md","source":"@site/blog/2020-01-02-how-to-use-ssl-slash-tls-with-redis-enterprise.md","title":"How to use SSL/TLS with Redis Enterprise","description":"In this article, I will explain how to secure your Redis databases using SSL (Secure Sockets Layer). In production, it is a good practice to use SSL to protect the data that are moving between various computers (client applications and Redis servers). Transport Level Security (TLS) guarantees that only allowed applications/computers are connected to the database, and also that data is not viewed or altered by a middle man process.","date":"2020-01-02T00:00:00.000Z","formattedDate":"January 2, 2020","tags":[{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"redis","permalink":"/blog/tags/redis"},{"label":"security","permalink":"/blog/tags/security"},{"label":"java","permalink":"/blog/tags/java"},{"label":"node","permalink":"/blog/tags/node"},{"label":"python","permalink":"/blog/tags/python"},{"label":"authentication","permalink":"/blog/tags/authentication"},{"label":"ssl","permalink":"/blog/tags/ssl"}],"readingTime":6,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"How to use SSL/TLS with Redis Enterprise","tags":["nosql","redis","security","java","node","python","authentication","ssl"]},"prevItem":{"title":"Simple caching service with Redis","permalink":"/blog/2020/05/16/simple-caching-service-with-redis"},"nextItem":{"title":"Redis Rolling Upgrade on Pivotal Cloud Foundry (PCF)","permalink":"/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf"}},"content":"![](/images/posts/how-to-use-ssl-slash-tls-with-redis-enterprise/000_header.jpeg)\\n\\n\\nIn this article, I will explain how to secure your Redis databases using SSL (Secure Sockets Layer). In production, it is a good practice to use SSL to protect the data that are moving between various computers (client applications and Redis servers). Transport Level Security (TLS) guarantees that only allowed applications/computers are connected to the database, and also that data is not viewed or altered by a middle man process.\\n\\nYou can secure the connections between your client applications and Redis cluster using:\\n\\n* One-Way SSL: the client (your application) get the certificate from the server (Redis cluster), validate it, and then all communications are encrypted\\n* Two-Way SSL: (aka mutual SSL) here both the client and the server authenticate each other and validate that both ends are trusted.\\n\\nIn this article, I will focus on the Two-Way SSL, and using Redis Enterprise.\\n\\n\x3c!--truncate--\x3e\\n\\nPrerequisites:\\n\\n* A Redis Enterprise 5.4.x database,\xa0(my database is protected by the password\xa0`secretdb01`, and listening on port\xa0`12000`)\\n* `redis-cli`\xa0to run basic commands\\n* Python, Node, and Java installed if you want to test various languages.\\n\\n\\n**Simple  Test**\\n\\nLet\'s make sure that the database is available:\\n\\n```\\nredis-cli -p 12000 -a secretdb01 INFO SERVER\\n```\\n\\nThis should print the Server information.\\n\\n\\n#### 1- Get the Certificate from Redis Cluster\\n\\nYou have access to the Redis Enterprise Cluster, you go to one of the nodes to retrieve the certificate (that is a self-generated one by default).\\n\\nThe cluster certificate is located at: `/etc/opt/redislabs/proxy_cert.pem`.\\n\\nYou have to copy it on each client machine; note that once it is done you can use this certificate to connect using \\"One-Way SSL\\", but not the purpose of this article.\\n\\nIn my demonstration I am using Docker and copy the certificate using this command from my host:\\n\\n```\\ndocker cp redis-node1:/etc/opt/redislabs/proxy_cert.pem ./certificates\\n```\\n\\n#### 2- Generate a New Client Certificate\\n\\nUsing the Two-Way SSL you need to have a certificate for the client that will be used by Redis database proxy to trust the client.\\n\\nIn this article I will use a self-signed certificate using OpenSSL, in this example, we are creating a certificate for an application named\xa0`app_001`.\\n\\nYou can create as many certificates as you want, or reuse this one for all servers/applications.\\n\\nOpen a terminal and run the following commands:\\n\\n```bash \\n\\nopenssl req \\\\\\n  -nodes \\\\\\n -newkey rsa:2048 \\\\\\n -keyout client_key_app_001.pem \\\\\\n -x509 \\\\\\n -days 36500 \\\\\\n -out client_cert_app_001.pem\\n\\n```\\n\\nThis command generate a new client key (`client_key_001.pem`) and certificate (`client_cert_001.pem`) with no passphrase.\\n\\n\\n#### 3- Configure the Redis Database\\n\\nThe next step is to take the certificate and add it to the database you want to protect. \\n\\nLet\'s copy the certificate and paste it into the Redis Enterprise Web Console.\\n\\nCopy the certificate in your clipboard:\\n\\nMac:\\n```bash\\npbcopy < client_cert_app_001.pem\\n```\\n\\nLinux:\\n```bash\\n xclip -sel clip < client_cert_app_001.pem\\n```\\n\\nWindows:\\n```bash\\nclip < client_cert_app_001.pem\\n```\\n\\nGo to the Redis Enterprise Admin Web Console and enable TLS on your database:\\n\\n1. Edit the database configuration\\n1. Check TLS\\n1. Select \\"Require TLS for All communications\\"\\n1. Check \\"Enforce client authentication\\"\\n1. Paste the certificate in the text area\\n1. Click the Save button to save the certificate\\n1. Click the Update button to save the configuration.\\n\\n![](/images/posts/how-to-use-ssl-slash-tls-with-redis-enterprise/001-tls-configuration.png)\\n\\n\\nThe database is now protected, and it is mandatory to use the SSL certificate to connect to it.\\n\\n\\n```\\nredis-cli -p 12000 -a secretdb01 INFO SERVER\\n(error) ERR unencrypted connection is prohibited\\n```\\n\\n#### 4- Connect to the Database using the Certificate\\n\\nIn all following examples, I am using a \\"self-signed\\" certificate, so I do not check the validity of the hostname. \\nYou should adapt the connections/TLS information based on your certificate configuration.\\n\\n#### 4.1 Using Redis-CLI\\n\\nTo connect to a SSL protected database using `redis-cli` you have to use [`stunnel`](https://www.stunnel.org/index.html).\\n\\nCreate a `stunnel.conf` file with the following content:\\n\\n```\\ncert = /path_to/certificates/client_cert_app_001.pem\\nkey = /path_to/certificates/client_key_app_001.pem\\ncafile = /path_to/certificates/proxy_cert.pem\\nclient = yes\\n\\n[redislabs]\\naccept = 127.0.0.1:6380\\nconnect = 127.0.0.1:12000\\n\\n```\\nStart stunnel using the command\\n\\n```\\nstunnel ./stunnel.conf\\n```\\n\\nThis will start a process that listen to port `6380` and used as a proxy to the Redis Enterprise database on port `12000`.\\n\\n```\\nredis-cli -p 6380 -a secretdb01 INFO SERVER\\n```\\n\\n\\n##### 4.2 Using Python\\n\\nUsing Python, you have to set the SSL connection parameters:\\n\\n``` python\\n#!/usr/local/bin/python3\\n\\nimport redis\\nimport pprint\\n\\ntry:\\n  r = redis.StrictRedis(\\n    password=\'secretdb01\',\\n    decode_responses=True,\\n    host=\'localhost\',\\n    port=12000,\\n    ssl=True, \\n    ssl_keyfile=\'./client_key_app_001.pem\', \\n    ssl_certfile=\'./client_cert_app_001.pem\', \\n    ssl_cert_reqs=\'required\', \\n    ssl_ca_certs=\'./proxy_cert.pem\',\\n    )\\n\\n  info = r.info()\\n  pprint.pprint(info)\\n\\nexcept Exception as err:\\n  print(\\"Error connecting to Redis: {}\\".format(err))\\n```\\n\\nMore information in the documentation \\"[Using Redis with Python](https://redislabs.com/lp/python-redis/)\\".\\n\\n##### 4.3 Using Node.JS\\n\\nFor [Node Redis](http://redis.js.org/), use the [TLS](https://nodejs.org/api/tls.html) library to configure the client connection:\\n\\n```javascript\\nvar redis = require(\\"redis\\");\\nvar tls = require(\'tls\');\\nvar fs = require(\'fs\');\\n\\nvar ssl = {\\n    key: fs.readFileSync(\'../certificates/client_key_app_001.pem\',encoding=\'ascii\'),\\n    cert: fs.readFileSync(\'../certificates/client_cert_app_001.pem\',encoding=\'ascii\'),\\n    ca: [ fs.readFileSync(\'../certificates/proxy_cert.pem\',encoding=\'ascii\') ],\\n    checkServerIdentity: () => { return null; },\\n};\\n\\nvar client = redis.createClient(12000,\'127.0.0.1\', \\n    {\\n      password : \'secretdb01\',\\n      tls: ssl\\n    }\\n);\\n        \\nclient.info( \\"SERVER\\", function (err, reply) {\\n    console.log(reply);\\n} );\\n\\n```\\n\\nMore information in the documentation \\"[Using Redis with Node.js](https://redislabs.com/lp/node-js-redis/)\\".\\n\\n##### 4.4 Using Java\\n\\nIn Java, to be able to connect using SSL, you have to install all the certificates in the Java environment using the [keytool](https://docs.oracle.com/en/java/javase/11/tools/keytool.html) utility.\\n\\n\\nCreate a **keystore** file that stores the key and certificate you have created earlier:\\n\\n```\\nopenssl pkcs12 -export \\\\\\n  -in ./client_cert_app_001.pem \\\\\\n  -inkey ./client_key_app_001.pem \\\\\\n  -out client-keystore.p12 \\\\\\n  -name \\"APP_01_P12\\"\\n```\\n\\nAs you can see the keystore is used to store the credentials associated with you client; it will be used later with the `-javax.net.ssl.keyStore` system property in the Java application.\\n\\nIn addition to the keys tore, you also have to create a trust store, that is used to store other credentials for example in our case the redis cluster certificate.\\n\\n\\nCreate a **trust store** file and add the Redis cluster certificate to it\\n\\n```\\nkeytool -genkey \\\\\\n  -dname \\"cn=CLIENT_APP_01\\" \\\\\\n  -alias truststorekey \\\\\\n  -keyalg RSA \\\\\\n  -keystore ./client-truststore.p12 \\\\\\n  -keypass secret\\n  -storepass secret\\n  -storetype pkcs12\\n```\\n\\n```\\nkeytool -import \\\\\\n  -keystore ./client-truststore.p12 \\\\\\n  -file ./proxy_cert.pem \\\\\\n  -alias redis-cluster-crt\\n```\\n\\nThe trustore will be used later with the `-javax.net.ssl.trustStore` system property in the Java application.\\n\\nYou can now run the Java application with the following environment variables:\\n\\n```\\njava -Djavax.net.ssl.keyStore=/path_to/certificates/java/client-keystore.p12 \\\\\\n-Djavax.net.ssl.keyStorePassword=secret \\\\\\n-Djavax.net.ssl.trustStore=/path_to/certificates/java/client-truststore.p12 \\\\\\n-Djavax.net.ssl.trustStorePassword=secret \\\\\\n-jar MyApp.jar\\n```\\n\\nFor this example and simplicity, I will hard code these property in the Java code itself:\\n\\n```java\\n\\nimport redis.clients.jedis.Jedis;\\nimport java.net.URI;\\n\\npublic class SSLTest {\\n\\n    public static void main(String[] args) {\\n\\n        System.setProperty(\\"javax.net.ssl.keyStore\\", \\"/path_to/certificates/client-keystore.p12\\");\\n        System.setProperty(\\"javax.net.ssl.keyStorePassword\\", \\"secret\\");\\n\\n        System.setProperty(\\"javax.net.ssl.trustStore\\",\\"/path_to/certificates/client-truststore.p12\\");\\n        System.setProperty(\\"javax.net.ssl.trustStorePassword\\",\\"secret\\");\\n\\n        URI uri = URI.create(\\"rediss://127.0.0.1:12000\\");\\n\\n        Jedis jedis = new Jedis(uri);\\n        jedis.auth(\\"secretdb01\\");\\n\\n\\n        System.out.println(jedis.info(\\"SERVER\\"));\\n        jedis.close();\\n    }\\n\\n}\\n```\\n\\n* line 8-12, the system environment variables are set to point to the keystore and trust store (this should be externalized)\\n* line 14, the Redis URL start with `rediss` with 2 s to indicate that the connection should be encrypted\\n* line 17, set the database password\\n\\n\\nMore information in the documentation \\"[Using Redis with Java](https://redislabs.com/lp/redis-java/)\\".\\n\\n\\n## Conclusion\\n\\nIn this article, you have learned how to:\\n\\n* retrieve the Redis Server certificate\\n* generate a client certificate\\n* protect your database to enforce transport level security (TLS) with 2 ways authentication\\n* connect to the database from `redis-cli`, Python, Node and Java"},{"id":"/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf","metadata":{"permalink":"/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2019-09-19-redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf.md","source":"@site/blog/2019-09-19-redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf.md","title":"Redis Rolling Upgrade on Pivotal Cloud Foundry (PCF)","description":"Introduction","date":"2019-09-19T00:00:00.000Z","formattedDate":"September 19, 2019","tags":[{"label":"redis","permalink":"/blog/tags/redis"},{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"cloud","permalink":"/blog/tags/cloud"},{"label":"cf","permalink":"/blog/tags/cf"},{"label":"pivotal","permalink":"/blog/tags/pivotal"},{"label":"java","permalink":"/blog/tags/java"},{"label":"cluster","permalink":"/blog/tags/cluster"},{"label":"failover","permalink":"/blog/tags/failover"},{"label":"ha","permalink":"/blog/tags/ha"}],"readingTime":6.59,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Redis Rolling Upgrade on Pivotal Cloud Foundry (PCF)","tags":["redis","nosql","cloud","cf","pivotal","java","cluster","failover","ha"]},"prevItem":{"title":"How to use SSL/TLS with Redis Enterprise","permalink":"/blog/2020/01/02/how-to-use-ssl-slash-tls-with-redis-enterprise"},"nextItem":{"title":"Multi-Nodes Redis Cluster with Docker","permalink":"/blog/2019/09/04/multi-nodes-redis-cluster-with-docker"}},"content":"### Introduction\\n\\nIn this article, I will show you how to update Redis Enterprise on PCF and see how Redis Enterprise cluster will guarantee a service continuity using out of the box failover.\\n\\nIf you need a Cloud Foundry application that calls Redis automatically you can use this [project simple-redis-spring-demo-pcf](https://github.com/tgrall/simple-redis-spring-demo-pcf).\\n\\nFor this article, I will upgrade [Redis Enterprise for PCF](https://docs.pivotal.io/partners/redis-labs-enterprise-pack/index.html) from the version v5.4.2400147 to the latest version, currently v5.4.40700169.\\n\\n\x3c!--truncate--\x3e\\n\\n**Prerequisites**\\n\\n* Pivotal Cloud Foundry up & running\\n    * Administrator access to Ops Manager and Apps Manager\\n* One of more Redis databases running on PCF\\n    * My environment has2 databases in version v5.4.2400147\\n    * One wit replication (`db:4`) another one without replication (`db:5`)\\n \\n\\n### Initial Environment\\n\\nLet\'s take a look to the environment before the update; for this you can access the Redis Enterprise Cluster Management Console:\\n\\n* https://[Cluster Management Console Subdomain].[System Domain] \\n* for example https://console-redis.sys.my-domain.cf-app.com . \\n\\n> Do not use this to create/delete a database, you must use Cloud Foundry to do it. (`cf` command or UI)\\n\\n\\nIn the Web console, go to \\"Cluster\\" then \\"Configuration\\", you can see the version of Redis Labs Enterprise Cluster (5.4.0-24), and Redis (5.0.2) versions.\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-001-webui-cluster-version.png)\\n\\nYou can also use the `rladmin` command line to achieve this.\\n\\n**Checking Redis cluster using the command line**\\n\\nSSH to your Ops Manager and, `bosh ssh` to one of the Redis cluster VMs.\\n\\nWhen I run the `bosh vms` command on my environment I can see the following VMs related to my Redis deployment:\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-002-redis-vms-list.png)\\n\\nThe deployment is made of 5 VMs:\\n\\n* the 3 first VMs are the Redis Nodes\\n* the 2 others are related to the PCF integration (Registrar and Service Broker)\\n\\nWe can look in more details into the role of each VMS in the cluster, for this I will `bosh ssh` into one of the nodes:\\n\\n```\\n$ bosh -d redis-enterprise-[your-deployment-id] ssh redis-enterprise-node/[your-vm-id]\\n```\\n\\nOnce connected use the `sudo rladmin status` to look at the Redis cluster deployed on PCF.\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-003-rladmin-view.png)\\n\\nIn this cluster you see:\\n\\n* in the ***Cluster Nodes*** section that we have 3 nodes in version 5.4.0-24\\n* in the ***Databases*** section that we have 2 database instances, the name is generated by Cloud Foundry. In this environment, the `db:4` is replicated with shards on `node:1` (master) and `node:2` (slave/replica), while `db:5` is not replicated.\\n\\n\\nLet\'s now see the Redis version of the databases using:\\n\\n* `sudo rladmin status databases extra redis_version`\\n\\nAs expected the version if 5.0.2, the same value that you have seen in the Web console.\\n\\n\\n## Installing the latest version of Redis Enterprise for PCF\\n\\nOnce the latest release of Redis Enterprise on PCF is imported, the upgrade is easy to do:\\n\\n1. Click on \\"Redis Enterprise on PCF\\" in the left menu.\\n1. Click on the \\"**+**\\" link.\\n    * The tiles is updated to the new version, you can review the configuration, not needed in this tutorial.\\n1. Click on \\"Review Pending Changes\\" button.\\n    ![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-004-tile-update.png)\\n\\n1. Unselect all product except Redis\\n1. Click Apply Changes\\n\\n\\nOnce you have clicked the update process will start, and you can follow the progress using the log information.\\n\\nNevertheless, it is interesting to see what is happening behind the scene using the command line on the VMS.\\n\\nThe update process using PCF will do the following:\\n\\n* Update and restart each node one by one (the 5 nodes of the Redis Enterprise deployment)\\n* during these steps, Redis Cluster will fail over moving the master and endpoint to another node to provide service continuity to the applications.\\n\\nLet\'s look at the following screenshots to see how the rolling upgrade was done by PCF.\\n\\n#### Starting Point\\n\\nThe cluster is up and running with 3 nodes with the version 5.4.0-24, and the `node:1` is the master of the cluster\\n\\n***Cluster Nodes:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-version.png)\\n\\n***Endpoints:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-version-endpoint.png)\\n\\nThe `node:1` is also the endpoint for the `db:4`\\n\\n***Shards:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-db.png)\\n\\nYou can see 3 shards in this deployment:\\n\\n* `db:4` is replicated and has 2 shards the master on `node:1` and a replica on `node:2`, the failover will automatically happen with no data loss.\\n* `db:5` is not replicated and has a single shard, so the database will be recreated fresh on a new node during the update.\\n\\nSo if you want to have a full service continuity with no data loss it is mandatory to use replication.\\n\\n\\n#### PCF Updating Node 1\\n\\nPCF has now started the process and stopped the `node:1`.\\n\\n***Cluster Nodes:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-status.png)\\n\\nAll the nodes are still on the \\"old version\\", but the cluster master has been moved now to `node:2`; so applications will continue to work.\\n\\nThe errors are here to indicate that the `node:1` is not accessible, and the `node:3` also raised an error since the replication link is not available.\\n\\n***Endpoints:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-version-endpoint.png)\\n\\nHere we see that the `db:4` endpoint, now on `node:2`, Redis Enterprises cluster manager has moved the endpoint to this node automatically.\\n\\n***Shards:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-db.png)\\n\\n* `db:4` is up and the master shard has been moved from `node:1` to `node:2`\\n* `db:5` is not present anymore, a new master will be created automatically on `node:3`, but empty.\\n\\n\\nThe fail over is done transparently with no impact for the application.\\n\\n#### PCF is restarting the updated Node 1\\n\\nOnce the node:1 VM is restarted with the updated version of Redis Enterprise you can see the new version number and status.\\n\\n***Cluster Nodes:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-version.png)\\n\\nThe 3 nodes of the cluster are up and running, and you can see that the `node:1` has been updated to the new version 5.4.4-7.\\n\\nThe master is still the `node:2`\\n\\nFor a short time the cluster will have heterogeneous nodes, this is not an issue.\\n\\n***Shards:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-db.png)\\n\\nYou can see that the `db:4` shards have the status `OK, OLD VERSION` that indicates that:\\n\\n* the database is up and running\\n* but the database itself has not yet been updated to the latest Redis version\\n\\nThe update of the database is done automatically, so after a while, if you run the command `sudo rladmin status databases extra redis_version` you will see something like:\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-db-version.png)\\n\\n\\n#### Updating all the nodes\\n\\nThe PCF update will continue and upgrade:\\n\\n* `node:2`, Redis Cluster will move the masters (cluster, shard, endpoint) to another node, in our case `node:1` for the replicated database (`db:4`)\\n* once the `node:2` is done the same work will be done on node 3.\\n\\n***Cluster Nodes:***\\n\\nAll the nodes of the clusters are now updated to the latest version of Redis Enterprise (5.4.4-7) supported on PCF.\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-version.png)\\n\\n***Shards:***\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-db.png)\\n\\n\\nThe update of the database is done automatically, so after a while if your run the command `sudo rladmin status databases extra redis_version` you will see something like:\\n\\n![](/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-db-version.png)\\n\\n\\n> In this example I am doing a \u201cminor upgrade\u201d, from Redis Cluster 5.4.0/Redis 5.0.2 to Redis Cluster 5.4.4/Redis 5.0.4, and everything is done automatically.\\n>\\n> If you are doing a major upgrade for example from 4.x to 5.x, the cluster will automatically be updated to the proper release, but you will have to manually update the existing databases as documented here.\\n\\n#### Updating Redis on PCF Services\\n\\nDuring the update, you will see other VMs stopped and started in the process. These VMs are used for:\\n\\n* Redis Registrar\\n* ResisLabs Service Broker\\n\\nThese services and nodes are not part of the \\"Redis Enterprise\\" per se, but are part of the integration with PCF.\\n\\n\\n## Conclusion\\n\\nThe update of the Redis Cluster is now complete:\\n\\n* All the nodes are on 5.4.4-7 (from 5.4.0-024)\\n* All the databases have been updated to the new Redis 5.0.4 (from 5.0.2)\\n\\nThe upgrade has been done automatically without any interruptions of service:\\n\\n* PCF scripts have been responsible for upgrading, stoping and starting each part of the installation in the correct order\\n* while Redis Enterprise Cluster has been responsible for keeping the databases available for the applications, during the process."},{"id":"/2019/09/04/multi-nodes-redis-cluster-with-docker","metadata":{"permalink":"/blog/2019/09/04/multi-nodes-redis-cluster-with-docker","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2019-09-04-multi-nodes-redis-cluster-with-docker.md","source":"@site/blog/2019-09-04-multi-nodes-redis-cluster-with-docker.md","title":"Multi-Nodes Redis Cluster with Docker","description":"As part of my on-boarding/training at RedisLabs I continue to play with the product, and I have decided today to install a local 3 nodes cluster of Redis Enterprise Software (RS); and show how easy is to move from a single node/shard database to a multi nodes highly available one.","date":"2019-09-04T00:00:00.000Z","formattedDate":"September 4, 2019","tags":[{"label":"redis","permalink":"/blog/tags/redis"},{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"cluster","permalink":"/blog/tags/cluster"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"container","permalink":"/blog/tags/container"},{"label":"cloud","permalink":"/blog/tags/cloud"}],"readingTime":7.175,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Multi-Nodes Redis Cluster with Docker","tags":["redis","nosql","cluster","docker","container","cloud"]},"prevItem":{"title":"Redis Rolling Upgrade on Pivotal Cloud Foundry (PCF)","permalink":"/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf"},"nextItem":{"title":"Getting Started with Redis Streams and Java","permalink":"/blog/2019/09/02/getting-with-redis-streams-and-java"}},"content":"As part of my on-boarding/training at RedisLabs I continue to play with the product, and I have decided today to install a local 3 nodes cluster of Redis Enterprise Software (RS); and show how easy is to move from a single node/shard database to a multi nodes highly available one.\\n\\nOnce your cluster is up & running, you will kill some containers to see how the system automatically fail-over to guarantee service continuity.\\n\\nThe deployment will look more or less like the schema below, (*[coming from RedisLabs documentation](https://docs.redislabs.com/latest/rs/getting-started/docker/)*)\\n\\n![](/images/posts/multi-nodes-redis-cluster-with-docker/docker-deployment.png )\\n\\nThis is a perfect environment for learning, developing and testing your applications, but it is not supported in production; for production, you can use:\\n\\n* [Redis Cloud](https://redislabs.com/redis-enterprise/pro/)\\n* [Redis Enterprise Software with Kubernetes and Red Hat OpenShift](https://docs.redislabs.com/latest/platforms/openshift/)\\n* [Redis Enterprise Software with Kubernetes Operator on PKS (Pivotal Container Service)](https://docs.redislabs.com/latest/platforms/pks/)\\n* [Redis Enterprise for Pivotal Cloud Foundry (PCF)](https://docs.redislabs.com/latest/platforms/pcf/).\\n\\n\x3c!--truncate--\x3e\\n\\n\\n**Prerequisites:**\\n\\n* Docker Desktop (*I am running Docker on Mac*)\\n\\n\\n### Installing and Running your First Redis Node\\n\\nAs usual, installing a new product with Docker is very simple just run the following command:\\n\\n```bash\\ndocker run -d --cap-add sys_resource \\\\\\n--name  redis-node1 \\\\\\n-p 8443:8443 \\\\\\n-p 9443:9443 \\\\\\n-p 12000:12000 \\\\\\nredislabs/redis\\n```\\n\\nLet\'s look at the parameters used here:\\n\\n* `-d`: run the container in the background\\n* `--cap-add sys_resource`: add Linux  `sys_resource`capabilities to set proper privileges\\n* `--name  redis-node1`: naming the container\\n* `-p 8443:8443`: to access the management web UI (HTTPS)\\n* `-p 9443:9443`: to access the REST API (HTTPS)\\n* `-p 12000:12000`: the TCP port that we will use for the database endpoint on this node\\n* `redislabs/redis`: use the RedisLabs image (the enterprise version of Redis)\\n\\n\\n#### Creating a new Cluster\\n\\nOnce the container is started you can configure the \\"cluster\\".\\n\\n1. Go top https://localhost:8443/ (accept the connect using the temporary certificate)\\n2. Click \\"Setup\\"\\n3. Change the Cluster Name to \\"my-redis-cluster.tug-demo.com\\"\\n4. Click \\"Next\\"\\n5. On the \\"cluster authentication\\" click \\"Next\\"  *(we will be using the free version)*\\n6. Enter the user admin credentials and click \\"Next\\".\\n\\nOnce it is configured, connect to the console to the console using the credentials you have created.\\n\\n#### Adding a new database\\n\\nNow you have to create a new database.\\n\\n1. Select \\"Redis Database\\" and \\"Single Region\\"\\n2. Enter the name \\"test-db-001\\", and \\"0.5\\" for the memory limit\\n3. Click \\"Show Advanced Options\\"\\n4. Enter 12000 in the \\"Endpoint port number\\" field\\n5. Click \\"Activate\\".\\n\\nAfter  fewseconds, the database is created and available.\\n\\nNote: we have not set anything special around clustering and replication; we will do that later.\\n\\n#### Using the Single Node Database\\n\\nYou can now connect to the database. You can use  `redis-cli`from your host, or you can connect to the container and do it from there:\\n\\n```bash\\n> docker  exec-it redis-node1 /bin/bash\\n\\nredislabs@0a174e819a6b:/opt$ redis-cli -p 12000\\n\\n127.0.0.1:12000> SET foo bar\\nOK\\n\\n127.0.0.1:12000> GET foo\\n\\"bar\\"\\n\\n127.0.0.1:12000>  exit\\n```\\n\\n\\n***Checkpoint***\\n\\nSo far you have:\\n\\n1. Install a single node cluster of Redis Enterprise using Docker\\n1. Create a new cluster\\n1. Created a database that listens on port 12000.\\n\\n\\nIn the container, run the  `rladminstatus`command, to get information about your deployment.\\n\\n![](/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status.png )\\n\\nIn the cluster node section, you can see the address of the node, 172.17.0.2 in my case. This is the IP address of the container, that will be used to create the multi-node cluster.\\n\\nIt is time to add new nodes to the cluster and enable replication and sharding\\n\\n### Adding new nodes\\n\\nTo add new nodes to the cluster, you start new containers. Since the 3 containers will be running on the same host, it is necessary, to avoid conflicts, to use different mapping to the Web UI, REST API, and database endpoint ports.\\n\\n**Start node 2:**\\n\\n```bash\\ndocker run -d --cap-add sys_resource \\\\\\n--name redis-node2 \\\\\\n-p 8444:8443 \\\\\\n-p 9444:9443 \\\\\\n-p 12001:12000 \\\\\\nredislabs/redis\\n```\\n\\n\\n**Start node 3:**\\n\\n```bash\\ndocker run -d --cap-add sys_resource \\\\\\n--name redis-node3 \\\\\\n-p 8445:8443 \\\\\\n-p 9445:9443 \\\\\\n-p 12002:12000 \\\\\\nredislabs/redis\\n```\\n\\nSo to configure each node you need to use the URLs:\\n\\n* node 2: https://localhost:8444/\\n* node 3: https://localhost:8445/\\n\\nI have just increase the port number of the Web UI (8443: node 1, 8444: node 2, 8445 node 3).\\n\\nFor these 2 new nodes, do the following steps to add them to the cluster:\\n\\n1. Click \\"Setup\\"\\n1. In  clusterconfiguration, select \\"Join Cluster\\",\\n    * Enter the IP address of the first node, 172.17.0.2 in my environment\\n    * Enter the credentials you have used during the installation of the first node.\\n1. Click \\"Next\\"\\n\\nAfter a few seconds, you will be redirected to the home page and see the list of nodes of your cluster.\\n\\nRepeat the same steps for the third node.\\n\\nYour environment should look like this after the installation and configuration of the 3 nodes.\\n\\n![](/images/posts/multi-nodes-redis-cluster-with-docker/cluster-view-3-nodes.png )\\n\\nYou can also reuse the  `rladminstatus`command on one of the containers and see the new configuration.\\n\\nIf you look carefully, you can see that you have only 1 shard in your cluster. Let\'s now add a new shard to the database.\\n\\n**Enabling Clustering and Replication to the DB**\\n\\nIn the Redis Enterprise Admin Web UI, (*you can use any of the nodes*):\\n\\n1. Click on the \\"databases\\" tab\\n1. Click on \\"test-db-001\\" database\\n1. Click on the \\"configuration\\"\\n1. Go to the bottom of the page and click \\"Edit\\"\\n1. Check the \\"Replication\\" checkbox, to create new shard that will be a replica, to provide high availability\\n1. Check \\"Database Clustering\\" and increase the number of shards to 2. This will  *distribute*the data in your database into 2 shards, this for better scalability.\\n    *You can see that the UI indicated that you have  *4 shards with replication*. Yes because you have a database that you have \\"divided in 2\\", and each of the portions of the database is replicated.\\n(Also with the free version of Redis Enterprise you are limited to 4 shards, so do not be surprised if you can not increase the number of shards to more than 4)\\n1. Click \\"Update\\" at the bottom of the page.\\n\\nGo back to the \\"nodes\\" tab, and you will see that you have now 4 shards distributed on your 3 nodes.\\n\\n\\n**Discovering the cluster topology**\\n\\nRun  `rladminstatus`to inspect your cluster and see how the various components are installed:\\n\\n![](/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status-with-db.png )\\n\\nFor example, you can see, that in my environment:\\n\\n*in the \\"CLUSTER NODES\\" section that the \\"node:1\\" is the master of the cluster\\n*in the \\"DATABASES\\" section that replication is enabled, and the database uses a \\"[dense placement](https://docs.redislabs.com/latest/rs/concepts/rebalancing-shard-placement/#dense-shard-placement-policy)\\"\\n*in the \\"SHARDS\\" section you can see the various shards and their placement (*node:1|2|3*), their role (*master|slave*) and their slots.\\n\\nUsing Redis Enterprise Enterprise Software (RS), all the clustering is managed transparently for you, and your applications. This means that you just have to connect your application to RS Cluster.\\n\\n**Clustering in Action**\\n\\nFirst of all, you have already seen a lot, just using the Web UI (and you could have done it using CLI and REST API), you have moved an existing database from a single instance to a distributed and highly available instance.\\n\\nSo now if something happens to the system, for example, if one of the masters disappears RS will automatically get another  oneelected.\\n\\nLet me kill for example the node 3 that contains the 2 masters for my database.\\n\\n```bash\\n> docker  killredis-node3\\n```\\n\\nAfter a few seconds, you should see that the master shards are now on another node, in my case node:1.\\n\\n![](/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status-with-db-002.png )\\n\\nSo if an application, is using this cluster it would be almost transparent as the election of the new master is happening in the  backgroud.\\n\\nIf you restart the node 3 it will rejoin the cluster, and the replicas will be updated on node 3 with any changes that happened to the masters.\\n\\n```bash\\n> docker start  redis-node3\\n```\\n\\nThe same automatic fail-over will happen if you kill a node with the cluster  manager,or the endpoint.\\n\\n#### Conclusion\\n\\nIn this small article you have learned how to:\\n\\n* deploy a 3 nodes Redis Enterprise Server (RS) on Docker (on a single host)\\n* create a database, and make it highly available and distributed easily using the Admin UI\\n* look at the deployment using  `rladminstatus`command.\\n\\nYou have also seen, by killing some nodes, how the cluster fail-over will various master services (shards, endpoint, master cluster node) to another node automatically. This to ensure a continuity of service for your application.\\n\\nIn another  postI will show what is the exact behavior of client applications during the fail-over."},{"id":"/2019/09/02/getting-with-redis-streams-and-java","metadata":{"permalink":"/blog/2019/09/02/getting-with-redis-streams-and-java","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2019-09-02-getting-with-redis-streams-and-java.md","source":"@site/blog/2019-09-02-getting-with-redis-streams-and-java.md","title":"Getting Started with Redis Streams and Java","description":"As you may have seen, I have joined Redis Labs a month ago; one of the first task as a new hire is to learn more about Redis. So I learned, and I am still learning.","date":"2019-09-02T00:00:00.000Z","formattedDate":"September 2, 2019","tags":[{"label":"redis","permalink":"/blog/tags/redis"},{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"stream","permalink":"/blog/tags/stream"},{"label":"streaming","permalink":"/blog/tags/streaming"},{"label":"java","permalink":"/blog/tags/java"}],"readingTime":7.77,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting Started with Redis Streams and Java","tags":["redis","nosql","stream","streaming","java"]},"prevItem":{"title":"Multi-Nodes Redis Cluster with Docker","permalink":"/blog/2019/09/04/multi-nodes-redis-cluster-with-docker"},"nextItem":{"title":"Getting Started With MapR-DB JSON REST API","permalink":"/blog/2018/05/23/getting-started-with-mapr-db-json-rest-api"}},"content":"As you may have seen, I have joined [Redis Labs](https://www.redislabs.com) a month ago; one of the first task as a new hire is to learn more about Redis. So I learned, and I am still learning.\\n\\nThis is when I discovered [Redis Streams](https://redis.io/topics/streams-intro). I am a big fan of streaming-based applications so it is natural that I start with a small blog post explaining how to use Redis Streams and Java.\\n\\n***What is Redis Streams?***\\n\\nRedis Streams is a Redis Data Type, that represents a log so you can add new information/message in an append-only mode *(this is not 100% accurate since you can remove messages from the log)*. Using Redis Streams you can build \\"Kafka Like\\" applications, what I mean by that you can:\\n\\n* create applications that publish and consume messages (nothing extraordinary here, you could already do that with Redis Pub/Sub)\\n* consume messages that are published even when your client application (consumer) is not running. This is a big difference with Redis Pub/Sub\\n* consume messages starting a specific offset, for example, read the whole history, or only new messages\\n\\nIn addition to this, Redis Streams has the concept of **Consumer Groups**. Redis Streams Consumer Groups, like Apache Kafka ones, allows the client applications to consume messages in a distributed fashion (multiple clients), providing an easy way to scale and create highly available systems.\\n\\n![](/images/posts/getting-with-redis-streams-and-java/redis-streams-101-img-1.png )\\n\\n\\n\\nEnroll in the [Redis University: Redis Streams](https://university.redislabs.com/courses/course-v1:redislabs+RU202+2019_03/about) to learn more and get certified.\\n\\n***Sample Application***\\n\\nThe [redis-streams-101-java GitHub Repository](https://github.com/tgrall/redis-streams-101-java) contains sample code that shows how to\\n\\n* post messages to a streams\\n* consume messages using a consumer group\\n\\n\x3c!-- truncate --\x3e\\n\\n#### Prerequisites\\n\\n* Redis 5.x, you have here multiple options:\\n    * [Download](https://redis.io) and install Redis Community\\n    * Install and Run a Docker image: [Community](https://hub.docker.com/_/redis) or [Redis Enterprise](https://hub.docker.com/r/redislabs/redis) \\n    * Create a online instance on [Redis Labs Cloud](https://redislabs.com/redis-enterprise/essentials/) (30mb for free)\\n* Java 8 or later\\n* Apache Maven 3.5.x\\n* Git\\n\\n\\n### Java & Redis Streams\\n\\nRedis has many Java clients developed by the community, as you can see on the [Redis.io site](https://redis.io/clients#java).\\n\\nIt looks, based on my short experience with Redis so far, that the most complete one around Redis Streams support is [Lettuce](https://lettuce.io), this is the one I will be using in the following code.\\n\\n####1- Adding Lettuce to Your Maven Project\\n\\nAdd the following dependency to your project file:\\n\\n```xml\\n        <dependency>\\n            <groupId>io.lettuce</groupId>\\n            <artifactId>lettuce-core</artifactId>\\n            <version>5.1.8.RELEASE</version>\\n        </dependency>\\n```\\n\\n####2- Connecting to Redis\\n\\n\\nImport the following classes\\n\\n```java\\nimport io.lettuce.core.*;\\nimport io.lettuce.core.api.StatefulRedisConnection;\\nimport io.lettuce.core.api.sync.RedisCommands;\\n```\\n\\nThen connect with:\\n\\n```java\\nRedisClient redisClient = RedisClient.create(\\"redis://password@host:port\\"); // change to reflect your environment\\nStatefulRedisConnection<String, String> connection = redisClient.connect();\\nRedisCommands<String, String> syncCommands = connection.sync();\\n```\\n\\nWhen your application is done with the connection you should disconnect with the following code:\\n\\n```java\\nconnection.close();\\nredisClient.shutdown();\\n```\\n\\n####3- Sending Message to Streams\\n\\nOnce you have a connection you can send a message. In this example, I will let Redis generate the message ID, which is time-based, and the body will be built using a Map representing IoT data, for example, a weather data capturing Wind speed and direction in real-time.\\n\\n```java\\n    public static void main(String[] args) {\\n\\n        RedisClient redisClient = RedisClient.create(\\"redis://localhost:6379\\"); // change to reflect your environment\\n        StatefulRedisConnection<String, String> connection = redisClient.connect();\\n        RedisCommands<String, String> syncCommands = connection.sync();\\n\\n        Map<String, String> messageBody = new HashMap<>();\\n        messageBody.put( \\"speed\\", \\"15\\" );\\n        messageBody.put( \\"direction\\", \\"270\\" );\\n        messageBody.put( \\"sensor_ts\\", String.valueOf(System.currentTimeMillis()) );\\n\\n        String messageId = syncCommands.xadd(\\n                \\"weather_sensor:wind\\",\\n                messageBody);\\n\\n        System.out.println( String.format(\\"Message %s : %s posted\\", messageId, messageBody) );\\n\\n        connection.close();\\n        redisClient.shutdown();\\n\\n    }\\n\\n```\\n\\nLet me explain the code:\\n\\n* Lines 3-5 are used to connect to Redis\\n* Lines 7-10 are used to create the message body, using a Map, since Redis Streams messages are string key/values.\\n* Lines 12-14 call the `syncCommands.xadd()` method using the streams key \\"weather_sensor:wind\\" and the message body itself\\n   * this method returns the message ID.\\n* line 16 just print the message ID and content\\n* the lines 18-19 close the connection and client.\\n\\nThe complete producer code is available [here](https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Producer.java).\\n\\n####4- Consuming Messages\\n\\nRedis Streams offers various way to consume/read messages using the commands: [XRANGE](https://redis.io/commands/xrange), [XREVRANGE](https://redis.io/commands/xrevrange), [XREAD](https://redis.io/commands/xread), [XREADGROUP](https://redis.io/commands/xreadgroup).\\n\\nI want to keep the article short and close to the way you would build an application with Apache Kafka, this is why I will use the [XREADGROUP](https://redis.io/commands/xreadgroup) command from Lettuce.\\n\\nThe Consumer Groups allow developers to create a group of clients that will cooperate to consume messages from the streams (for scale and high availability); it is also a way to associate the client to specific applications roles; for example:\\n\\n* a consumer group called \\"data warehouse\\" will consume messages and send them to a data warehouse\\n* another consumer group called \\"aggregator\\" will consume the messages and aggregate the data and send them to another sink (another stream or storage)\\n\\nEach of this group will act independently, and each of this group could have multiple \\"consumers\\" (client).\\n\\nLet\'s see how you use this in Java.\\n\\n\\n```java\\n...\\n\\n        try {\\n            // WARNING: Streams must exist before creating the group\\n            //          This will not be necessary in Lettuce 5.2, see https://github.com/lettuce-io/lettuce-core/issues/898\\n            syncCommands.xgroupCreate( XReadArgs.StreamOffset.from(\\"weather_sensor:wind\\", \\"0-0\\"), \\"application_1\\"  );\\n        }\\n        catch (RedisBusyException redisBusyException) {\\n            System.out.println( String.format(\\"\\\\t Group \'%s already\' exists\\",\\"application_1\\"));\\n        }\\n\\n\\n        System.out.println(\\"Waiting for new messages\\");\\n\\n        while(true) {\\n\\n            List<StreamMessage<String, String>> messages = syncCommands.xreadgroup(\\n                    Consumer.from(\\"application_1\\", \\"consumer_1\\"),\\n                    XReadArgs.StreamOffset.lastConsumed(\\"weather_sensor:wind\\")\\n            );\\n\\n            if (!messages.isEmpty()) {\\n                for (StreamMessage<String, String> message : messages) {\\n                    System.out.println(message);\\n                    // Confirm that the message has been processed using XACK\\n                    syncCommands.xack(STREAMS_KEY, \\"application_1\\",  message.getId());\\n                }\\n            }\\n\\n\\n        }\\n\\n...\\n```\\n\\nThis code is a subset of the `main()` method I have removed the connection management part, to add readability. Let\'s take a look to the code:\\n\\n* line 3 to 10, using the method `xgroupCreate()`, that matches the [XGROUP CREATE](https://redis.io/commands/xgroup) command,\\n    * is used to create a new group called `application_1`,\\n    * consume messages from the stream `weather_sensor:wind` \\n    * starting at the first message in the stream, this is indicated using the message ID `0-0`. *Note that it is also possible to indicate to the group to start to read at a specific message ID, or only the new messages that arrive after the creating of the consumer group using `$` special ID (or the helper method `XReadArgs.StreamOffset.latest()`*.\\n* line 15 to 30, in this example we use an infinite loop (`while(true)`) to wait for any new messages published to the streams\\n* line 17 to 20, the method `xreadgroup()` returns the messages based on the group configuration\\n    * line 18 define the consumer named `consumer_1` that is associated with the group `application_1`: you can create new group do distribute the read to multiple clients\\n    * line 19 indicates where to start, in this case, `StreamOffset.lastConsumed(\\"weather_sensor:wind\\")` the consumer will consume messages that have not been read already. With the current configuration of the group (offset `0-0`), when the consumer will start for the first time, it will read all the existing messages.\\n* line 22 to 28, the application iterates on each messages, and:\\n    * line 24, process the message, a simple print in this case\\n    * line 26, sends a acknowledgment using `xack()` command. You have to use the ack command to confirm that a message has been read and processed. The [`XACK`](https://redis.io/commands/xack) command removes the message from the pending list of the consumer group.\\n\\nThe complete consumer code is available [here](https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Consumer.java).\\n\\n### Build & Run the Simple Java Application\\n\\nNow that you have a better understanding of the code, let\'s run the producer and consumer. You can run this from your IDE, or using Maven.\\n\\nLet\'s do it using Maven CLI, for this open 2 terminals:\\n\\n* one to produce messages\\n* one to consume them\\n\\n\\n*1- Clone and Build the project*\\n\\n\\n```bash\\n> git clone https://github.com/tgrall/redis-streams-101-java.git\\n\\n> cd redis-streams-101-java\\n\\n> mvn clean verify\\n```\\n\\n*2- Post a new message*\\n\\n```bash\\n\\n> mvn exec:java -Dexec.mainClass=\\"com.kanibl.redis.streams.simple.RedisStreams101Producer\\"\\n\\n```\\n\\n*3- Consume messages*\\n\\nOpen a new terminal and run the following command:\\n\\n```bash\\n\\n> mvn exec:java -Dexec.mainClass=\\"com.kanibl.redis.streams.simple.RedisStreams101Consumer\\"\\n\\n```\\n\\nThe consumer will start and consume the message you just posted, and wait for any new messages.\\n\\n\\n*4- In the first terminal post 100 new messages*\\n\\n```bash\\n\\n> mvn exec:java -Dexec.mainClass=\\"com.kanibl.redis.streams.simple.RedisStreams101Producer\\" -Dexec.args=\\"100\\"\\n\\n```\\n\\nThe consumer will receive and print all the messages.\\n\\n*5- Kill the consumer and post more messages*\\n\\nLet\'s now do another test, stop the consumer using a simple `ctrl+C`.\\n\\nThen post 5 new messages.\\n\\n```bash\\n\\n> mvn exec:java -Dexec.mainClass=\\"com.kanibl.redis.streams.simple.RedisStreams101Producer\\" -Dexec.args=\\"5\\"\\n\\n```\\n\\nThe messages are not yet consumed by any application, but still store in Redis Streams.\\n\\nSo when you start the consumer, it will consumes these new messages.\\n\\n```bash\\n\\n> mvn exec:java -Dexec.mainClass=\\"com.kanibl.redis.streams.simple.RedisStreams101Consumer\\"\\n\\n```\\n\\nThis is a one of the differences between [Redis Streams](https://redis.io/topics/streams-intro) and [Redis PubSub](https://redis.io/topics/pubsub). The producer application has publish many messages while the consumer application was not running. Since the consumer is ran with `StreamOffset.lastConsumed()`, when the consumer is starting, it looks to the last consumed ID, and start to read the streams from there. This method generate a XGROUPREAD command with the group\\n\\n\\n### Conclusion\\n\\nIn this small project, you have learned, how to use Lettuce, a Java client for Redis to:\\n\\n* publish messages to a stream\\n* create a consumer group\\n* consume messages using the consumer group.\\n\\nThis is a very basic example, and in a next post I will show you how to work with multiple consumers, and to configure the Consumer Group and Consumers to control which messages you want to read\\n\\nMore to come!"},{"id":"/2018/05/23/getting-started-with-mapr-db-json-rest-api","metadata":{"permalink":"/blog/2018/05/23/getting-started-with-mapr-db-json-rest-api","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2018-05-23-getting-started-with-mapr-db-json-rest-api.md","source":"@site/blog/2018-05-23-getting-started-with-mapr-db-json-rest-api.md","title":"Getting Started With MapR-DB JSON REST API","description":"Introduction","date":"2018-05-23T00:00:00.000Z","formattedDate":"May 23, 2018","tags":[{"label":"api","permalink":"/blog/tags/api"},{"label":"maprdb","permalink":"/blog/tags/maprdb"},{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"rest","permalink":"/blog/tags/rest"}],"readingTime":2.82,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting Started With MapR-DB JSON REST API","tags":["api","maprdb","nosql","rest"]},"prevItem":{"title":"Getting Started with Redis Streams and Java","permalink":"/blog/2019/09/02/getting-with-redis-streams-and-java"},"nextItem":{"title":"Getting started with MapR-DB Table Replication","permalink":"/blog/2017/08/08/getting-started-with-mapr-db-table-replication"}},"content":"## Introduction\\n\\nIn this project you will learn how to use the MapR-DB JSON REST API to:\\n\\nCreate and Delete tables\\nCreate, Read, Update and Delete documents (CRUD)\\nMapR Extension Package 5.0 (MEP) introduced the MapR-DB JSON REST API that allow application to use REST to interact with MapR-DB JSON.\\n\\nYou can find information about the MapR-DB JSON REST API in the documentation: [Using the MapR-DB JSON REST API](https://maprdocs.mapr.com/home/MapR-DB/JSON_DB/UsingMapRDBJSONRESTAPI.html)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nYou system should have the following components:\\n\\n* A running MapR 6.0.1 & MEP 5.0 cluster with the MapR-DB REST API service installed\\n* `curl` or equivalent tool\\n\\n\\n\\n## Discover the MapR-DB JSON REST API\\n\\nThe easiest way to discover it, is to use curl command (or equivalent).\\n\\n**1 - Create a table**\\n\\n```\\ncurl -X PUT \\\\\\n  \'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp\' \\\\\\n  -u root:mapr \\\\\\n  -k\\n```\\n\\nIn this command:\\n\\n* the MapR-DB REST Service (MapR Data Access Gateway) is running on the mapr-node host with the default port `8243` using HTTPS\\n* the HTTP verb `PUT` on `/api/v2/table/` endoint creates a new table\\n* the protocol is HTTP since HTTPS is not enabled on this cluster\\n* the new table will be created wit the path `/apps/emp` that is encoded to `%2Fapps%2Femp`\\n* the user `root` with the password `mapr` is used for authentication, using basic authentication\\n* the `-k` parameter is used to indicate to turn off curl\u2019s verification of the certificate.\\n\\nIn this example, you use the basic authentication, it is also possible to use [JSON Web Token](https://jwt.io/introduction/). You will learn more about this when you will write an application in Go.\\n\\n\\n**2 - Insert Documents**\\n\\nInsert one document\\n\\n```\\ncurl -X POST \\\\\\n  \'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp\' \\\\\\n  -u root:mapr \\\\\\n  -H \'Content-Type: application/json\' \\\\\\n  -d \'{\\"_id\\":\\"user001\\",\\"first_name\\":\\"John\\",\\"last_name\\":\\"Doe\\", \\"age\\" : 28}\' \\\\\\n  -k\\n```\\n\\nIn this command:\\n\\n* the `/api/v2/table/{path}` with the verb `GET` is used with a `condition` query parameter\\n* the OJAI JSON syntax is used to express the condition: `{\\"$eq\\":{\\"last_name\\":\\"Doe\\"}}`\\n\\n**3 - Update a document**\\n\\nThe following example will increment the age by 1 and update the last name.\\n\\n```\\ncurl -X POST \\\\\\n  \'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp%2F/document/user001\' \\\\\\n  -u root:mapr \\\\\\n  -H \'Content-Type: application/json\' \\\\\\n  -d \'{\\"$set\\" : {\\"last_name\\" : \\"New Doe\\"}, \\"$increment\\" : {\\"age\\":1}}\' \\\\\\n  -k\\n```\\n\\nIn this comamnd:\\n\\n* the URL points to the document `_id` to update\\n* the HTTP verb `POST` is used to modify the resource\\n* the request body `-d` is the OJAI JSON Mutation that update the last name and increment the age.\\n\\nYou can check that the document has been updated using the following command:\\n\\n```\\ncurl -X GET \\\\\\n  \'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp%2F/document/user001\' \\\\\\n  -u root:mapr \\\\\\n  -k\\n```\\n\\n**4 - Delete a document**\\n\\nDelete the document with the `_id` user001.\\n\\n```\\ncurl -X DELETE \\\\\\n  \'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp%2F/document/user001\' \\\\\\n  -u root:mapr \\\\\\n  -k\\n```\\n\\nIn this command:\\n\\n* the URI `/api/v2/table/{path}/document/{id}` with the HTTP verb `DELETE` is used to delete the document\\n\\n**5 - Delete the MapR-DB JSON table**\\n\\nThe last step of this tutorial is to delete the table using the following command:\\n\\n```\\ncurl -X DELETE \\\\\\n  \'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp\' \\\\\\n  -u root:mapr \\\\\\n  -k\\n```\\n\\n## Conclusion\\n\\nIn this tutorial you have learned how to use the MapR-DB JSON REST API to:\\n\\n* Create a table\\n* Insert and query documents\\n* Update and delete documents\\n* Drop table\\n\\nYou can now use the API to create MapR-DB JSON Application using your favorite language."},{"id":"/2017/08/08/getting-started-with-mapr-db-table-replication","metadata":{"permalink":"/blog/2017/08/08/getting-started-with-mapr-db-table-replication","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2017-08-08-getting-started-with-mapr-db-table-replication.md","source":"@site/blog/2017-08-08-getting-started-with-mapr-db-table-replication.md","title":"Getting started with MapR-DB Table Replication","description":"Introduction","date":"2017-08-08T00:00:00.000Z","formattedDate":"August 8, 2017","tags":[],"readingTime":5.19,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting started with MapR-DB Table Replication","categories":"nosql maprdb mapr replication"},"prevItem":{"title":"Getting Started With MapR-DB JSON REST API","permalink":"/blog/2018/05/23/getting-started-with-mapr-db-json-rest-api"},"nextItem":{"title":"Getting Started with Kafka REST Proxy for MapR Streams","permalink":"/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams"}},"content":"## Introduction\\n\\nMapR-DB Table Replication allows data to be replicated to another table that could be on on the same cluster or in another cluster. This is different from the automatic and intra-cluster replication that copies the data into different physical nodes for high availability and prevent data loss.\\n\\nThis tutorial focuses on the MapR-DB Table Replication that replicates data between tables on different clusters.\\n\\nReplicating data between different clusters allows you to:\\n\\n* provide another level of disaster recovery that protects your data and applications against global data center failure,\\n* push data close to the applications and users, \\n* aggregate the data from mutliple datacenters.\\n\\n**Replication Topologies**\\n\\nMapR-DB Table Replication provides various topologies to adapt the replication to the business and technical requirements:\\n\\n* *Master-slave replication* : in this topology, you replicate one way from source tables to replicas. The replicas can be in a remote cluster or in the cluster where the source tables are located.\\n* *Multi-Master replication* : in this replication topology, there are two master-slave relationships, with each table playing both the role of a master and a slave. Client applications update both tables and each table replicates updates to the other.\\n\\nIn this example you will learn how to setup multi-master replication.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Prerequisites\\n\\n* 2 MapR Clusters 5.x with Enterprise Edition license\\n    * in this demonstration they are called `cluster1` and `cluster2`\\n\\n## Setting Up Replication\\n\\nIn the next steps you will configure your clusters to enable mutip-master replication as follow:\\n\\n![Architecture](http://tgrall.github.io/images/posts/maprdb-replication/replication.png)\\n\\n\\n\\n### Configuring the clusters\\n\\nEach node of the source cluster must communicate with the destination cluster\'s CLDB nodes. On each node of your source cluster edit the `mapr-clusters.conf` file and add the destination cluster information.\\n\\n*Cluster 1 Configuration*\\n\\nIn all the nodes of `cluster1`, edit the  `/opt/mapr/conf/mapr-clusters.conf` file and add the `cluster2` configuration. The file should look like the following:\\n\\n```\\ncluster1 secure=false cluster1-node1:7222 cluster1-node2:7222 cluster1-node2:7222\\n\\ncluster2 secure=false cluster2-node1:7222 cluster2-node2:7222 cluster2-node3:7222\\n```\\n\\n*Cluster 2 Configuration*\\n\\nIn all the nodes of `cluster2`, edit the  `/opt/mapr/conf/mapr-clusters.conf` file and add the `cluster1` configuration. The file should look like the following:\\n\\n```\\ncluster2 secure=false cluster2-node1:7222 cluster2-node2:7222 cluster2-node3:7222\\n\\ncluster1 secure=false cluster1-node1:7222 cluster1-node2:7222 cluster1-node2:7222\\n```\\n\\nYou can find information about the `mapr-clusters.conf` format in [the documentation](http://maprdocs.mapr.com/home/ReferenceGuide/mapr-clusters.conf.html).\\n\\nOpen a terminal window on one of the `cluster1` node using `mapr` user, and do the following:\\n\\n```\\n$ ls /mapr/cluster1/\\napps   hbase  installer  opt  tmp  user  var\\n\\n$ ls /mapr/cluster2/\\napps   hbase  installer  opt  tmp  user  var\\n\\n```\\n\\n### Installing and Configuring the MapR Gateway\\n\\nA MapR gateway mediates one-way communication between a source MapR cluster and a destination MapR cluster. In this example you will use mult-master replication, this means that data will be replicated from `cluster1` to `cluster2` and from `cluster2` to `cluster1`.\\n\\nThe good practice is to install the MapR-Gateway to the destination cluster, so in our case let\'s install one gateway on one of the `cluster1` node, and one gateway on one of the `cluster2` node. Note that this configuration will not be highly available, and usually you will deploy more than 1 gateway by cluster.\\n\\n\\n#### Installing the MapR-Gateway\\n\\nAs root on one node of the `cluster1`, adapt the command to your linux environment, for example on the node `cluster1-node2`\\n\\n```\\n$ yum install mapr-gateway\\n\\n\\n# Update MapR configuration\\n$ /opt/mapr/server/configure.sh -N cluster1 -C cluster1-node1:7222,cluster1-node2:7222,cluster1-node3:7222 -R\\n\\n```\\n\\nDo the same on `cluster2`, for example on the node `cluster2-node2`:\\n\\n\\n```\\n$ yum install mapr-gateway\\n\\n\\n# Update MapR configuration\\n$ /opt/mapr/server/configure.sh -N cluster1 -C cluster2-node1:7222,cluster2-node2:7222,cluster2-node3:7222 -R\\n\\n```\\n\\n\\n#### Registering the Gateway to the Clusters\\n\\nNow that we have a gateway running on each cluster, you have to ***register the gateway*** in each cluster.\\n\\nOn `cluster1` run the following command to register the `cluster2` gateway as destination:\\n\\n```\\n$ maprcli cluster gateway set -dstcluster cluster2 -gateways cluster2-node2\\n\\n# Check the configuration\\n$ maprcli cluster gateway list\\n```\\n\\nOn `cluster2` run the following command to register the `cluster1` gateway as destination:\\n\\n```\\n$ maprcli cluster gateway set -dstcluster cluster1 -gateways cluster1-node2\\n\\n# Check the configuration\\n$ maprcli cluster gateway list\\n```\\n\\n\\n### Creating Table with Replication\\n\\nIn a terminal window, as `mapr` user on `cluster1`, create a table and insert documents:\\n\\n```\\n$ maprcli table create -path /apps/user_profiles  -tabletype json\\n\\n```\\nThis create a new JSON table; it is also possible to use `/mapr/cluster1/apps/user_profiles`.\\n\\nLet\'s now add documents using MapR-DB Shell:\\n\\n```\\n$ mapr dbshell\\n\\nmaprdb mapr:> insert /apps/user_profiles --value \'{\\"_id\\":\\"user001\\" , \\"first_name\\":\\"John\\", \\"last_name\\":\\"Doe\\"}\'\\n\\nmaprdb mapr:> find /apps/user_profiles\\n\\n```\\n\\n#### Adding Table Replication\\n\\nLet\'s now enable replication between `user_profiles` on `cluster1` to a `user_profiles` table in `cluster2`.\\n\\nIn `cluster1`, on a terminal window as `mapr` run the following command:\\n\\n```\\n$ maprcli table replica autosetup -path /apps/user_profiles -replica /mapr/cluster2/apps/user_profiles -multimaster yes\\n```\\n\\nYou can get information about the replication configuration for the table using the following command:\\n\\n```\\n$ maprcli table replica list -path /apps/user_profiles -json\\n```\\n\\n\\n#### Testing Replication\\n\\nOpen another terminal in `cluster2` and use MapR-DB Shell to look at the replicated data:\\n\\n```\\n$ mapr dbshell\\n\\nmaprdb mapr:> find /apps/user_profiles\\n{\\"_id\\":\\"user001\\",\\"first_name\\":\\"John\\",\\"last_name\\":\\"Doe\\"}\\n1 document(s) found.\\n\\n```\\nYou can also use the full path `/mapr/cluster2/apps/user_profiles`\\n\\nIn `cluster1` add a new document using MapR-DB Shell:\\n\\n```\\n$ mapr dbshell\\n\\nmaprdb mapr:> insert /apps/user_profiles --value \'{\\"_id\\":\\"user002\\" , \\"first_name\\":\\"Simon\\", \\"last_name\\":\\"Dupont\\"}\'\\n\\nmaprdb mapr:> find /apps/user_profiles\\n```\\n\\nDo a find in `cluster2` table, and you will see that the data have been replicated.\\n\\nYou can insert or delete a document in `cluster2` and do a find in `cluster1`, you will see that the new document is also replicated in the other direction.\\n\\nNote, for this demonstration, we use 2 terminals connected to each cluster you can do some test using the Global Namespace in a single MapR-DB Shell. \\n\\n## Conclusion\\n\\nIn this tutorial you have learned how to setup the MapR-DB Multi-Master replication to have data automatically replicated between 2 clusters.\\n\\nMapR-DB Table Replication provides many options, not only in term of topology (master-slave/mult-master), but also some options and commands to:\\n\\n* replicate some columns/attributes or column family\\n* configure replication in a secured cluster\\n* pause replication.\\n\\nYou can find more information about the MapR-DB Table Replication, and MapR-Gateway in the documentation:\\n\\n* [Table Replication](http://maprdocs.mapr.com/home/MapR-DB/ReplicatingMapR-DBTables.html) \\n* [Setting up Table Replication](http://maprdocs.mapr.com/home/MapR-DB/ConfiguringMapRClustersForTR.html)\\n* [Configuring and Managing MapR Gateways](http://maprdocs.mapr.com/home/Gateways/MapRGateways.html)"},{"id":"/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams","metadata":{"permalink":"/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2017-01-20-getting-started-with-kafka-rest-proxy-for-mapr-streams.md","source":"@site/blog/2017-01-20-getting-started-with-kafka-rest-proxy-for-mapr-streams.md","title":"Getting Started with Kafka REST Proxy for MapR Streams","description":"Introduction","date":"2017-01-20T00:00:00.000Z","formattedDate":"January 20, 2017","tags":[],"readingTime":7.785,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting Started with Kafka REST Proxy for MapR Streams","categories":"mapr streams kafka rest streaming"},"prevItem":{"title":"Getting started with MapR-DB Table Replication","permalink":"/blog/2017/08/08/getting-started-with-mapr-db-table-replication"},"nextItem":{"title":"Getting Started with MQTT and Java","permalink":"/blog/2017/01/02/getting-started-with-mqtt"}},"content":"## Introduction\\n\\nMapR Ecosystem Package 2.0 (MEP) is coming with some new features related to MapR Streams:\\n\\n* [Kafka REST Proxy for MapR Streams](http://maprdocs.mapr.com/home/Kafka/kafkaREST.html) provides a RESTful interface to MapR Streams and Kafka clusters to consume and product messages and to perform administrative operations.\\n* [Kafka Connect for MapR Streams](http://maprdocs.mapr.com/home/Kafka/kafkaConnect.html) is a utility for streaming data between MapR Streams and Apache Kafka and other storage systems.\\n\\nMapR Ecosystem Packs (MEPs) are a way to deliver ecosystem upgrades decoupled from core upgrades - allowing you to upgrade your tooling independently of your Converged Data Platform. You can lean more about MEP 2.0 in [this article](https://www.mapr.com/blog/announcing-mapr-ecosystem-pack-mep-20).\\n\\nIn this blog we describe how to use the REST Proxy to publish and consume messages to/from MapR Streams. The REST Proxy is a great addition to the MapR Converged Data Platform allowing any programming language to use MapR Streams.\\n\\nThe Kafka REST Proxy provided with the MapR Streams tools, can be used with MapR Streams (default), but also used in a hybrid mode with Apache Kafka. In this article we will focus on MapR Streams.\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\n* MapR Converged Data Platform 5.2 with MEP 2.0\\n  * with MapR Streams Tools\\n* curl, wget or any HTTP/REST Client tool\\n\\n\\n## Create the MapR Streams and Topic\\n\\nA stream is a collection of topics that you can manage as a group by:\\n\\n1. Setting security policies that apply to all topics in that stream\\n2. Setting a default number of partitions for each new topic that is created in the stream\\n3. Set a time-to-live for messages in every topic in the stream\\n\\nYou can find more information about MapR Streams concepts in the [documentation](http://maprdocs.mapr.com/home/MapR_Streams/mapr_streams.html).\\n\\nOn your Mapr Cluster or Sandbox, run the following commands:\\n\\n```\\n$ maprcli stream create -path /apps/iot-stream -produceperm p -consumeperm p -topicperm p\\n\\n$ maprcli stream topic create -path /apps/iot-stream -topic sensor-json -partitions 3\\n\\n$ maprcli stream topic create -path /apps/iot-stream -topic sensor-binary -partitions 3\\n```\\n\\n## Start Kafka Console Producers and Consumers\\n\\nOpen two terminal windows and run the consumer Kafka utilities using the following commands:\\n\\n#### Consumer\\n\\n* Topic sensor-json\\n\\n```\\n$ /opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/iot-stream:sensor-json\\n```\\n* Topic sensor-binary\\n\\n```\\n$ /opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/iot-stream:sensor-binary\\n```\\n\\nThis two terminal windows will allow you to see the messages posted on the different topics\\n\\n## Using Kafka REST Proxy\\n\\n### Inspect Topic Metadata\\n\\nThe endpoint `/topics/[topic_name]` allows you to get some informations about the topic. In MapR Streams, topics are part of a *stream* identified by a path;\\nto use the topic using the REST API you have to use the full path, and encode it in the URL; for example:\\n\\n* `/apps/iot-stream:sensor-json` will be encoded with `%2Fapps%2Fiot-stream%3Asensor-json`\\n\\nRun the following command, to get information about the `sensor-json` topic\\n\\n```\\n$ curl -X GET  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json\\n```\\n\\nNote: For simplicity reason I am running the command from the node where the Kafka REST proxy is running, so it is possible to use `localhost`.\\n\\nYou can print JSON in a pretty way, by adding a Python command such as :\\n\\n```\\n$ curl -X GET  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json | python -m json.tool\\n```\\n\\n**Default Stream**\\n\\nAs mentioned above, the Stream path is part of the topic name you have to use in the command;\\nhowever it is possible to configure the MapR Kafka REST Proxy to use a default stream.\\nFor this you should add the following property in the `/opt/mapr/kafka-rest/kafka-rest-2.0.1/config/kafka-rest.properties` file:\\n\\n* `streams.default.stream=/apps/iot-stream`\\n\\n When you change the Kafka REST proxy configuration, you must restart the service using maprcli or MCS.\\n\\n The main reason to use the `streams.default.stream` properties is to simplify the URLs used by the application for example\\n * with `streams.default.stream` you can use `curl -X GET  http://localhost:8082/topics/`\\n * without this configuration, or if you want to use a specific stream you must specify it in the URL `http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json`\\n\\n In this article, all the URLs contains the encoded stream name, like that you can start using the Kafka REST proxy without changind the configuration and also use it with different streams.\\n\\n### Publishing Messages\\n\\nThe Kafka REST Proxy for MapR Streams allows application to publish messages to MapR Streams. Messages could be send as JSON or Binary content (base64 encoding).\\n\\n\\n####To send a JSON Message:\\n\\n* the query should be a HTTP `POST`\\n* the Content-Type should be : `application/vnd.kafka.json.v1+json`\\n* the Body:\\n\\n```JSON\\n{\\n  \\"records\\":\\n  [\\n    {\\n      \\"value\\":\\n      {\\n        \\"temp\\" : 10 ,\\n        \\"speed\\" : 40 ,\\n        \\"direction\\" : \\"NW\\"\\n        }  \\n      }\\n  ]\\n}\\n```\\nThe complete request is:\\n\\n```\\ncurl -X POST -H \\"Content-Type: application/vnd.kafka.json.v1+json\\" \\\\\\n  --data \'{\\"records\\":[{\\"value\\": {\\"temp\\" : 10 , \\"speed\\" : 40 , \\"direction\\" : \\"NW\\"}  }]}\' \\\\\\n  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json\\n```\\n\\nYou should see the message printed in the terminal window where the `/apps/iot-stream:sensor-json` consumer is running.\\n\\n\\n####To send a binary Message:\\n\\n* the query should be a HTTP `POST`\\n* the Content-Type should be : `application/vnd.kafka.binary.v1+json`\\n* the Body:\\n\\n```JSON\\n{\\n  \\"records\\":\\n  [\\n    {\\n      \\"value\\":\\"SGVsbG8gV29ybGQ=\\"\\n    }\\n  ]\\n}\\n```\\n\\nNote that `SGVsbG8gV29ybGQ=` is the string \\"Hello World\\" encoded in Base64.\\n\\nThe complete request is:\\n\\n```\\ncurl -X POST -H \\"Content-Type: application/vnd.kafka.binary.v1+json\\" \\\\\\n  --data \'{\\"records\\":[{\\"value\\":\\"SGVsbG8gV29ybGQ=\\"}]}\' \\\\\\n  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-binary\\n```\\n\\nYou should see the message printed in the terminal window where the `/apps/iot-stream:sensor-binary` consumer is running.\\n\\n####Sending multiple messages\\n\\nThe `records` field of the HTTP Body allows you to send multiple messages for example you can send:\\n\\n```\\ncurl -X POST -H \\"Content-Type: application/vnd.kafka.json.v1+json\\" \\\\\\n  --data \'{\\"records\\":[{\\"value\\": {\\"temp\\" : 12 , \\"speed\\" : 42 , \\"direction\\" : \\"NW\\"}  }, {\\"value\\": {\\"temp\\" : 10 , \\"speed\\" : 37 , \\"direction\\" : \\"N\\"}  } ]}\' \\\\\\n  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json\\n```\\n\\nThis command will send 2 messages, and increment the offset by 2. You can do the same\\nwith binary content, just add new element in the JSON array; for example:\\n\\n```\\ncurl -X POST -H \\"Content-Type: application/vnd.kafka.binary.v1+json\\" \\\\\\n  --data \'{\\"records\\":[{\\"value\\":\\"SGVsbG8gV29ybGQ=\\"}, {\\"value\\":\\"Qm9uam91cg==\\"}]}\' \\\\\\n  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-binary\\n```\\n\\nAs you probably know, it is possible to set a key to a message to be sure that all the messages\\nwith the same key will arrive in the same partition. For this, add the `key` attribute to the message as follow:\\n\\n```JSON\\n{\\n  \\"records\\":\\n  [\\n    {\\n      \\"key\\": \\"K001\\",\\n      \\"value\\":\\n      {\\n        \\"temp\\" : 10 ,\\n        \\"speed\\" : 40 ,\\n        \\"direction\\" : \\"NW\\"\\n        }  \\n      }\\n  ]\\n}\\n```\\n\\nNow that you know how to post messages to MapR Stream topics usinf the REST Proxy, let\'s see how to consume the messages.\\n\\n### Consuming Messages\\n\\nThe REST proxy can also be used to consume messages from topics; for this you need to:\\n\\n1. Create a consumer instance.\\n2. Use this URL returned by the first call to read message.\\n3. Delete the consumer instanced if needed.\\n\\n#### Creating the consumer instance\\n\\nThe following request creates the consumer instance:\\n\\n```\\ncurl -X POST -H \\"Content-Type: application/vnd.kafka.v1+json\\" \\\\\\n      --data \'{\\"name\\": \\"iot_json_consumer\\", \\"format\\": \\"json\\", \\"auto.offset.reset\\": \\"earliest\\"}\' \\\\\\n      http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json\\n```\\n\\nThe response from the server looks like:\\n\\n``` json\\n{\\n  \\"instance_id\\":\\"iot_json_consumer\\",\\n  \\"base_uri\\":\\"http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json/instances/iot_json_consumer\\"\\n}\\n```\\n\\nNote that we have used the `/consumers/[topic_name]` to create the consumer.\\n\\nThe `base_uri` will be used by the subsequent requests to get the messages from the topic. Like any MapR Streams/Kafka consumer the `auto.offset.reset` defines its behavior. In this example the value is set to `earliest`, this means that the consumer will read the messages from the beginning. You can find more information about the consumer configuration in the [MapR Streams documentation](http://maprdocs.mapr.com/home/MapR_Streams/configuration_parameters_for_consumers.html).\\n\\n\\n#### Consuming the messages\\n\\nTo consume the messages, just add the Mapr Streams topic to the URL of the consumer isntance.\\n\\nThe following request consumes the messages from the topic:\\n\\n```\\ncurl -X GET -H \\"Accept: application/vnd.kafka.json.v1+json\\" \\\\\\nhttp://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json/instances/iot_json_consumer/topics/%2Fapps%2Fiot-stream%3Asensor-json\\n```\\n\\nThis call returns the messages in a JSON document:\\n\\n```json\\n[\\n  {\\"key\\":null,\\"value\\":{\\"temp\\":10,\\"speed\\":40,\\"direction\\":\\"NW\\"},\\"topic\\":\\"/apps/iot-stream:sensor-json\\",\\"partition\\":1,\\"offset\\":1},\\n  {\\"key\\":null,\\"value\\":{\\"temp\\":12,\\"speed\\":42,\\"direction\\":\\"NW\\"},\\"topic\\":\\"/apps/iot-stream:sensor-json\\",\\"partition\\":1,\\"offset\\":2},\\n  {\\"key\\":null,\\"value\\":{\\"temp\\":10,\\"speed\\":37,\\"direction\\":\\"N\\"},\\"topic\\":\\"/apps/iot-stream:sensor-json\\",\\"partition\\":1,\\"offset\\":3}\\n]\\n```\\n\\nEach call to the API returns the new messages published, based on the offset of the last call.\\n\\nNote that the Consumer will be destroyed:\\n\\n* after some idle time set by the `consumer.instance.timeout.ms` (default value set to 300000ms / 5 minutes)\\n* where it is destroyed using a REST API call (see below).\\n\\n\\n### Consuming binary format messages\\n\\nThe approach is the same if you need to consume binary messages, you need to change the format and accept header.\\n\\nCall this URL to create a consumer instance for the binary topic:\\n\\n```\\ncurl -X POST -H \\"Content-Type: application/vnd.kafka.v1+json\\" \\\\\\n      --data \'{\\"name\\": \\"iot_binary_consumer\\", \\"format\\": \\"binary\\", \\"auto.offset.reset\\": \\"earliest\\"}\' \\\\\\n      http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-binary\\n```\\n\\nThen consume messages, the accept header is set to `application/vnd.kafka.binary.v1+json`:\\n\\n```\\ncurl -X GET -H \\"Accept: application/vnd.kafka.binary.v1+json\\" \\\\\\nhttp://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-binary/instances/iot_binary_consumer/topics/%2Fapps%2Fiot-stream%3Asensor-binary\\n\\n```\\n\\nThis call returns the messages in a JSON document, and the value is encoded in Base64\\n\\n```json\\n[\\n  {\\"key\\":null,\\"value\\":\\"SGVsbG8gV29ybGQ=\\",\\"topic\\":\\"/apps/iot-stream:sensor-binary\\",\\"partition\\":1,\\"offset\\":1},\\n  {\\"key\\":null,\\"value\\":\\"Qm9uam91cg==\\",\\"topic\\":\\"/apps/iot-stream:sensor-binary\\",\\"partition\\":1,\\"offset\\":2}\\n]\\n```\\n\\n### Delete consumer instances\\n\\nAs mentioned before the consumer will be destroyed automatically based on the `consumer.instance.timeout.ms` configuration of the REST Proxy;\\nit is also possible to destroyed the instance using the consumer instance URI and an HTTP DELETE call, as follow:\\n\\n```\\ncurl -X DELETE http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-binary/instances/iot_binary_consumer\\n```\\n\\n\\n## Conclusion\\n\\nIn this article you have learned how to use the Kafka REST Proxy for MapR Streams that allow any application to\\nuse messages published in the MapR Converged Data Platform.\\n\\nYou can find more information about the Kafka REST Proxy in the [MapR documentation](http://maprdocs.mapr.com/home/Kafka/REST-proxy.html) and the following resources:\\n\\n* [Getting Started with MapR Streams](https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams)\\n* [\\"Streaming Architecture: New Designs Using Apache Kafka and MapR Streams\\" ebook by Ted Dunning and Ellen Friedman](https://www.mapr.com/streaming-architecture-using-apache-kafka-mapr-streams)"},{"id":"/2017/01/02/getting-started-with-mqtt","metadata":{"permalink":"/blog/2017/01/02/getting-started-with-mqtt","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2017-01-02-getting-started-with-mqtt.md","source":"@site/blog/2017-01-02-getting-started-with-mqtt.md","title":"Getting Started with MQTT and Java","description":"MQTT (MQ Telemetry Transport) is a lightweight publish/subscribe messaging protocol.","date":"2017-01-02T00:00:00.000Z","formattedDate":"January 2, 2017","tags":[],"readingTime":3.485,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting Started with MQTT and Java","categories":"iot java mqtt streaming"},"prevItem":{"title":"Getting Started with Kafka REST Proxy for MapR Streams","permalink":"/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams"},"nextItem":{"title":"Getting started with Apache Flink and Mapr Streams","permalink":"/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams"}},"content":"MQTT (MQ Telemetry Transport) is a lightweight publish/subscribe messaging protocol.\\nMQTT is used a lot in the Internet of Things applications, since it has been designed to\\nrun on remote locations with system with small footprint.\\n\\nThe MQTT 3.1 is an OASIS standard, and you can find all the information at http://mqtt.org/\\n\\nThis article will guide you into the various steps to run your first MQTT application:\\n\\n1. Install and Start a MQTT Broker\\n2. Write an application that publishes messages\\n3. Write an application that consumes messages\\n\\nThe source code of the sample application is available on [GitHub](https://github.com/tgrall/mqtt-sample-java).\\n\\n\x3c!-- truncate --\x3e\\n\\n#### Prerequisites\\n\\n* Apache Maven 3.x\\n* Git\\n\\n### Install and Start a MQTT Broker\\n\\nYou can find many MQTT Brokers, for this example I will use one of the most common broker [Mosquitto](https://mosquitto.org).\\n\\nYou can download and install from the [binary package](https://mosquitto.org/download/). I have used [Homebrew](http://brew.sh/) to install it on my Mac:\\n\\n```\\n$ brew install mosquitto\\n```\\n\\nStart the MQTT Broker with the default configuration\\n\\n```\\n$ /usr/local/sbin/mosquitto\\n```\\n\\n### Publish and Consume messages\\n\\nOpen two terminal windows and run the following commands :\\n\\nConsume\\n\\n```\\n$ mosquitto_sub -h 127.0.0.1 -t iot_data\\n```\\n\\nPublish\\n\\n```\\n$ mosquitto_pub -h 127.0.0.1 -t iot_data -m \\"Hello world\\"\\n```\\n\\nYou should see the message `Hello world` in the consumer/subscriber window.\\n\\n### Write your first MQTT Application\\n\\nFor this example I will write a small Java application, since it is the language\\nthat I am using in my global project.\\n\\n#### Maven Dependencies\\n\\nAdd the [Eclipse Paho](https://eclipse.org/paho/) dependency to your Maven project\\n\\n```xml\\n<dependency>\\n  <groupId>org.eclipse.paho</groupId>\\n  <artifactId>org.eclipse.paho.client.mqttv3</artifactId>\\n  <version>1.1.0</version>\\n</dependency>\\n\\n```\\n\\n#### Publishing a Message\\n\\nPublishing a message is quite easy, create a MqttClient and use it to post on a topic.\\n\\n``` java\\nMqttClient client = new MqttClient(\\"tcp://localhost:1883\\", MqttClient.generateClientId());\\nclient.connect();\\nMqttMessage message = new MqttMessage();\\nmessage.setPayload(\\"Hello world from Java\\".getBytes());\\nclient.publish(\\"iot_data\\", message);\\nclient.disconnect();\\n```\\n\\nYou have many other options, configurations that you can use when posting a message\\nsuch as security, quality of service (QoS), and more; but in this post I want to simply\\nshow how easy is to publish and consume MQTT messages.\\n\\n#### Consuming messages\\n\\nTo consume messages you need to implement a `org.eclipse.paho.client.mqttv3.MqttCallback` that will receive the message and used this Callback class in the MqttClient of the Subscriber application.\\n\\nThe Callback class:\\n\\n```java\\npublic class SimpleMqttCallBack implements MqttCallback {\\n\\n  public void connectionLost(Throwable throwable) {\\n    System.out.println(\\"Connection to MQTT broker lost!\\");\\n  }\\n\\n  public void messageArrived(String s, MqttMessage mqttMessage) throws Exception {\\n    System.out.println(\\"Message received:\\\\n\\\\t\\"+ new String(mqttMessage.getPayload()) );\\n  }\\n\\n  public void deliveryComplete(IMqttDeliveryToken iMqttDeliveryToken) {\\n    // not used in this example\\n  }\\n}\\n```\\n\\nThis Callback class is used in the Subscriber application as follow:\\n\\n```java\\nMqttClient client=new MqttClient(\\"tcp://localhost:1883\\", MqttClient.generateClientId());\\nclient.setCallback( new SimpleMqttCallBack() );\\nclient.connect();\\n```\\n\\nLike for the publisher, I am using the broker and client without any option (QoS, security).\\n\\n## Build and Run the Application\\n\\n**1- Get the Sample Code**\\n\\nClone the project from GitHub\\n\\n```\\n$ git clone https://github.com/tgrall/mqtt-sample-java.git\\n```\\n\\n\\n**2- Build the project with Apache Maven:**\\n\\nThis project is a simple Java application that runs a publisher and subscriber using the [Eclipse Paho library](https://eclipse.org/paho/).\\n\\n\\n```\\n$ mvn clean package\\n```\\n\\nFor convenience, the example programs project is set up so that the maven package target produces a single executable,\\n`/mqtt-sample `, that includes all of the example programs and dependencies.\\n\\n\\n**3- Run the Subscriber**\\n\\nThe subscriber will receive and print all messages published on the `iot_data` topic.\\n\\n```\\n$ ./target/mqtt-sample subscriber\\n```\\n\\n**4- Run the Publisher**\\n\\nRun the publisher with the following command, the second parameter is the message to publish\\n\\n```\\n$ ./target/mqtt-sample publisher \\"My first MQTT message...\\"\\n```\\n\\n## Conclusion\\n\\nIn this article you have learned how to:\\n\\n* Install and start a MQTT Broker, Mosquitto\\n* Create a publisher and subscriber developed in Java\\n\\nThis article is very simple by choice, to quickly run your first MQTT Application. I wrote this article as part of a global IoT project I am working on that will capture devices data, publish them into MapR Converged Data Platform using MQTT and MapR Streams; this is why I used Java for the application. You can use any MQTT client library to build the publishers and subscribers."},{"id":"/2016/10/17/getting-started-with-apache-flink-and-mapr-streams","metadata":{"permalink":"/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-10-17-getting-started-with-apache-flink-and-mapr-streams.md","source":"@site/blog/2016-10-17-getting-started-with-apache-flink-and-mapr-streams.md","title":"Getting started with Apache Flink and Mapr Streams","description":"Introduction","date":"2016-10-17T00:00:00.000Z","formattedDate":"October 17, 2016","tags":[],"readingTime":6.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting started with Apache Flink and Mapr Streams","categories":"streaming flink mapr"},"prevItem":{"title":"Getting Started with MQTT and Java","permalink":"/blog/2017/01/02/getting-started-with-mqtt"},"nextItem":{"title":"Getting started with Apache Flink and Kafka","permalink":"/blog/2016/10/12/getting-started-with-apache-flink-and-kafka"}},"content":"## Introduction\\n\\n[Apache Flink](https://flink.apache.org/) is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.\\n\\nIt is very common for Flink applications to use [Apache Kafka](http://kafka.apache.org/) for data input and output.\\n\\nThis article will guide you into  the steps to use Apache Flink with [MapR Streams](https://www.mapr.com/products/mapr-streams). MapR Streams is a distributed messaging system for streaming event data at scale, and it\u2019s integrated into the [MapR Converged Data Platform](https://www.mapr.com/products/mapr-converged-data-platform), based on the Apache Kafka API (0.9.0), \\nthis article use the same code and approach than the [Flink and Kafka Getting Started](http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/).\\n\\n![MapR Streams and Flink](http://tgrall.github.io/images/posts/flink-kafka/flink-mapr-streams.png).\\n\\n\x3c!-- truncate --\x3e\\n\\n### Prerequisites\\n\\n* MapR 5.2\\n  * You can use [MapR Converged Data Platform Sandbox](https://www.mapr.com/products/mapr-sandbox-hadoop)\\n* MapR Client installed on your development host\\n  *  [Installation and Configuration steps](http://maprdocs.mapr.com/home/AdvancedInstallation/SettingUptheClient-install-mapr-client.html)\\n* Git\\n* Maven 3.x or later\\n\\n\\n\\n\\n## Create your Flink Streaming Project\\n\\nThe first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the [Apache Flink Quick Start Example](https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html), with a clear focus on data input and output with MapR Streams. \\n\\nIn this application we will create two jobs:\\n\\n* `WriteToKafka` : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.\\n* `ReadFromKafka` : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.\\n\\nThe full project is available on GitHub:\\n\\n* [MapR Streams Flink Demo](https://github.com/mapr-demos/mapr-streams-flink-demo)\\n\\n\\n\\nLet\u2019s create the project using Apache Maven:\\n\\n```\\nmvn archetype:generate \\\\\\n      -DarchetypeGroupId=org.apache.flink\\\\\\n      -DarchetypeArtifactId=flink-quickstart-java \\\\\\n      -DarchetypeVersion=1.1.2 \\\\\\n      -DgroupId=com.mapr.demos \\\\\\n      -DartifactId=mapr-streams-flink-demo \\\\\\n      -Dversion=1.0-SNAPSHOT \\\\\\n      -DinteractiveMode=false \\n\\n```\\n\\nMaven will create the following structure:\\n\\n```\\n$ tree mapr-streams-flink-demo/\\nmapr-streams-flink-demo/\\n\u251c\u2500\u2500 pom.xml\\n\u2514\u2500\u2500 src\\n    \u2514\u2500\u2500 main\\n        \u251c\u2500\u2500 java\\n        \u2502   \u2514\u2500\u2500 com\\n        \u2502       \u2514\u2500\u2500 mapr\\n        \u2502           \u2514\u2500\u2500 demos\\n        \u2502               \u251c\u2500\u2500 BatchJob.java\\n        \u2502               \u251c\u2500\u2500 SocketTextStreamWordCount.java\\n        \u2502               \u251c\u2500\u2500 StreamingJob.java\\n        \u2502               \u2514\u2500\u2500 WordCount.java\\n        \u2514\u2500\u2500 resources\\n            \u2514\u2500\u2500 log4j.properties\\n```   \\n\\nThis project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.\\n\\nThe project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.\\n\\n## Add Kafka & MapR Streams Dependencies\\n\\nOpen the `pom.xml` and add the following dependencies to your project:\\n\\n**1- Add MapR Maven Repository**\\n\\nIn the `<repositories>` element add :\\n\\n```\\n   <repository>\\n     <id>mapr-releases</id>\\n     <url>http://repository.mapr.com/maven/</url>\\n     <snapshots><enabled>false</enabled></snapshots>\\n     <releases><enabled>true</enabled></releases>\\n   </repository>\\n```\\n\\n**2- Add MapR Streams libraries**\\n\\nIn the `<dependencies>` element:\\n\\n```\\n <dependency>\\n   <groupId>com.mapr.streams</groupId>\\n   <artifactId>mapr-streams</artifactId>\\n   <version>5.2.0-mapr</version>\\n </dependency>\\n <dependency>\\n   <groupId>org.apache.kafka</groupId>\\n   <artifactId>kafka-clients</artifactId>\\n   <version>0.9.0.0-mapr-1602</version>\\n </dependency>\\n```\\n\\n\\n**3- Add Flink Kafka Connector libraries**\\n\\t\\nAs a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:\\n\\nYou must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the `<dependencies>` element:\\n\\n```\\n <dependency>\\n      <groupId>org.apache.flink</groupId>\\n      <artifactId>flink-connector-kafka-0.9_2.10</artifactId>\\n      <version>${flink.version}</version>\\n </dependency>\\n```\\n\\n**4- Exclude Kafka Client to allow use of MapR Streams Client**\\n\\nAs you may know, MapR Streams uses the Kafka 0.9.0 API to produce and consume messages. So we need now to remove (exclude) tha Apache Kafka Client API to be sure that Flink use MapR Streams.\\n\\nIn the Flink Kafka Connector dependency add the following exclusion:\\n\\n```\\n  <dependency>\\n    <groupId>org.apache.flink</groupId>\\n    <artifactId>flink-connector-kafka-0.9_2.10</artifactId>\\n    <version>${flink.version}</version>\\n      <exclusions>\\n        <exclusion>\\n          <groupId>org.apache.kafka</groupId>\\n          <artifactId>kafka-clients</artifactId>\\n        </exclusion>\\n        <exclusion>\\n          <groupId>org.apache.kafka</groupId>\\n          <artifactId>kafka_2.10</artifactId>\\n        </exclusion>\\n      </exclusions>\\n  </dependency>\\n```\\n\\nThe Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from MapR Streams.\\n\\nLet\u2019s now create a Stream in MapR and write some simple Flink code to use it.\\n\\n## Create the MapR Streams and Topic\\n\\nA stream is a collection of topics that you can manage as a group by:\\n\\n1. Setting security policies that apply to all topics in that stream\\n2. Setting a default number of partitions for each new topic that is created in the stream\\n3. Set a time-to-live for messages in every topic in the stream\\n\\nYou can find more information about MapR Streams concepts in the [documentation](http://maprdocs.mapr.com/51/MapR_Streams/concepts.html).\\n\\nOn your Mapr Cluster or Sandbox run the following commands: \\n\\n```\\n$ maprcli stream create -path /apps/application-stream -produceperm p -consumeperm p -topicperm p\\n$ maprcli stream topic create -path /apps/application-stream -topic flink-demo \\n```\\n\\n\\n### Install and use MapR Kafka utilities\\n\\nInstall `the mapr-kafka` package on your cluster :\\n\\n```\\nyum install mapr-kafka\\n```\\n\\nOpen two terminal windows and run the producer and consumer kafka utilities using the following commands:\\n\\nProducer\\n\\n```\\n/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-producer.sh --broker-list this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo\\n```\\n\\nConsumer\\n\\n```\\n/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo\\n```\\n\\nIn the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between MapR Streams and Flink.\\n\\n\\n## Write your Flink application\\n\\nLet\u2019s now use the Flink Kafka Connector to send messages to MapR Streams and consume them.\\n\\n### Producer\\n\\nThe producer generates messages using the `SimpleStringGenerator()` class and send the string to the `/apps/application-stream:flink-demo` topic.\\n\\n```\\n  public static void main(String[] args) throws Exception {\\n    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\n\\n    Properties properties = new Properties();\\n    // properties.setProperty(\\"bootstrap.servers\\", \\"<kafka-broker>:9092\\"); // not used by MapR Streams\\n    properties.setProperty(\\"streams.buffer.max.time.ms\\", \\"200\\");\\n\\n    DataStream<String> stream = env.addSource(new SimpleStringGenerator());\\n    stream.addSink(new FlinkKafkaProducer09<>(\\"/apps/application-stream:flink-demo\\", new SimpleStringSchema(), properties));\\n\\n    env.execute();\\n  }\\n    \\n```\\n\\nThe `SimpleStringGenerator()` method code is available [here](https://github.com/mapr-demos/mapr-streams-flink-demo/blob/master/src/main/java/com/mapr/demos/WriteToKafka.java#L46-L61).\\n\\n\\nThe main steps are:\\n\\n* create a new `StreamExecutionEnvironment` the basis of any Flink application\\n* create a new `DataStream` in the application environment, the `SimpleStringGenerator` class implements the `[SourceFunction](https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/)` the base interface for all streams data sources in Flink.\\n* add the `FlinkKafkaProducer09` sink to the streams; since MapR Streams is based on Kafka API 0.9, it is possible to use the FlinkKafkaProducer09 class; with 2 small differences:\\n\\t* the broker list (first parameter) is not used since MapR Streams use the cluster location defined in the `/opt/mapr/conf/mapr-clusters.conf` class.\\n\\t* the topic name include the path and name of the MapR Stream stream in which the topic is located for example `/apps/application-stream:flink-demo`\\n\\n### Consumer\\n\\nThe consumer simply reads the messages from the `/apps/application-stream:flink-demo` topic, and print them into the console.\\n\\n```\\n  public static void main(String[] args) throws Exception {\\n    // create execution environment\\n    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\n\\n    Properties properties = new Properties();\\n    // properties.setProperty(\\"bootstrap.servers\\", \\"<kafka-broker>:9092\\"); // not used by MapR Streams\\n    properties.setProperty(\\"group.id\\", \\"flink_consumer\\");\\n\\n    DataStream<String> stream = env.addSource(new FlinkKafkaConsumer09<>(\\n    \\t\\"/apps/application-stream:flink-demo\\", new SimpleStringSchema(), properties) );\\n\\n    stream.map(new MapFunction<String, String>() {\\n      private static final long serialVersionUID = -6867736771747690202L;\\n\\n      @Override\\n      public String map(String value) throws Exception {\\n        return \\"Stream Value: \\" + value;\\n      }\\n    }).print();\\n\\n    env.execute();\\n  }\\n  ```\\n  \\nThe main steps are:\\n\\n* create a new `StreamExecutionEnvironment` the basis of any Flink application\\n* create a set of properties with the consumer information, in this application we can only set the consumer `group.id`. Note that the `bootstrap.servers` property is not used by MapR Streams, so no need to set it.\\n* use the `FlinkKafkaConsumer09` to get the message from the MapR Streams topic `/apps/application-stream:flink-demo`\\n\\n\\n\\n## Build and Run the application\\n\\nLet\u2019s run the application directly from Maven (or from your favorite IDE).\\n\\n1- Build the project:\\n\\n```\\n$ mvn clean package\\n```\\n\\n2- Run the Flink Producer Job\\n\\n```\\n$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka\\n```\\n\\n3- Run the Flink Consumer Job\\n\\n```\\n$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka\\n```\\n\\nIn the terminal, you should see the messages generated from the producer\\n\\n\\nYou can now deploy and execute this job on your Flink cluster.\\n\\n## Conclusion\\n\\nIn this article you have learned how to use Flink with MapR Streams to write and read data streams. The key element is the configuration of the Maven Dependencies to configure the project to use MapR Streams libraries instead of Kafka ones.\\n\\nThis was originally published on the [MapR blog here](https://www.mapr.com/blog/getting-started-apache-flink-and-mapr-streams).\\n\\nLearn about what Apache Flink can do and how it maintains consistency and provides flexibility in the \\"[Introduction to Apache Flink](https://www.mapr.com/introduction-to-apache-flink)\\" ebook."},{"id":"/2016/10/12/getting-started-with-apache-flink-and-kafka","metadata":{"permalink":"/blog/2016/10/12/getting-started-with-apache-flink-and-kafka","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-10-12-getting-started-with-apache-flink-and-kafka.md","source":"@site/blog/2016-10-12-getting-started-with-apache-flink-and-kafka.md","title":"Getting started with Apache Flink and Kafka","description":"Introduction","date":"2016-10-12T00:00:00.000Z","formattedDate":"October 12, 2016","tags":[],"readingTime":4.805,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Getting started with Apache Flink and Kafka","categories":"kafka flink streaming how-to"},"prevItem":{"title":"Getting started with Apache Flink and Mapr Streams","permalink":"/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams"},"nextItem":{"title":"Streaming Analytics in a Digitally Industrialized World","permalink":"/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world"}},"content":"## Introduction\\n\\n[Apache Flink](https://flink.apache.org/) is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.\\n\\nIt is very common for Flink applications to use [Apache Kafka](http://kafka.apache.org/) for data input and output. This article will guide you into  the steps to use Apache Flink with Kafka.\\n\\n![]( center /images/posts/flink-kafka/flink-kafka.png Flink-Kafka )\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n### Prerequisites\\n\\n* Apache Kafka 0.9.x\\n* Git\\n* Maven 3.x or later\\n\\n\\n## Create your Flink Streaming Project\\n\\nThe first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the [Apache Flink Quick Start Example](https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html), with a clear focus on data input and output with MapR Streams. \\n\\nIn this application we will create two jobs:\\n\\n* `WriteToKafka` : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.\\n* `ReadFromKafka` : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.\\n\\nThe full project is available on GitHub:\\n\\n* [Flink and Kakfa Application](https://github.com/tgrall/kafka-flink-101)\\n\\n\\nLet\u2019s create the project using Apache Maven:\\n\\n``` bash\\nmvn archetype:generate \\\\\\n      -DarchetypeGroupId=org.apache.flink\\\\\\n      -DarchetypeArtifactId=flink-quickstart-java \\\\\\n      -DarchetypeVersion=1.1.2 \\\\\\n      -DgroupId=com.grallandco.demos \\\\\\n      -DartifactId=kafka-flink-101 \\\\\\n      -Dversion=1.0-SNAPSHOT \\\\\\n      -DinteractiveMode=false \\n\\n```\\n\\nMaven will create the following structure:\\n\\n``` bash\\ntree kafka-flink-101/\\nkafka-flink-101/\\n\u251c\u2500\u2500 pom.xml\\n\u2514\u2500\u2500 src\\n    \u2514\u2500\u2500 main\\n        \u251c\u2500\u2500 java\\n        \u2502\xa0\xa0 \u2514\u2500\u2500 com\\n        \u2502\xa0\xa0     \u2514\u2500\u2500 grallandco\\n        \u2502\xa0\xa0         \u2514\u2500\u2500 demos\\n        \u2502\xa0\xa0             \u251c\u2500\u2500 BatchJob.java\\n        \u2502\xa0\xa0             \u251c\u2500\u2500 SocketTextStreamWordCount.java\\n        \u2502\xa0\xa0             \u251c\u2500\u2500 StreamingJob.java\\n        \u2502\xa0\xa0             \u2514\u2500\u2500 WordCount.java\\n        \u2514\u2500\u2500 resources\\n            \u2514\u2500\u2500 log4j.properties\\n\\n7 directories, 6 files\\n```   \\n\\nThis project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.\\n\\nThe project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.\\n\\n## Add Kafka Connector\\n\\nOpen the `pom.xml` and add the following dependencies to your project:\\n\\t\\nAs a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:\\n\\nYou must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the `<dependencies>` element:\\n\\n```\\n <dependency>\\n      <groupId>org.apache.flink</groupId>\\n      <artifactId>flink-connector-kafka-0.9_2.10</artifactId>\\n      <version>${flink.version}</version>\\n </dependency>\\n```\\n\\nThe Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from Apache Kafka.\\n\\n\\n## Install and Start Kafka\\n\\nDownload Kafka, enter the following commands in your terminal:\\n\\n\\n``` bash\\ncurl -O http://www.us.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz\\ntar -xzf kafka_2.11-0.9.0.0.tgz\\ncd kafka_2.11-0.9.0.0\\n```\\n\\nKafka uses ZooKeeper, if you do not have Zookeeper running, you can start it using the following command:\\n\\n```bash\\n./bin/zookeeper-server-start.sh config/zookeeper.properties\\n```\\n\\nStart a Kafka broker by running the following command in a new terminal:\\n\\n``` bash\\n./bin/kafka-server-start.sh config/server.properties\\n```\\n\\nIn another terminal, run the following command to create a Kafka topic called `flink-demo`:\\n\\n``` bash\\n./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic flink-demo\\n\\n```\\n\\nUse the Kafka tools to post and consume messages to the `flink-demo` topic.\\n\\nProducer\\n\\n```bash\\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic flink-demo\\n```\\n\\nConsumer\\n\\n```bash\\n./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic flink-demo --from-beginning\\n```\\n\\n\\nIn the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between Kafka and Flink.\\n\\n\\n## Write your Flink application\\n\\nLet\u2019s now use the Flink Kafka Connector to send messages to Kafka and consume them.\\n\\n### Producer\\n\\nThe producer generates messages using the `SimpleStringGenerator()` class and send the string to the `flink-demo` topic.\\n\\n```\\n  public static void main(String[] args) throws Exception {\\n    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\n\\n    Properties properties = new Properties();\\n    properties.setProperty(\\"bootstrap.servers\\", \u201clocalhost:9092\\"); \\n\\n    DataStream<String> stream = env.addSource(new SimpleStringGenerator());\\n    stream.addSink(new FlinkKafkaProducer09<>(\\"flink-demo\\", new SimpleStringSchema(), properties));\\n\\n    env.execute();\\n  }\\n    \\n```\\n\\nThe `SimpleStringGenerator()` method code is available [here](https://github.com/tgrall/kafka-flink-101/blob/master/src/main/java/com/grallandco/demos/WriteToKafka.java#L45-L60).\\n\\n\\nThe main steps are:\\n\\n* create a new `StreamExecutionEnvironment` the basis of any Flink application\\n* create a new `DataStream` in the application environment, the `SimpleStringGenerator` class implements the [SourceFunction](https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/) the base interface for all streams data sources in Flink.\\n* add the `FlinkKafkaProducer09` sink to the topic. \\n\\n\\n### Consumer\\n\\nThe consumer simply reads the messages from the `flink-demo` topic, and print them into the console.\\n\\n```\\n  public static void main(String[] args) throws Exception {\\n    // create execution environment\\n    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\n\\n    Properties properties = new Properties();\\n    properties.setProperty(\\"bootstrap.servers\\", \u201clocalhost:9092\\");\\n    properties.setProperty(\\"group.id\\", \\"flink_consumer\\");\\n\\n    DataStream<String> stream = env.addSource(new FlinkKafkaConsumer09<>(\\n    \\t\\"flink-demo\\", new SimpleStringSchema(), properties) );\\n\\n    stream.map(new MapFunction<String, String>() {\\n      private static final long serialVersionUID = -6867736771747690202L;\\n\\n      @Override\\n      public String map(String value) throws Exception {\\n        return \\"Stream Value: \\" + value;\\n      }\\n    }).print();\\n\\n    env.execute();\\n  }\\n\\n```\\n\\nThe main steps are:\\n\\n* create a new `StreamExecutionEnvironment` the basis of any Flink application\\n* create a set of properties with the consumer information, in this application we can only set the consumer `group.id`. \\n* use the `FlinkKafkaConsumer09` to get the message from the topic `flink-demo`\\n\\n\\n## Build and Run the application\\n\\nLet\u2019s run the application directly from Maven (or from your favorite IDE).\\n\\n1- Build the project:\\n\\n```\\n$ mvn clean package\\n```\\n\\n2- Run the Flink Producer Job\\n\\n```\\n$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka\\n```\\n\\n3- Run the Flink Consumer Job\\n\\n```\\n$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka\\n```\\n\\nIn the terminal, you should see the messages generated from the producer\\n\\nYou can now deploy and execute this job on your Flink cluster.\\n\\n## Conclusion\\n\\nIn this article you have learned how to use Flink with kafka to write and read data streams.\\n\\nLearn about what Apache Flink can do and how it maintains consistency and provides flexibility in the \\"[Introduction to Apache Flink](https://www.mapr.com/introduction-to-apache-flink)\\" ebook."},{"id":"/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world","metadata":{"permalink":"/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-09-26-streaming-analytics-in-a-digitally-industrialized-world.md","source":"@site/blog/2016-09-26-streaming-analytics-in-a-digitally-industrialized-world.md","title":"Streaming Analytics in a Digitally Industrialized World","description":"Get an introduction to streaming analytics, which allows you real-time insight from captured events and big data. There are applications across industries, from finance to wine making, though there are two primary challenges to be addressed.","date":"2016-09-26T00:00:00.000Z","formattedDate":"September 26, 2016","tags":[],"readingTime":3.69,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Streaming Analytics in a Digitally Industrialized World","categories":"streaming analytics bigdata"},"prevItem":{"title":"Getting started with Apache Flink and Kafka","permalink":"/blog/2016/10/12/getting-started-with-apache-flink-and-kafka"},"nextItem":{"title":"Setting up Spark Dynamic Allocation on MapR","permalink":"/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr"}},"content":"Get an introduction to streaming analytics, which allows you real-time insight from captured events and big data. There are applications across industries, from finance to wine making, though there are two primary challenges to be addressed.\\n\\nDid you know that a plane flying from Texas to London can generate 30 million data points per flight? As Jim Daily of GE Aviation notes, that equals [10 billion data points](https://www.ge.com/digital/blog/industrial-iot-improving-airline-economics) in one year. And we\u2019re talking about one plane alone. So you can understand why [another top GE executive recently told Ericsson Business Review](http://cloudblog.ericsson.com/cloud-scalability-combined-with-speed-inside-ges-cloud-transformation) that \\"Cloud is the future of IT,\\" with a focus on supporting challenging applications in industries such as aviation and energy.\\n\\n\x3c!-- truncate --\x3e\\n\\n### The benefits of big data\\n\\nToday, thanks to modern big data platforms, many companies are able to take advantage of the same methods as industry giant GE to store, process, and analyze massive amounts of data. This means that you can also capture core business data, such as that coming from a CRM system, or traffic sensors, or say jet engines, and associate it to other data such as social, application, blog or industrial data. Ultimately, this will result in greater data insights and can enable things like better customer segmentation and prediction.\\n\\nSounds great, right? There\u2019s a catch. The challenge is that these data are processed in batch mode, meaning that you have to wait a few hours or even days to access relevant KPI\u2019s and insights. Not only is there a delay, but analysis is based on data that\u2019s out of date \u2013 even if only by a few hours.\\n\\n### Analysis of big event streams\\n\\nThat\u2019s where streaming analytics comes in. Streaming analytics is the [analysis of large event streams](http://searchcloudapplications.techtarget.com/opinion/Streaming-analytics-lets-you-view-the-past-to-see-the-future?utm_medium=EM&asrc=EM_NLN_58517129&utm_campaign=20160606_Seeing%20the%20future...with%20the%20past?_fchurchville&utm_source=NLN&track=NL-1839&ad=908120&src=908120), which is data that is in constant movement. These streams can include actions that can be incredibly small, say one click, yet result in an explosion of data. The benefit is that you can capture events and data as they happen, delivering value to the enterprise in near real time.\\n\\nWhat does streaming analytics look like regarding the previously mentioned CRM system? It means that an enterprise can get immediate feedback on something like a specific marketing campaign, website update, or product alteration. When a user clicks within one of these realms and an order is immediately processed, that information is pushed out in real time to various tools, allowing the enterprise to adjust its customer interaction.\\n\\n### Streaming analytics in practice\\n\\nWhere else is streaming analytics applicable? Think about the benefits of fraud detection in the financial sector, or the growth of sensors in manufacturing, or collection of data from large scale machines. One example is where Ericsson has collaborated with [vintners in Germany, using IoT to improve traditional harvesting methods](http://cloudblog.ericsson.com/can-the-networked-society-and-iot-make-better-wine) through greater precision and speed of response to outside factors. In these streaming analytics scenarios, the key is \u201c[High-Frequency Decisioning,](https://www.mapr.com/blog/lets-get-real-acting-data-real-time)\u201d where you move from knowing to doing in real-time synergy. Time to action is compressed, resulting in dramatic results like achieving greater user satisfaction, higher revenue, or reduced risk.\\n\\n### Two primary challenges with streaming analytics\\n\\nStreaming analytics is not without challenges. Systems must capture events in near real time, at scale, and in a distributed fashion. And of course, events must be stored and processed in real time. Since big data is generated one event at a time, you need to have an incredibly powerful storage and processing layer, which can provide deep analytics and rich features like machine learning systems.\\n\\nTo take on the first challenge, new messaging systems like [Apache Kafka](http://kafka.apache.org) and [MapR Streams](https://www.mapr.com/products/mapr-streams) provide a common API for developers to publish and subscribe to any event. And to process and store data in real-time, one of the most efficient methods would be to use a [distributed file system](https://www.mapr.com/products/mapr-fs) and [NoSQL databases](https://www.mapr.com/products/mapr-db-in-hadoop-nosql). This provides horizontal scalability and flexibility. A storage system must also process and do analytics calculations in real time in a distributed manner.\\n\\nTools such as [Apache Spark](https://www.mapr.com/products/apache-spark) and [Flink](http://flink.apache.org) come to mind, which provide rich analytical functions that can be integrated with any tool, including real time alerting systems. It is also interesting to mention that all the events, data, that are now stored in real time continue to be accessible with traditional analytics tools; thanks to the distributed, and scalable distributed SQL Engines provided in modern big data platforms.\\n\\nThis was originally published on the [Ericsson Cloud blog here](http://cloudblog.ericsson.com/streaming-analytics-of-big-data-in-real-time)."},{"id":"/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr","metadata":{"permalink":"/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-09-01-setting-up-spark-dynamic-allocation-on-mapr.md","source":"@site/blog/2016-09-01-setting-up-spark-dynamic-allocation-on-mapr.md","title":"Setting up Spark Dynamic Allocation on MapR","description":"Apache Spark can use various cluster manager to execute application (Stand Alone, YARN, Apache Mesos). When you install Apache Spark on MapR you can submit application in a Stand Alone mode or using YARN.","date":"2016-09-01T00:00:00.000Z","formattedDate":"September 1, 2016","tags":[],"readingTime":2.155,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Setting up Spark Dynamic Allocation on MapR","categories":"spark yarn mapr"},"prevItem":{"title":"Streaming Analytics in a Digitally Industrialized World","permalink":"/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world"},"nextItem":{"title":"Save MapR Streams messages into MapR DB JSON","permalink":"/blog/2016/03/30/save-mapr-streams-messages-into-mapr-db-json"}},"content":"Apache Spark can use various cluster manager to execute application (Stand Alone, YARN, Apache Mesos). When you install Apache Spark on MapR you can submit application in a Stand Alone mode or using YARN.\\n\\nThis article focuses on YARN and Dynamic Allocation, a feature that lets Spark add or remove executors dynamically based on the workload. You can find more information about this feature in this presentation from Databricks:\\n\\n* [Dynamic Allocation in Spark](http://www.slideshare.net/databricks/dynamic-allocation-in-spark)\\n\\nLet\u2019s see how to configure Spark and YARN to use dynamic allocation (that is disabled by default).\\n\\n\x3c!-- truncate --\x3e\\n\\n#### Prerequisites\\n\\n* MapR Converged Data Platform Cluster\\n* Apache Spark for MapR installed\\n\\nThis example has been described for MapR 5.2 with Apache Spark 1.6.1, you just need to adapt the version to your environment.\\n\\n### Enabling Dynamic Allocation in Apache Spark\\n\\nThe first thing to do is to enable Dynamic Allocation in Spark, for this you need to edit the spark configuration file on each Spark node.\\n\\n```\\n/opt/mapr/spark/spark-1.6.1/conf/spark-defaults.conf\\n```\\n\\nand add the following entries:\\n\\n```\\nspark.dynamicAllocation.enabled = true\\nspark.shuffle.service.enabled = true\\nspark.dynamicAllocation.minExecutors = 5 \\nspark.executor.instances = 0\\n```\\n\\nYou can find additional configuration options in the [Apache Spark Documentation](http://spark.apache.org/docs/1.6.1/configuration.html#dynamic-allocation).\\n\\n\\n### Enabling Spark External Shuffle for YARN\\n\\nYou have now to edit YARN configuration to add information about Spark Shuffle Service, edit the following file, on each YARN node:\\n\\n```\\n/opt/mapr/hadoop/hadoop-2.7.0/etc/hadoop/yarn-site.xml\\n```\\n\\nadd these properties: \\n\\n```\\n  <property>\\n    <name>yarn.nodemanager.aux-services</name>\\n    <value>mapreduce_shuffle,mapr_direct_shuffle,spark_shuffle</value>\\n  </property>\\n  <property>\\n    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\\n    <value>org.apache.spark.network.yarn.YarnShuffleService</value>\\n  </property>\\n```\\n\\n#### Add Spark Shuffle to YARN classpath\\n\\nSpark Shuffle service must be added to the YARN classpath. The jar is located in the spark distribution:\\n\\n```\\n/opt/mapr/spark/spark-1.6.1/lib/spark-1.6.1-mapr-1605-yarn-shuffle.jar\\n```\\n\\nTo achieve this add the jar in the following folder on each node:\\n\\n```\\n/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/yarn/lib\\n```\\n\\nYou can either copyy the file or create a symlink:\\n\\n```\\n$ ln -s /opt/mapr/spark/spark-1.6.1/lib/spark-1.6.1-mapr-1605-yarn-shuffle.jar /opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/yarn/lib\\n```\\n\\n#### Restart YARN\\n\\nSince you have changed the YARN configuration *you must restart your node managers* using the following command:\\n\\n```\\n$ maprcli node services -name nodemanager -action restart -nodes [list of nodes]\\n```\\n\\n### Submitting a Spark Job\\n\\nYour MapR Cluster is not ready to use Spark dynamic allocation, this means that when you submit a job you do not need to specify any resource configuration, for example:\\n\\n```\\n/opt/mapr/spark/spark-1.6.1/bin/spark-submit \\\\\\n  --class com.mapr.demo.WordCountSorted \\\\\\n  --master yarn \\\\\\n  ~/spark-examples-1.0-SNAPSHOT.jar \\\\\\n  /mapr/my.cluster.com/input/4gb_txt_file.txt \\\\\\n  /mapr/my.cluster.com/user/mapr/output/\\n```\\n\\nnote that you can still specify the resources, but in this case the dynamic allocation will not be used for this specific job, for example:\\n\\n```\\n/opt/mapr/spark/spark-1.6.1/bin/spark-submit \\\\\\n  --class com.mapr.demo.WordCountSorted \\\\\\n  --master yarn \\\\\\n  --num-executors 3\\n  --executor-memory 1G \\\\\\n  ~/spark-examples-1.0-SNAPSHOT.jar \\\\\\n  /mapr/my.cluster.com/input/4gb_txt_file.txt \\\\\\n  /mapr/my.cluster.com/user/mapr/output/\\n```"},{"id":"/2016/03/30/save-mapr-streams-messages-into-mapr-db-json","metadata":{"permalink":"/blog/2016/03/30/save-mapr-streams-messages-into-mapr-db-json","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-03-30-save-mapr-streams-messages-into-mapr-db-json.md","source":"@site/blog/2016-03-30-save-mapr-streams-messages-into-mapr-db-json.md","title":"Save MapR Streams messages into MapR DB JSON","description":"In this article you will learn how to create a MapR Streams Consumer that saves all the messages into a MapR-DB JSON Table.","date":"2016-03-30T00:00:00.000Z","formattedDate":"March 30, 2016","tags":[],"readingTime":3.005,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Save MapR Streams messages into MapR DB JSON","categories":"kafka streams howto maprdb json mapr"},"prevItem":{"title":"Setting up Spark Dynamic Allocation on MapR","permalink":"/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr"},"nextItem":{"title":"Getting Started with MapR Streams","permalink":"/blog/2016/03/10/getting-started-with-mapr-streams"}},"content":"In this article you will learn how to create a MapR Streams Consumer that saves all the messages into a [MapR-DB JSON Table](http://maprdocs.mapr.com/51/#MapR-DB/JSON_DB/mapr_db_json_top.html).\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n### Install and Run the sample MapR Streams application\\n\\nThe steps to install and run the applications are the same as the one defined in the following article:\\n\\n* [MapR Streams application](https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams)\\n\\nOnce you have the default producer and consumer running in your environment using the commands:\\n\\nProducer:\\n\\n```\\n$ java -cp $(mapr classpath):./mapr-streams-examples-1.0-SNAPSHOT-jar-with-dependencies.jar com.mapr.examples.Run producer\\n```\\n\\nConsumer:\\n\\n```\\n$ java -cp $(mapr classpath):./mapr-streams-examples-1.0-SNAPSHOT-jar-with-dependencies.jar com.mapr.examples.Run consumer\\n```\\n\\n### Save messages into MapR-DB JSON\\n\\nThe [DBConsumer](https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java) class is a copy of the [Consumer](https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/Consumer.java) class with small changes to save the messages coming from the `/sample-stream:fast-messages` topic into a MapR-DB table named `/apps/fast-messages`. \\n\\n**1- Add MapR-DB Maven dependency to your project**\\n\\nEdit the `pom.xml` file and add the following entry in the `dependencies` tag:\\n\\n```xml\\n   <dependency>\\n      <groupId>com.mapr.db</groupId>\\n      <artifactId>maprdb</artifactId>\\n      <version>5.1.0-mapr</version>\\n   </dependency>\\n```\\n\\nThis add support for:\\n\\n* [OJAI](http://ojai.io/) Open JSON Application Interface\\n* [MapR-DB JSON API](http://maprdocs.mapr.com/51/#MapR-DB/JSON_DB/crud_with_maprdb_ojai_java_api.html) \\n\\n**2- Create and Get a JSON Table**\\n\\nTo save the messages, the application must access a JSON Table, for this just call the `MapRDB.getTable(TABLE_PATH)` method. If the table does not exist, create it with the `MapRDB.createTable(TABLE_PATH)`.\\n\\nThis is what the [`DBConsumer.getTable(TABLE_PATH)`](https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L113-L119) method is doing.\\n\\n```java\\n  private static Table getTable(String tablePath) {\\n    if ( ! MapRDB.tableExists(tablePath)) {\\n      return MapRDB.createTable(tablePath);\\n    } else {\\n      return MapRDB.getTable(tablePath);\\n    }\\n  }\\n```\\n\\nWhen the DBConsumer starts the [`getTable(\\"/apps/fast-messages\\")`](https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L41) method is called.\\n\\n```\\n  Table fastMessagesTable = getTable(\\"/apps/fast-messages\\");\\n```\\n\\nThe table `fastMessagesTable` is not available to the consumer.\\n\\n\\n**3- Save messages into the JSON Table**\\n\\nMessages can be saved into the table using the [MapR-DB JSON Java API](https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L78-L81).\\n\\nThe producer sends the message as JSON String that is converted into a JSON object names `msg`. This object can be used to create an OJAI Document:\\n\\n```\\n  Document messageDocument = MapRDB.newDocument(msg);\\n```\\n\\nTo be saved into MapR-DB, a document must have a `_id` field. In this example let\u2019s use the message number generated by the producer *(JSON field `k`)*.\\n\\n```\\n  messageDocument.setId( Integer.toString(messageDocument.getInt(\\"k\\")));\\n```\\n\\nLet\u2019s now save the document into the table:\\n\\n```\\n  fastMessagesTable.insertOrReplace( messageDocument );       \\n```\\n\\nEach time the producer will be executed, the message number counter will be initialized to 0. So the document _id will be the same, and the document into the table must be replaced; this is why the `insertOrReplace` method is used.\\n\\nLet\u2019s run the new consumer.\\n\\n\\n**4- Run the DBConsumer**\\n\\nTo run the DBConsumer just pass the parameter `dbconsumer` as follow:\\n\\nConsumer:\\n\\n```\\n$ java -cp $(mapr classpath):./mapr-streams-examples-1.0-SNAPSHOT-jar-with-dependencies.jar com.mapr.examples.Run dbconsumer\\n```\\n\\nNote that a new [group is created](https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L48-L54) to be sure that messages are read by the two different consumers (Consumer and DBConsumer).\\n\\n**5- Query the messages saved into MapR-DB**\\n\\nMessages are saved into the `/apps/fast-messages` table, let\u2019s used the MapR DBShell to query the data. On your cluster run the following commands, as `mapr`:\\n\\n```\\n$ mapr dbshell\\nmaprdb mapr:> find /apps/fast-messages --id 100\\n{\\"_id\\":\\"100\\",\\"type\\":\\"test\\",\\"t\\":64986.787,\\"k\\":{\\"$numberLong\\":100}}\\n```\\n\\n\\n### Conclusion\\n\\nIn this very simple example, the DBConsumer takes each message and saved it as a simple JSON Document into MapR-DB JSON. The table can be used to create any type of application, or using Apache Drill *(1.6 or later)* to do some analytics.\\n\\nIn a real application the messages will probably be modified, enriched and/or aggregated and then the result be saved into MapR-DB Table. The goal of this sample is just to show that it is easy to integrate MapR Streams and MapR-DB.\\n\\nYou have also other alternative to achieve the same thing using for example:\\n\\n* Spark Streaming\\n* 3rd Party ETL and Tools"},{"id":"/2016/03/10/getting-started-with-mapr-streams","metadata":{"permalink":"/blog/2016/03/10/getting-started-with-mapr-streams","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-03-10-getting-started-with-mapr-streams.md","source":"@site/blog/2016-03-10-getting-started-with-mapr-streams.md","title":"Getting Started with MapR Streams","description":"You can find a new tutorial that explains how to deploy an Apache Kafka application to MapR Streams, the tutorial is available here:","date":"2016-03-10T00:00:00.000Z","formattedDate":"March 10, 2016","tags":[],"readingTime":0.38,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Getting Started with MapR Streams","categories":"kafka streams java howto"},"prevItem":{"title":"Save MapR Streams messages into MapR DB JSON","permalink":"/blog/2016/03/30/save-mapr-streams-messages-into-mapr-db-json"},"nextItem":{"title":"Getting Started with Sample Programs for Apache Kafka 0.9","permalink":"/blog/2016/02/10/getting-started-with-sample-programs-for-apache-kafka-0-dot-9"}},"content":"You can find a new tutorial that explains how to deploy an Apache Kafka application to MapR Streams, the tutorial is available here:\\n\\n* [Getting Started with MapR Streams](https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams)\\n\\nMapR Streams is a new distributed messaging system for streaming event data at scale, and it\u2019s integrated into the MapR converged platform. \\nMapR Streams uses the Apache Kafka API, so if you\u2019re already familiar with Kafka, you\u2019ll find it particularly easy to get started with MapR Streams."},{"id":"/2016/02/10/getting-started-with-sample-programs-for-apache-kafka-0-dot-9","metadata":{"permalink":"/blog/2016/02/10/getting-started-with-sample-programs-for-apache-kafka-0-dot-9","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2016-02-10-getting-started-with-sample-programs-for-apache-kafka-0-dot-9.md","source":"@site/blog/2016-02-10-getting-started-with-sample-programs-for-apache-kafka-0-dot-9.md","title":"Getting Started with Sample Programs for Apache Kafka 0.9","description":"Ted Dunning and I have worked on a tutorial that explains how to write your first Kafka application. In this tutorial you will learn how to:","date":"2016-02-10T00:00:00.000Z","formattedDate":"February 10, 2016","tags":[],"readingTime":0.295,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Getting Started with Sample Programs for Apache Kafka 0.9","categories":"kafka streams howto"},"prevItem":{"title":"Getting Started with MapR Streams","permalink":"/blog/2016/03/10/getting-started-with-mapr-streams"},"nextItem":{"title":"Using Apache Drill REST API to build ASCII Dashboard with node","permalink":"/blog/2015/12/10/using-apache-drill-rest-api-to-build-ascii-dashboard-with-node"}},"content":"Ted Dunning and I have worked on a tutorial that explains how to write your first Kafka application. In this tutorial you will learn how to:\\n\\n* Install and start Kafka\\n* Create and Run a producer and a consumer\\n\\nYou can find the tutorial on the MapR blog:\\n\\n* [Getting Started with Sample Programs for Apache Kafka 0.9](https://goo.gl/cWmbmY)"},{"id":"/2015/12/10/using-apache-drill-rest-api-to-build-ascii-dashboard-with-node","metadata":{"permalink":"/blog/2015/12/10/using-apache-drill-rest-api-to-build-ascii-dashboard-with-node","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-12-10-using-apache-drill-rest-api-to-build-ascii-dashboard-with-node.md","source":"@site/blog/2015-12-10-using-apache-drill-rest-api-to-build-ascii-dashboard-with-node.md","title":"Using Apache Drill REST API to build ASCII Dashboard with node","description":"Apache Drill has a hidden gem: an easy to use REST interface. This API can be used to Query, Profile and Configure Drill engine.","date":"2015-12-10T00:00:00.000Z","formattedDate":"December 10, 2015","tags":[],"readingTime":4.01,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Using Apache Drill REST API to build ASCII Dashboard with node","categories":"Drill SQL node.js dataviz"},"prevItem":{"title":"Getting Started with Sample Programs for Apache Kafka 0.9","permalink":"/blog/2016/02/10/getting-started-with-sample-programs-for-apache-kafka-0-dot-9"},"nextItem":{"title":"Convert a CSV File to Apache Parquet with Drill","permalink":"/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill"}},"content":"import Gist from \'react-gist\';\\n\\n[Apache Drill](http://drill.apache.org) has a hidden gem: an easy to use REST interface. This API can be used to Query, Profile and Configure Drill engine.\\n\\nIn this blog post I will explain how to use Drill REST API to create ascii dashboards using [Blessed Contrib](https://www.npmjs.com/package/blessed-contrib).\\n\\nThe ASCII Dashboard looks like\\n\\n![Dashboard](/images/posts/drill_dashboard/dashboard_demo.gif)\\n\\n\x3c!-- truncate --\x3e\\n\\n#### Prerequisites\\n* Node.js\\n* Apache Drill 1.2\\n* For this post, you will use the SFO Passengers CSV file available [here](http://www.flysfo.com/media/facts-statistics/air-traffic-statistics). \\n  * Download this locally, unzip the files and put the CSV into a folder that will be access uzing the following path in Drill : ```dfs.data.`/airport/*.csv` ```\\n\\n\\n*Note: I am still using Apache 1.2 to allow this example to be executed in context of a MapR cluster.*\\n\\n## The Query and View\\n\\nIn Drill 1.2, CSV headers are not automatically parsed. (This is one of the new features of 1.3: look for `extractHeader` in the [documentation](https://drill.apache.org/docs/text-files-csv-tsv-psv/)). \\n\\nFor simplicity, remove the first line of the CSV.\\n\\nThe basic query will look like:\\n\\n```\\nSELECT\\nCAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,\\nCAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,\\ncolumns[1] as `AIRLINE`,\\ncolumns[2] as `IATA_CODE`,\\ncolumns[3] as `AIRLINE_2`,\\ncolumns[4] as `IATA_CODE_2`,\\ncolumns[5] as `GEO_SUMMARY`,\\ncolumns[6] as `GEO_REGION`,\\ncolumns[7] as `ACTIVITY_CODE`,\\ncolumns[8] as `PRICE_CODE`,\\ncolumns[9] as `TERMINAL`,\\ncolumns[10] as `BOARDING_AREA`,\\nCAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`\\nFROM dfs.data.`/airport/*.csv`\\nLIMIT 10\\n```\\n\\nLet\'s now create a view with these columns: *(do not put any limit !)*\\n\\n```\\nCREATE OR REPLACE VIEW dfs.tmp.`airport_data_view` AS\\nSELECT\\nCAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,\\nCAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,\\ncolumns[1] as `AIRLINE`,\\ncolumns[2] as `IATA_CODE`,\\ncolumns[3] as `AIRLINE_2`,\\ncolumns[4] as `IATA_CODE_2`,\\ncolumns[5] as `GEO_SUMMARY`,\\ncolumns[6] as `GEO_REGION`,\\ncolumns[7] as `ACTIVITY_CODE`,\\ncolumns[8] as `PRICE_CODE`,\\ncolumns[9] as `TERMINAL`,\\ncolumns[10] as `BOARDING_AREA`,\\nCAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`\\nFROM dfs.data.`/airport/*.csv`\\n```\\n\\nSo you can now use the view in your query:\\n\\n```\\nselect * from dfs.tmp.`airport_data_view` limit 5;\\n```\\n\\n\\n\\n## Use the REST API\\n\\nNow that you have the query you can use the REST API to retrieve the data as JSON document over HTTP. Open a terminal and run this curl command:\\n\\n\\n```\\ncurl  \\\\\\n  --header \\"Content-type: application/json\\" \\\\\\n  --request POST \\\\\\n  --data \'{\\n    \\"queryType\\" : \\"SQL\\",\\n    \\"query\\" : \\"select * from dfs.tmp.`airport_data_view` limit 5 \\" }\' \\\\\\n  http://localhost:8047/query.json\\n```\\n\\nThe returned JSON document looks like:\\n\\n```\\n{\\n  \\"columns\\" : [ \\"YEAR\\", \\"MONTH\\", ... , \\"PASSENGER_COUNT\\" ],\\n  \\"rows\\" : [ {\\n    \\"GEO_REGION\\" : \\"US\\",\\n    \\"IATA_CODE_2\\" : \\"TZ\\",\\n\\t\\t...\\n\\t\\t...\\n    \\"AIRLINE\\" : \\"ATA Airlines\\",\\n    \\"MONTH\\" : \\"7\\",\\n    \\"ACTIVITY_CODE\\" : \\"Deplaned\\"\\n  }, {\\n    \\"GEO_REGION\\" : \\"US\\",\\n    \\"IATA_CODE_2\\" : \\"TZ\\",\\n    \\"GEO_SUMMARY\\" : \\"Domestic\\",\\n    ...\\n  }\\n  ]\\n}\\n```\\n\\nAs you can see it is quite simple:\\n\\n* a first JSON attribute that list the columns\\n* the list of rows, as JSON documents in an array.\\n\\n \\n\\n## Create a Graph using Node.js & Blessed Contrib\\n\\nLet\'s create a node application. \\n\\nFirst you have to include:\\n\\n* `request` : to call the REST API\\n* `blessed` : to get a rich Terminal API\\n* `blessed-contrib` : for the dashboard\\n\\nand then create a `screen` and a `bar` chard from Contrib.\\n\\nSo the *header* of your Javascript file looks like:\\n\\n```javascript \\nvar blessed = require(\'blessed\')\\n  , contrib = require(\'blessed-contrib\')\\n  , request = require(\'request\')\\n  , screen = blessed.screen()\\n  , bar = contrib.bar(\\n       { label: \'Bar Chart\'\\n       , barWidth: 20\\n       , barSpacing: 20\\n       , maxHeight: 9\\n       , height: \\"100%\\"\\n       , width: \\"100%\\"})\\n```\\n\\nSo here we have defined a bar char, that will be populated with the columns and rows. For this we need a query, let\'s use the number of passengers per year, as follow:\\n\\n```\\nSELECT `YEAR`, SUM(`PASSENGER_COUNT`) FROM dfs.tmp.`airport_data_view` GROUP BY `YEAR`\\n```\\n\\nThe complete Bar Chat application looks like:\\n\\n<Gist id=\\"00c5d83b85f59d80ad95\\" file=\\"app001.js\\" />\\n\\n* The lines 15-17 contain the query object used by the Drill REST API\\n* The lines 26-38 contain the callback from the HTTP call, and the results values are store in the data object (lines 33-34), and then set in the bar chart (line 36)\\n\\n### Run the \\"Dashboard\\"\\n\\n```\\nnpm install request blessed blessed-contrib\\n\\nnode app001.js\\n\\n```\\n\\nThis application shows a simple bar chart, in your terminal. Let\'s now create a richer dashboard.\\n\\n## Complete Dashboard\\n\\nThe Bless-Contrib node package allows developer to create rich dashboards that aggregate multiple graphs and could be refresh automatically, as seen in the screencast at the top of this post.\\n\\nYou can find a simple dashboard in this [Github repository](https://github.com/tgrall/drill-node-dashboard.git), once you have cloned it, you just need to run: (be sure that your view is called \'dfs.tmp.`airport_data_view`\'\\n\\n```\\ngit clone https://github.com/tgrall/drill-node-dashboard.git\\n\\ncd drill-node-dashboard\\n\\nnpm install\\n\\nnode dashboard.js http://localhost:8047\\n\\n```\\n\\nYou can even change the CSV file, for example adding new months, and the line chart on the right will be refreshed automatically.\\n\\n*Note: this dashboard sample is very basic and just a quick example explaning how to use Drill REST API in a node.js application*"},{"id":"/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill","metadata":{"permalink":"/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-08-17-convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill.md","source":"@site/blog/2015-08-17-convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill.md","title":"Convert a CSV File to Apache Parquet with Drill","description":"A very common use case when working with Hadoop is to store and query simple files (CSV, TSV, ...); then to get better performance and efficient storage convert these files into more efficient format, for example Apache Parquet.","date":"2015-08-17T00:00:00.000Z","formattedDate":"August 17, 2015","tags":[],"readingTime":2.93,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Convert a CSV File to Apache Parquet with Drill","categories":"drill parquet hadoop bigdata"},"prevItem":{"title":"Using Apache Drill REST API to build ASCII Dashboard with node","permalink":"/blog/2015/12/10/using-apache-drill-rest-api-to-build-ascii-dashboard-with-node"},"nextItem":{"title":"Apache Drill : How to create a new function?","permalink":"/blog/2015/07/21/apache-drill-how-to-create-a-new-function"}},"content":"A very common use case when working with Hadoop is to store and query simple files (CSV, TSV, ...); then to get better performance and efficient storage convert these files into more efficient format, for example [Apache Parquet](https://parquet.apache.org/).\\n\\n[Apache Parquet](https://parquet.apache.org/) is a [columnar storage format](https://en.wikipedia.org/wiki/Column-oriented_DBMS) available to any project in the Hadoop ecosystem. Apache Parquet has the following characteristics:\\n\\n* Self-describing\\n* Columnar format\\n* Language-independent\\n\\nLet\'s take a concrete example, you can find many interesting Open Data sources that distribute data as CSV files- or equivalent format-. So you can store them into your distributed file system and use them in your applications/jobs/analytics queries. This is not the most efficient way especially when we know that these data won\'t move that often. So instead of simply storing the CSV let\'s copy this information into Parquet.\\n\\n\\n### How to convert CSV files into Parquet files?\\n\\nYou can use code to achieve this, as you can see in the [ConvertUtils](https://github.com/Parquet/parquet-compatibility/blob/master/parquet-compat/src/test/java/parquet/compat/test/ConvertUtils.java) sample/test class. You can use a simpler way with Apache Drill. Drill allows you save the result of a query as Parquet files.\\n\\nThe following steps will show you how to do convert a simple CSV into a Parquet file using Drill.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n#### Prerequisites\\n\\n* Apache Drill : Standalone [Apache Drill](https://drill.apache.org/) or using [Apache Drill Sandbox from MapR](https://www.mapr.com/products/mapr-sandbox-hadoop/download-sandbox-drill)\\n* Some CSV Files: for example [Passenger Dataset from SFO Air Traffic Statistics](http://www.flysfo.com/media/facts-statistics/air-traffic-statistics)\\n\\n\\n#### Querying the CSV file\\n\\nLet\'s execute a basic query:\\n\\n```sql\\nSELECT *\\nFROM dfs.`/opendata/Passenger/SFO_Passenger_Data/MonthlyPassengerData_200507_to_201503.csv`\\nLIMIT 5;\\n\\n[\\"200507\\",\\"ATA Airlines\\",\\"TZ\\",\\"ATA Airlines\\",\\"TZ\\",\\"Domestic\\",\\"US\\",\\"Deplaned\\",\\"Low Fare\\",\\"Terminal 1\\",\\"B\\",\\"27271\\\\r\\"]\\n...\\n...\\n```\\nAs you can see, by default Drill processes each line as an array of columns, all values being simple String. So if you need to do some operations with these values (projection or where clause) you must use the column index, and cast the value to the proper type. You can see a simple example below:\\n\\n```\\nSELECT\\ncolumns[0] as `DATE`,\\ncolumns[1] as `AIRLINE`,\\nCAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`\\nFROM dfs.`/opendata/Passenger/SFO_Passenger_Data/*.csv`\\nWHERE CAST(columns[11] AS DOUBLE) < 5\\n;\\n\\n+---------+-----------------------------------+------------------+\\n|  DATE   |              AIRLINE              | PASSENGER_COUNT  |\\n+---------+-----------------------------------+------------------+\\n| 200610  | United Airlines - Pre 07/01/2013  | 2.0              |\\n...\\n...\\n```\\n\\nWe are now ready to create our Parquet files using the \\"Create Table As Select\\" (aka [CTAS](http://drill.apache.org/docs/create-table-as-ctas-command/))\\n\\n``` sql\\nalter session set `store.format`=\'parquet\';\\n\\n\\nCREATE TABLE dfs.tmp.`/stats/airport_data/` AS\\nSELECT\\nCAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,\\nCAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,\\ncolumns[1] as `AIRLINE`,\\ncolumns[2] as `IATA_CODE`,\\ncolumns[3] as `AIRLINE_2`,\\ncolumns[4] as `IATA_CODE_2`,\\ncolumns[5] as `GEO_SUMMARY`,\\ncolumns[6] as `GEO_REGION`,\\ncolumns[7] as `ACTIVITY_CODE`,\\ncolumns[8] as `PRICE_CODE`,\\ncolumns[9] as `TERMINAL`,\\ncolumns[10] as `BOARDING_AREA`,\\nCAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`\\nFROM dfs.`/opendata/Passenger/SFO_Passenger_Data/*.csv`\\n\\n```\\n\\nThat\'s it! You have now a Parquet file, a single file in our case since our dataset is really small. Apache Drill will create multiples files for the tables depending of the size and configuration your environment.\\n\\n\\nI invite you to read this Chapter in the Apache Drill documentation to learn more about [Drill and Parquet](https://drill.apache.org/docs/parquet-format/).\\n\\n\\n### Query Parquet Files\\n\\nNow that you have created your Parquet files you can use them in any of your Hadoop processes, but you can also use them in Drill, as follow:\\n\\n```\\n\\nSELECT *\\nFROM dfs.tmp.`/stats/airport_data/*`\\n\\n```\\n\\n## Conclusion\\n\\nIn this article you have learned how to convert a CSV file using an Apache Drill query.\\n\\nYou can do that with any source supported by Drill, for example from JSON to Parquet, or even a complex join query between multiple data sources. You can also chose a different output format for example JSON, or a CSV."},{"id":"/2015/07/21/apache-drill-how-to-create-a-new-function","metadata":{"permalink":"/blog/2015/07/21/apache-drill-how-to-create-a-new-function","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-07-21-apache-drill-how-to-create-a-new-function.md","source":"@site/blog/2015-07-21-apache-drill-how-to-create-a-new-function.md","title":"Apache Drill : How to create a new function?","description":"Apache Drill allows users to explore any type of data using ANSI SQL. This is great, but Drill goes even further than that and allows you to create custom functions to extend the query engine. These custom functions have all the performance of any of the Drill primitive operations, but allowing that performance makes writing these functions a little trickier than you might expect.","date":"2015-07-21T00:00:00.000Z","formattedDate":"July 21, 2015","tags":[],"readingTime":7.425,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Apache Drill : How to create a new function?","categories":"Drill, UDF, SQL, Java"},"prevItem":{"title":"Convert a CSV File to Apache Parquet with Drill","permalink":"/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill"},"nextItem":{"title":"MongoDB : Playing with Arrays","permalink":"/blog/2015/04/21/mongodb-playing-with-arrays"}},"content":"[Apache Drill](https://drill.apache.org/) allows users to explore _any type of_ data using ANSI SQL. This is great, but Drill goes even further than that and allows you to create custom functions to extend the query engine. These custom functions have all the performance of any of the Drill primitive operations, but allowing that performance makes writing these functions a little trickier than you might expect.\\n\\nIn this article, I\'ll explain step by step how to create and deploy a new function using a very basic example. Note that you can find lot of information about [Drill Custom Functions in the documentation](https://drill.apache.org/docs/develop-custom-functions-introduction/).\\n\\nLet\'s create a new function that allows you to mask some characters in a string, and let\'s make it very simple. The new function will allow user to hide _x_ number of characters from the start and replace then by any characters of their choice. This will look like:\\n\\n```\\nMASK( \'PASSWORD\' , \'#\' , 4 ) => ####WORD\\n```\\n\\nYou can find the full project in the following [Github Repository](https://github.com/tgrall/drill-simple-mask-function).\\n\\nAs mentioned before, we could imagine many advanced features to this, but my goal is to focus on the steps to write a custom function, not\\nso much on what the function does.\\n\\n\x3c!--more--\x3e\\n\\n## Prerequisites\\n\\nFor this you will need:\\n\\n* Java Developer Kit 7 or later\\n* Apache Drill 1.1 or later\\n* Maven 3.0 or later\\n\\n## Dependencies\\n\\nThe following Drill dependency should be added to your maven project\\n\\n``` xml\\n<dependency>\\n      <groupId>org.apache.drill.exec</groupId>\\n      <artifactId>drill-java-exec</artifactId>\\n      <version>1.1.0</version>\\n</dependency>\\n```\\n\\n## Source\\n\\nThe `Mask` function is an implementation of the [`DrillSimpleFunc`](https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/DrillSimpleFunc.java).\\n\\nDevelopers can create 2 types of custom functions:\\n\\n* Simple Functions: these functions have a single row as input and produce a single value as output\\n* Aggregation Functions: that will accept multiple rows as input and produce one value as output\\n\\nSimple functions are often referred to as UDF\'s which stands for user defined function.  Aggregation functions are referred to as UDAF which\\nstands for user defined aggregation function.\\n\\nIn this example, we just need to transform the value of a column on each row, so a simple function is enough.\\n\\n#### Create the function\\n\\nThe first step is to implement the [`DrillSimpleFunc`](https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/DrillSimpleFunc.java) interface.\\n\\n``` java\\npackage org.apache.drill.contrib.function;\\n\\nimport org.apache.drill.exec.expr.DrillSimpleFunc;\\nimport org.apache.drill.exec.expr.annotations.FunctionTemplate;\\n\\n@FunctionTemplate(\\n        name=\\"mask\\",\\n        scope= FunctionTemplate.FunctionScope.SIMPLE,\\n        nulls = FunctionTemplate.NullHandling.NULL_IF_NULL\\n)\\npublic class SimpleMaskFunc implements DrillSimpleFunc{\\n\\n    public void setup() {\\n\\n    }\\n\\n    public void eval() {\\n\\n    }\\n}\\n\\n```\\n\\n\\nThe behavior of the function is driven by annotations (line 6-10)\\n  * _Name_ of the function\\n  * _Scope_ of the function, in our case Simple\\n  * What to do when the value is NULL, in this case Reverse will just returns NULL\\n\\nNow we need to implement the logic of the function using `setup()` and `eval()` methods.\\n\\n* `setup` is self-explanatory, and in our case we do not need to setup anything.\\n* `eval` that is the core of the function. As you can see this method does not have any parameter, and return void. So how does it work?\\n\\nIn fact the function will be generated dynamically (see [DrillSimpleFuncHolder](https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/DrillSimpleFuncHolder.java#L42)), and the input parameters and output holders are defined using holders by annotations. Let\'s look into this.\\n\\n``` java\\nimport io.netty.buffer.DrillBuf;\\nimport org.apache.drill.exec.expr.DrillSimpleFunc;\\nimport org.apache.drill.exec.expr.annotations.FunctionTemplate;\\nimport org.apache.drill.exec.expr.annotations.Output;\\nimport org.apache.drill.exec.expr.annotations.Param;\\nimport org.apache.drill.exec.expr.holders.IntHolder;\\nimport org.apache.drill.exec.expr.holders.NullableVarCharHolder;\\nimport org.apache.drill.exec.expr.holders.VarCharHolder;\\n\\nimport javax.inject.Inject;\\n\\n\\n@FunctionTemplate(\\n        name = \\"mask\\",\\n        scope = FunctionTemplate.FunctionScope.SIMPLE,\\n        nulls = FunctionTemplate.NullHandling.NULL_IF_NULL\\n)\\npublic class SimpleMaskFunc implements DrillSimpleFunc {\\n\\n    @Param\\n    NullableVarCharHolder input;\\n\\n    @Param(constant = true)\\n    VarCharHolder mask;\\n\\n    @Param(constant = true)\\n    IntHolder toReplace;\\n\\n    @Output\\n    VarCharHolder out;\\n\\n    @Inject\\n    DrillBuf buffer;\\n\\n    public void setup() {\\n    }\\n\\n    public void eval() {\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\nWe need to define the parameters of the function. In this case we have 3 parameters, each defined using the `@Param` annotation.  In addition, we also have to define the returned value using the `@Output` annotation.\\n\\nThe parameters of our mask function are:\\n\\n* A nullable string\\n* The mask char or string\\n* The number of characters to replace starting from the first\\n\\nThe function returns :\\n\\n* A string\\n\\nFor each of these parameters you have to use an holder class. For the `String`, this is managed by a `VarCharHolder` or `NullableVarCharHolder` -lines 21, 24,30- that provides a buffer to manage larger objects in a efficient way. Since we are manipulating a `VarChar` you also have to inject another buffer that will be used for the output -line 33-. Note that Drill doesn\'t actually use the Java heap for data being processed in a query but instead keeps this data off the heap and manages the life-cycle for us without using the Java\\ngarbage collector.\\n\\nWe are almost done since we have the proper class, the input/output object, we just need to implement the `eval()` method itself, and use these objects.\\n\\n``` java\\npublic void eval() {\\n\\n    // get the value and replace with\\n    String maskValue = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.getStringFromVarCharHolder(mask);\\n    String stringValue = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(input.start, input.end, input.buffer);\\n\\n    int numberOfCharToReplace = Math.min(toReplace.value, stringValue.length());\\n\\n    // build the mask substring\\n    String maskSubString = com.google.common.base.Strings.repeat(maskValue, numberOfCharToReplace);\\n    String outputValue = (new StringBuilder(maskSubString)).append(stringValue.substring(numberOfCharToReplace)).toString();\\n\\n    // put the output value in the out buffer\\n    out.buffer = buffer;\\n    out.start = 0;\\n    out.end = outputValue.getBytes().length;\\n    buffer.setBytes(0, outputValue.getBytes());\\n}\\n```\\n\\nThe code is quite simple:\\n\\n* Get the mask itself - line 4\\n* Get the value - line 5\\n* Get the number of character to replace - line 7\\n* Generate a new string with masked values - lines 10/11\\n* Create and populate the output buffer - lines 14 to 17\\n\\nThis code does, however, look a bit strange to somebody used to reading Java code. This strangeness arises because the final code that is executed in a query will actually be generated on the fly. This allows Drill to leverage Java\'s just-in-time (JIT) compiler for maximum speed. To make this work, you have to respect some basic rules:\\n\\n* **Do not use imports, but instead use the fully qualified class name**, this is what is done on line 10 with the `Strings` class. (coming from the Google Guava API packaged in Apache Drill)\\n* The `ValueHolders` classes, in our case `VarCharHolder` and `IntHolder` should be manipulated like structs, so you must call helper methods, for example `getStringFromVarCharHolder` and `toStringFromUTF8`. Calling methods like `toString` will result in very bad problems.\\n\\nStarting in Apache Drill 1.3.x, it is mandatory to specify the package name of your function in the `./resources/drill-module.conf` file as follow:\\n\\n```\\ndrill {\\n  classpath.scanning {\\n    packages : ${?drill.classpath.scanning.packages} [\\n      org.apache.drill.contrib.function\\n    ]\\n  }\\n}\\n```\\n\\nWe are now ready to deploy and test this new function.\\n\\n### Package\\n\\nOnce again since, Drill will generate source, _**you must prepare your package in a way that classes and sources of the function are present in the classpath**_. This is different from the way that Java code is normally packaged but is necessary for Drill to be able to do the necessary code generation. Drill uses the compiled code to access the annotations and uses the source code to do code generation.\\n\\nAn easy way to do that is to use maven to build your project, and, in particular, use the [maven-source-plugin](https://maven.apache.org/plugins/maven-source-plugin/) like this in your `pom.xml` file:\\n\\n``` xml\\n<plugin>\\n    <groupId>org.apache.maven.plugins</groupId>\\n    <artifactId>maven-source-plugin</artifactId>\\n    <version>2.4</version>\\n    <executions>\\n        <execution>\\n            <id>attach-sources</id>\\n            <phase>package</phase>\\n            <goals>\\n                <goal>jar-no-fork</goal>\\n            </goals>\\n        </execution>\\n    </executions>\\n</plugin>\\n```\\n\\nNow, when you build using `mvn package`, Maven will generate 2 jars:\\n\\n* The default jar with the classes and resources (_drill-simple-mask-1.0.jar_)\\n* A second jar with the sources (_drill-simple-mask-1.0-sources.jar_)\\n\\nFinally you must add a `drill-module.conf` file in the resources folder of your project, to tell Drill that your jar contains a custom function. If you have no specific configuration to set for your function you can keep this file empty.\\n\\nWe are all set, you can now package and deploy the new function, just package and copy the Jars into the Drill 3rd party folder; $DRILL_HOME/jars/3rdparty , where $DRILL_HOME being your Drill installation folder.\\n\\n```\\nmvn clean package\\n\\ncp target/*.jar  $DRILL_HOME/jars/3rdparty\\n\\n```\\n\\nRestart drill.\\n\\n\\n### Run !\\n\\nYou should now be able to use your function in your queries:\\n\\n```\\nSELECT MASK(first_name, \'*\' , 3) FIRST , MASK(last_name, \'#\', 7) LAST  FROM cp.`employee.json` LIMIT 5;\\n+----------+------------+\\n|  FIRST   |    LAST    |\\n+----------+------------+\\n| ***ri    | ######     |\\n| ***rick  | #######    |\\n| ***hael  | ######     |\\n| ***a     | #######ez  |\\n| ***erta  | #######    |\\n+----------+------------+\\n```\\n\\n## Conclusion\\n\\nIn this simple project you have learned how to write, deploy and use a custom Apache Drill Function. You can now extend this to create your own function.\\n\\nOne important thing to remember when extending Apache Drill (using a custom function, storage plugin or format), is that Drill runtime is generating dynamically lot of code. This means you may have to use a very specific pattern when writing and deploying your extensions. With our basic function this meant we had to:\\n\\n* deploy **classes AND sources**\\n* use **fully Qualified Class Names**\\n* use value holder classes and helper methods to manipulate parameters\\n*"},{"id":"/2015/04/21/mongodb-playing-with-arrays","metadata":{"permalink":"/blog/2015/04/21/mongodb-playing-with-arrays","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-04-21-mongodb-playing-with-arrays.md","source":"@site/blog/2015-04-21-mongodb-playing-with-arrays.md","title":"MongoDB : Playing with Arrays","description":"As you know,  you have many differences between relational and document databases. The biggest, for the developer, is probably the data model: Row versus Document. This is particularly true when we talk about \\"relations\\" versus \\"embedded documents (or values)\\". Let\'s look at some examples, then see what are the various operations provided by MongoDB to help you to deal with this.","date":"2015-04-21T00:00:00.000Z","formattedDate":"April 21, 2015","tags":[],"readingTime":6.2,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"MongoDB : Playing with Arrays","categories":"mongodb json arrays lists"},"prevItem":{"title":"Apache Drill : How to create a new function?","permalink":"/blog/2015/07/21/apache-drill-how-to-create-a-new-function"},"nextItem":{"title":"Introduction to MongoDB Security","permalink":"/blog/2015/02/04/introduction-to-mongodb-security"}},"content":"As you know,  you have many differences between relational and document databases. The biggest, for the developer, is probably the data model: Row versus Document. This is particularly true when we talk about \\"relations\\" versus \\"embedded documents *(or values)*\\". Let\'s look at some examples, then see what are the various operations provided by MongoDB to help you to deal with this.\\n\\n\x3c!-- truncate --\x3e\\n\\nI won\'t use this post to go in all the details about \\"document design\\", but I just want to focus on the operations you can to with these arrays/list (so useful operations once you have chosen to embed documents).\\n\\nLet\'s use a very simple example based on e-commerce platform, with two document types : ***user*** and ***orders***.\\n\\nThe first thing you have, is a simple list of values into a JSON array. Let\'s look at a user profile where user have a list of interests  (field `interested_by`) :\\n\\n```json\\n{\\n  \\"_id\\" : 654321,\\n  \\"first_name\\" : \\"John\\",\\n  \\"last_name\\" : \\"Doe\\",\\n  \\"interested_by\\" : [ \\"electronics\\", \\"sports\\", \\"music\\" ],\\n  \\"address\\" : {\\n    \\"name\\" : \\"John Doe\\",\\n    \\"company\\" : \\"Resultri\\",\\n    \\"street\\" : \\"1015 Mapple Street\\",\\n    \\"city\\" : \\"San Francisco\\",\\n    \\"state\\" : \\"CA\\",\\n    \\"zip_code\\" : 94105\\n  }\\n}\\n```\\n\\nAnother thing you do with array, is to represent \\"one-to-many\\" relations. These relations in your RDBMS are based on multiple tables and foreign keys.\\nIn document databases, like MongoDB, these relations are, most of the time, represented using an *array of documents*, something like (look at the `items` field):\\n\\n```json\\n{\\n  \\"_id\\" : 45218468309,\\n  \\"date\\" : ISODate(\\"2015-01-28T09:40:50.615Z\\"),\\n  \\"customer\\" : {\\n    \\"id\\" : 654321,\\n    \\"name\\" : \\"John Doe\\"\\n  },\\n  \\"ship_to\\" : {\\n    \\"name\\" : \\"John Doe\\",\\n    \\"company\\" : \\"Resultri\\",\\n    \\"street\\" : \\"1015 Mapple Street\\",\\n    \\"city\\" : \\"San Francisco\\",\\n    \\"state\\" : \\"CA\\",\\n    \\"zip_code\\" : 94105\\n  },\\n  \\"items\\" : [\\n    {\\n      \\"sku\\" : \\"WA34R\\",\\n      \\"description\\" : \\"Wireless Qwerty Keyboard\\",\\n      \\"quantity\\" : 1,\\n      \\"unit_price\\" : 41.5,\\n      \\"price\\" : 41.5,\\n      \\"vat\\" : 20\\n    },\\n    {\\n      \\"sku\\" : \\"MW003\\",\\n      \\"description\\" : \\"MiWatch\\",\\n      \\"quantity\\" : 2,\\n      \\"unit_price\\" : 245,\\n      \\"price\\" : 490,\\n      \\"vat\\" : 20\\n    }\\n  ]\\n}\\n\\n```\\n\\nDocuments above are not necessary complete, I just want to focus on the various operations you can do on these arrays.\\n\\n*Note: you can add these documents into your MongoDB instance, I will use the collections `customers` and `orders`.*\\n\\n\\n### Adding new interest to the user\\n\\nTo achieve this you have 2 operators that you can use in your update: [`$push`](http://docs.mongodb.org/manual/reference/operator/update/push/) and [`$addToSet`](http://docs.mongodb.org/manual/reference/operator/update/addToSet/). Since these one a very simple I won\'t go into too much details.\\n\\nThe `$push` will add the value at the end of the list, if the value already exits it will be added (many copies), this is why it makes sense to use the `$addToSet` operator, that only add the value if the value does not already exist in the array.\\n\\n\\n```javascript\\ndb.customers.update(\\n  { \\"_id\\" : 654321  },\\n  { \\"$addToSet\\" : { \\"interested_by\\" :  \\"sports\\"}  }\\n);\\n```\\nThis update command above **will not change** the document since the \\"sports\\" value is already in the list.\\n\\n```javascript\\ndb.customers.update(\\n  { \\"_id\\" : 654321  },\\n  { \\"$addToSet\\" : { \\"interested_by\\" :  \\"books\\"}  }\\n);\\n```\\nThis **will add** the \\"books\\" value at the end of the list.\\n\\nIf the attribute `interested_by` does not exist in the document, it will be added with one single entry (here the `$push` is working the same way ).\\n\\nIf the attribute is not an array, the database will not do anything and return the error [#16837](https://github.com/mongodb/mongo/blob/master/docs/errors.md\\n) *\\"The field \'first_name\' must be an array but is of type String in document\\"*.\\n\\nHere we use *interest*, but you can imagine doing the same thing for tagging, or any other business use case with a list of values.\\n\\n### Adding a new item into an order\\n\\nThe previous case, is very simple since it is a scalar value. Now I need to add a new order line, it is not harder than before:\\n\\n```js\\ndb.orders.update(\\n  { \\"_id\\" : 45218468309   },\\n  {\\n    \\"$push\\" : {\\n      \\"items\\" : {\\n        \\"sku\\" : \\"MO001\\",\\n        \\"description\\" : \\"Bluetooth mouse\\",\\n        \\"quantity\\" : 1,\\n        \\"unit_price\\" : 20.00,\\n        \\"price\\" : 20.00,\\n        \\"vat\\" : 20.00\\n      }\\n    }\\n  }\\n);\\n```\\nSo you can see now that the value is added at the end of the list.\\n\\n### Updating an item in the order\\n\\nLet\'s look at another requirement. Now I need to update for example the quantity of one of the line. In your relational application it is *easy* in the sense that you have one single record to update(in reality it is a different story since application are using complex data layer).\\n\\nYou can do the same, meaning you can only update the *items* directly in the array -- (no need to replace the full document or list like I see too often).\\n\\nFor this, you  just need to use the `update` and `$set` and specify the positional operation `$`.\\n\\nThe `$` operator is a placeholder for the first entry in the array that match the filter (query document) sent to the update/findAndModify command.\\n\\nIn our example, to update a specific line in the order\\n\\nThe proper way is simply to use the an update and `$set`,\\n but you have to select the exact entry in the array in your filter. For example in our case we want to update the number of mouses and the price, this will look like:\\n\\n```js\\ndb.orders.update(\\n  {\\n    \\"_id\\" : 45218468309,\\n    \\"items.sku\\" : \\"MO001\\"\\n  },\\n  {\\n    \\"$set\\" :\\n    {\\n        \\"items.$\\" : {\\n          \\"sku\\" : \\"MO001\\",\\n          \\"description\\" : \\"Bluetooth mouse\\",\\n          \\"quantity\\" : 2,\\n          \\"unit_price\\" : 20.00,\\n          \\"price\\" : 40.00,\\n          \\"vat\\" : 20.00\\n        }\\n    }\\n  }\\n);\\n```\\n\\nAs you can see, the `$` operator is telling MongoDB to update one specific entry in the array.\\n\\n## Remove an item from the Array\\n\\nYou have learned so far that you can easily query and add values into an array; using the same appraoch you can also remove entry in an array. This is done using 3 operators : `$pop`, `$pull` and `$pullAll`\\n\\n* The `$pop` operator removes one element from the end of the array\\n* The `$pull` operator removes *all* elements in the array that match a specified value.\\n* The `$pullAll` operator removes *all* elements in the array that match any of the specified values.\\n\\n#### Remove some interests from a customer\\n\\nFor example, let\'s remove the \\"electronics\\" interest from the customer id 654321.\\n\\n```js\\ndb.customers.update(\\n  { \\"_id\\" : 654321  },\\n  { \\"$pull\\" : { \\"interested_by\\" :  \\"electronics\\"}  }\\n);\\n\\n```\\n\\nIf you want to remove sports and music interest from the customer you can use the `$pullAll` operator as follow:\\n\\n```js\\ndb.customers.update(\\n  { \\"_id\\" : 654321  },\\n  { \\"$pullAll\\" : { \\"interested_by\\" :  [\\"sports\\",\\"music\\"]}  }\\n);\\n\\n```\\n\\nHere we use *interest*, but you can imagine doing the same thing for tagging, or any other business use case with a list of values.\\n\\n#### Remove item into an order\\n\\nUsing the same operator you can also remove a line order (item) from an order document, for example let\'s remove the line with the item MO001 (Bluetooth mouse). For this we can use the `$pull` operator with the proper sku.\\n\\n```js\\ndb.orders.update(\\n  {\\n    \\"_id\\" : 45218468309,\\n  },\\n  {\\n    \\"$pull\\" : { \\"items\\" : { \\"sku\\" : \\"MO001\\" } }\\n  }\\n);\\n\\n```\\n\\n### Conclusion\\n\\nIn this article you have learn how to create/edit arrays in JSON documents.\\n\\nIn addition to all the update operators, MongoDB provides many operators for querying arrays such as  [$size](http://docs.mongodb.org/manual/reference/operator/query/size/\\n) or [`$elemMatch`](http://docs.mongodb.org/manual/reference/operator/query/elemMatch/)."},{"id":"/2015/02/04/introduction-to-mongodb-security","metadata":{"permalink":"/blog/2015/02/04/introduction-to-mongodb-security","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-02-04-introduction-to-mongodb-security.md","source":"@site/blog/2015-02-04-introduction-to-mongodb-security.md","title":"Introduction to MongoDB Security","description":"Last week at the Paris MUG, I had a quick chat about security and MongoDB, and I have decided to create this post that explains how to configure out of the box security available in MongoDB.","date":"2015-02-04T00:00:00.000Z","formattedDate":"February 4, 2015","tags":[],"readingTime":5.825,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Introduction to MongoDB Security","categories":"mongodb nosql security"},"prevItem":{"title":"MongoDB : Playing with Arrays","permalink":"/blog/2015/04/21/mongodb-playing-with-arrays"},"nextItem":{"title":"Moving my beers from Couchbase to MongoDB","permalink":"/blog/2015/02/01/moving-my-beers-from-couchbase-to-mongodb"}},"content":"Last week at the Paris MUG, I had a quick chat about security and MongoDB, and I have decided to create this post that explains how to configure out of the box security available in MongoDB.\\n\\nYou can find all information about MongoDB Security in following documentation chapter:\\n\\n* [http://docs.mongodb.org/manual/security/](http://docs.mongodb.org/manual/security/)\\n\\n\\n![](/images/posts/intro-mongodb-security/password.jpg )\\n\\n\\n\\nIn this post, *I won\'t go* into the detail about how to deploy your database in a secured environment (DMZ/Network/IP/Location/...)\\n\\nI will focus on **Authentication** and **Authorization**, and provide you the steps to secure the access to your database and data.\\n\\nI have to mention that by default, when you install and start MongoDB, security is not enabled. Just to make it easier to work with.\\n\\nThe first part of the security is the **Authentication**, you have multiple choices documented [here](http://docs.mongodb.org/manual/core/authentication/). Let\'s focus on \\"MONGODB-CR\\" mechanism.\\n\\nThe second part is **Authorization** to select what a user can do or not once he is connected to the database. The documentation about authorization is available [here](http://docs.mongodb.org/manual/core/authorization/).\\n\\nLet\'s now document how-to:\\n\\n1. Create an Administrator User\\n2. Create Application Users\\n\\nFor each type of users I will show how to grant specific permissions.\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Start MongoDB\\n\\nAs I said before, by default security is not enabled when you start MongoDB; so the first think to do is to enable it using the `--auth` parameter.\\n\\n```\\n> mkdir /data/db\\n\\n> mongod --auth\\n\\n....\\n....\\n2015-02-04T06:56:37.875+0100 [conn1] note: no users configured in admin.system.users, allowing localhost access\\n...\\n\\n```\\nMongoDB is starting, and until you have created a user you can connect from localhost to create some users (especially the administrator one). This is what is called the *localhost exception*.\\n\\nNote: I am here documenting security in simple configuration, I invite you to look to the documentation when deploying a [Sharded cluster](http://docs.mongodb.org/v2.2/administration/sharded-clusters/#sharded-cluster-security-considerations).\\n\\nNow that we have started MongoDB, we can create users.\\n\\n\\n##2. Create Admin User\\n\\nThe first thing is to create an admin user, that can also create users, So we have to:\\n\\n1. go to the mongo shell\\n2. connect to the `admin\' database\\n3. create a user and assign him the role [**userAdminAnyDatabase**](http://docs.mongodb.org/manual/reference/built-in-roles/#userAdminAnyDatabase)\\n\\n```\\nuse admin\\n\\nvar user = {\\n\\t\\"user\\" : \\"admin\\",\\n\\t\\"pwd\\" : \\"manager\\",\\n\\troles : [\\n\\t\\t{\\n\\t\\t\\t\\"role\\" : \\"userAdminAnyDatabase\\",\\n\\t\\t\\t\\"db\\" : \\"admin\\"\\n\\t\\t}\\n\\t]\\n}\\n\\ndb.createUser(user);\\n\\nexit\\n```\\n\\nNow that you have created a user, in a MongoDB running with `--auth`, anonymous connections won\'t be able to do do anything with the database.\\n\\nYou can test for example to execute `show dbs` or `db.coll.insert({\'x\':0})` commands, you\'ll see authorization errors.\\n\\n\\n### Connect with the Admnistrator user\\n\\nNow that we have an admin user you can connect to the database with this user:\\n\\n```\\n\\n> mongo admin -u admin -p\\n\\n\\n```\\n\\nOur admin user, has the role **userAdminAnyDatabase**. With this role you can manage user; but this role cannot read/write data from application datatabases/collections.\\n\\nSo we need now to create a new user for our \\"eCommerce\\" application.\\n\\n## 3. Create Application User\\n\\nNow we will create a new user *website* that is responsible of the ecommerce database.\\n\\n```\\n> mongo admin -u admin -p\\n\\nuse ecommerce\\n\\nvar user = {\\n\\t\\"user\\" : \\"website\\",\\n\\t\\"pwd\\" : \\"abc123\\",\\n\\troles : [\\n\\t\\t{\\n\\t\\t\\t\\"role\\" : \\"readWrite\\",\\n\\t\\t\\t\\"db\\" : \\"ecommerce\\"\\n\\t\\t}\\n\\t]\\n}\\n\\ndb.createUser(user);\\n\\nexit\\n\\n```\\n\\nThis user will be able to read/write on the *ecommerce* database\\n\\n### Connect with the application user\\n\\nUsing the mongo shell you can now connect and create/query data\\n\\n```\\n> mongo ecommerce -u website -p\\n\\ndb.products.insert({ \\"title\\" : \\"MongoDB in Action\\"  });\\n\\ndb.products.findOne();\\n\\ndb.products.update({}, {\\"$set\\" : { \\"type\\" : \\"book\\" } })\\n\\n```\\n\\nAs you can see this user has the perfect profile for your application.\\n\\nNote, that if you try to query or modify another database with this user you will receive authorization exceptions.\\n\\n\\n## Create a reporting user (Read Only)\\n\\nYou may need in your application, user that can only read data, let\'s say in all databases. For this you just need to assign the role **readAnyDatabase**.\\n\\n```\\n\\n> mongo admin -u admin -p\\n\\nvar user = {\\n\\t\\"user\\" : \\"reporting\\",\\n\\t\\"pwd\\" : \\"abc123\\",\\n\\troles : [\\n\\t\\t{\\n\\t\\t\\t\\"role\\" : \\"readAnyDatabase\\",\\n\\t\\t\\t\\"db\\" : \\"admin\\"\\n\\t\\t}\\n\\t]\\n}\\n\\ndb.createUser(user);\\n\\nexit\\n```\\n\\nThis user will be able to query all the databases and collections, including `show dbs` command.\\n\\nLet\'s connect with the reporting user:\\n\\n```\\n> mongo admin -u reporting -p\\n\\nshow dbs\\n\\nuse ecommerce\\n\\ndb.products.find();\\n\\n\\n```\\n\\nIf you try to inser/update/delete document you will receive an exception.\\n\\n## Add new role to a user\\n\\nLet\'s now see how to add a new role to a user. For example I want to let the admin the power of read and write any database. For this I just need to add the role **readWriteAnyDatabase** to the admin user.\\n\\n```\\n> mongo admin -u admin -p\\n\\ndb.grantRolesToUser(\\n\\t\\"admin\\",\\n\\t[{ \\"role\\" : \\"readWriteAnyDatabase\\", \\"db\\" : \\"admin\\" }]\\n)\\n\\ndb.getUser(\\"admin\\");\\n\\n```\\n\\nUsing the `db.grantRolesToUser` command I have added the role to the admin user, and using the `db.getUser` I can look at the user profile.\\n\\nNow, admin user should be able to create new databases, collections and documents, let\'s try:\\n\\n```\\nuse hr\\n\\ndb.employees.insert({ \\"name\\":\\"John Doe\\", \\"hire_date\\" : new Date() });\\n\\ndb.organization.insert({ \\"name\\" : \\"Development\\" });\\n\\ndb.employees.findOne();\\n\\n```\\n\\n## Create and use custom roles\\n\\nAnother feature that is used a lot around security is related to the roles. In some case you want to provide multiple roles to a user, for example:\\n\\n* all permission on database *ecommerce*\\n* read the collection *employees* in database *hr*\\n\\nFor this you can create a role that provide all the permissions and assign it to users. Let\'s do that using admin user.\\n\\n```\\nuse admin\\n\\nvar role = {\\n\\t\\"role\\"  : \\"webSiteManagerRole\\",\\n\\tprivileges : [\\n\\t\\t{\\n\\t\\t\\t\\"resource\\": {\\"db\\" : \\"hr\\", \\"collection\\" : \\"employees\\"},\\n\\t\\t\\t\\"actions\\": [\\"find\\"]\\n\\t\\t}\\n\\t],\\n\\t\\"roles\\" : [\\n\\t\\t{\\n\\t\\t\\t\\"role\\" : \\"readWrite\\",\\n\\t\\t\\t\\"db\\" : \\"ecommerce\\"\\n\\t\\t}\\n\\t]\\n}\\n\\ndb.createRole( role );\\n\\nvar user = {\\n\\t\\"user\\" : \\"master\\",\\n\\t\\"pwd\\" : \\"abc123\\",\\n\\troles : [\\n\\t\\t{\\n\\t\\t\\t\\"role\\" : \\"webSiteManagerRole\\",\\n\\t\\t\\t\\"db\\" : \\"admin\\"\\n\\t\\t}\\n\\t]\\n}\\n\\ndb.createUser(user);\\n\\nexit\\n\\n```\\n\\nIf you connect now with the user \\"master\\", you will see that, the user:\\n\\n* can do anything you want in the ecommerce database\\n* can read the \\"hr.employees\\" collection, on only this\\n* cannot do anything else.\\n\\n\\n## Roles and Privileges\\n\\nAs you have seen in the previous section, you can create roles, and assign privileges to these roles. This is very powerful and you can really control each action on the database.\\n\\nI am inviting you to look in detail to the built-in roles and privileges, this will help you a lot to select the proper ones for your application:\\n\\n* [Built-in Roles](http://docs.mongodb.org/manual/reference/built-in-roles/)\\n* [Privileges](http://docs.mongodb.org/manual/reference/privilege-actions/)\\n\\n\\n## Conclusion\\n\\nIn this blog post I quickly explained, how to:\\n\\n* Use MongoDB Authentication\\n* Create Users\\n* Assign Roles and Privileges for Users.\\n\\nIt is interesting to know that everything that I have showed you in the shell could be done from a user interface in [MMS](http://mms.mongodb.com)"},{"id":"/2015/02/01/moving-my-beers-from-couchbase-to-mongodb","metadata":{"permalink":"/blog/2015/02/01/moving-my-beers-from-couchbase-to-mongodb","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-02-01-moving-my-beers-from-couchbase-to-mongodb.md","source":"@site/blog/2015-02-01-moving-my-beers-from-couchbase-to-mongodb.md","title":"Moving my beers from Couchbase to MongoDB","description":"Few days ago I have posted a joke on Twitter","date":"2015-02-01T00:00:00.000Z","formattedDate":"February 1, 2015","tags":[],"readingTime":6.875,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Moving my beers from Couchbase to MongoDB","categories":"mongodb nosql couchbase java"},"prevItem":{"title":"Introduction to MongoDB Security","permalink":"/blog/2015/02/04/introduction-to-mongodb-security"},"nextItem":{"title":"Everybody says \u201cHackathon\u201d!","permalink":"/blog/2015/01/23/everybody-says-hackathon"}},"content":"Few days ago I have posted a *joke* on Twitter\\n\\n<blockquote class=\\"twitter-tweet\\" lang=\\"en\\"><p>Moving my Java from Couchbase to MongoDB <a href=\\"http://t.co/Wnn3pXfMGi\\">pic.twitter.com/Wnn3pXfMGi</a></p>&mdash; Tugdual Grall (@tgrall) <a href=\\"https://twitter.com/tgrall/status/559664540041117696\\">January 26, 2015</a></blockquote>\\n<script async src=\\"//platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nSo I decided to move it from a simple picture to a *real* project. Let\'s look at the two phases of this so called project:\\n\\n* Moving the data from Couchbase to MongoDB\\n* Updating the application code to use MongoDB\\n\\nLook at this screencast to see it in action:\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/Fpl74Z0HbC0\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n## Moving the data\\n\\nI have created a replication server, that uses the Couchbase XDCR protocol to get the document out and insert them into MongoDB. This server use the Couchbase CAPI Server project available [here](https://github.com/couchbaselabs/couchbase-capi-server).\\n\\nThis server will receive all the mutations made in the Couchbase:\\n\\n* When a document is inserted or updated the full document is sent\\n* When a document is deleted, only the medata are sent\\n* The `replication server`, save the data into MongoDB (inserts and/or updates - no delete), and then return the list to Couchbase as part of the XDCR Protocol.\\n\\nOne of the challenges is the fact Couchbase does not have the notion of \\"types\\" or \\"collections\\". You put everything in a *bucket* and the application code knows how to deal with the data. Not necessary a problem, just a choice of implementation, but make it sometime harder than expected when you want to write tools. So here the logic that I apply in my replication server, to organize the data in multiple collections when it makes sense *(and when it is possible)*:\\n\\n* If the JSON document does not contains a *type field*, all the documents will be saved in a single collection\\n* If the JSON document contains a *type field* then a collection will be created for each type and documents will be inserted/updated in these collections\\n* MongoDB does not allow attributes key to have . and $ signs, so it is necessary to change the name with alternative characters. This is done automatically during the copy of the data.\\n\\nAll this, and more is configurable in the tool.\\n\\nAs you can see in the screencast this is straightforward.*(note that I have only tested very simple use cases and deployment)*\\n\\nYou can download the tool and the source code here:\\n\\n* [https://github.com/tgrall/mongodb-cb-replicator](https://github.com/tgrall/mongodb-cb-replicator)\\n* Download the [MongoCBReplicator.jar](http://goo.gl/WkuHBk) file.\\n\\n\\n## Updating the application code\\n\\nThe next step is to use these data in an application. For this I simply use the Beer Sample Java application available on [Couchbase repository](https://github.com/couchbaselabs/beersample-java).\\n\\nI just recreated the project and modified few things, to get the application up and running:\\n\\n* Change the connection string\\n* Remove the code that generate views\\n* Replace set/get by MongoDB operations\\n* Replace call to the views by simple queries\\n\\nThe code of the MongoDBeer application is available here:\\n\\n* [https://github.com/tgrall/mongodbeer]\\n\\nI did not change any business logic, or added features, or even replaced the way navigation and page rendition is made. I just focused on the database access, for example :\\n\\n``` java\\n\\n// Couchbase Query\\nView view = client.getView(\\"beer\\", \\"by_name\\");\\n    Query query = new Query();\\n    query.setIncludeDocs(true).setLimit(20);\\n    ViewResponse result = client.query(view, query);\\n\\n    ArrayList<HashMap<String, String>> beers =\\n      new ArrayList<HashMap<String, String>>();\\n    for(ViewRow row : result) {\\n      HashMap<String, String> parsedDoc = gson.fromJson(\\n        (String)row.getDocument(), HashMap.class);\\n\\n      HashMap<String, String> beer = new HashMap<String, String>();\\n      beer.put(\\"id\\", row.getId());\\n      beer.put(\\"name\\", parsedDoc.get(\\"name\\"));\\n      beer.put(\\"brewery\\", parsedDoc.get(\\"brewery_id\\"));\\n      beers.add(beer);\\n    }\\n    request.setAttribute(\\"beers\\", beers);\\n\\n\\n// MongoDB Query\\nDBCursor cursor = db.getCollection(\\"beer\\").find()\\n                                                   .sort( BasicDBObjectBuilder.start(\\"name\\",1).get() )\\n                                                   .limit(20);\\n     ArrayList<HashMap<String, String>> beers =\\n             new ArrayList<HashMap<String, String>>();\\n     while (cursor.hasNext()) {\\n         DBObject row = cursor.next();\\n         HashMap<String, String> beer = new HashMap<String, String>();\\n         beer.put(\\"id\\", (String)row.get(\\"_id\\"));\\n         beer.put(\\"name\\", (String)row.get(\\"name\\"));\\n         beer.put(\\"brewery\\", (String)row.get(\\"brewery_id\\"));\\n         beers.add(beer);\\n     }\\n\\n\\n\\n// Couchbase update\\nclient.set(beerId, 0, gson.toJson(beer));\\n\\n// MongoDB update\\ndb.getCollection(\\"beer\\").save(new BasicDBObject(beer));\\n\\n```\\n\\nI did not attend to optimize the MongoDB code,  but just to replace as few lines of code as possible.\\n\\n\\nNote: I have not created any index during the process. Obviously if your application have more and more data and you do intense work with it you must analyze your application/queries to see which indexes must be created.\\n\\n## Adding new features\\n\\nOnce you have the data into MongoDB you can do a lot more without anything more than MongoDB:\\n\\n#### Full Text Search\\n\\nYou can create a Text index on various fields in the collection to provide advanced search capabilities to your users.\\n\\n``` json\\ndb.brewery.ensureIndex(\\n  {\\n    \\"name\\" : \\"text\\",\\n    \\"description\\" : \\"text\\"\\n  },\\n  {\\n    \\"weights\\" :\\n    {\\n      \\"name\\" : 10,\\n      \\"description\\" : 5\\n    },\\n    \\"name\\" : \\"TextIndex\\"\\n  }\\n\\n);\\n```\\n\\nThen you can query the database using the `$text` operation, for example all breweries with *Belgium* and without *Ale*\\n\\n``` json\\ndb.brewery.find( { \\"$text\\" : { \\"$search\\" : \\"belgium -ale\\" }   }  , { \\"name\\" : 1  } );\\n{ \\"_id\\" : \\"daas\\", \\"name\\" : \\"Daas\\" }\\n{ \\"_id\\" : \\"chimay_abbaye_notre_dame_de_scourmont\\", \\"name\\" : \\"Chimay (Abbaye Notre Dame de Scourmont)\\" }\\n{ \\"_id\\" : \\"brasserie_de_cazeau\\", \\"name\\" : \\"Brasserie de Cazeau\\" }\\n{ \\"_id\\" : \\"inbev\\", \\"name\\" : \\"InBev\\" }\\n{ \\"_id\\" : \\"new_belgium_brewing\\", \\"name\\" : \\"New Belgium Brewing\\" }\\n{ \\"_id\\" : \\"palm_breweries\\", \\"name\\" : \\"Palm Breweries\\" }\\n```\\n\\n\\n#### Some analytics\\n\\nNot sure these queries really make sense, but it is just to show that now you can leverage your documents without the need of any 3rd party tool.\\n\\nNumber of beer by category, from the most common to the less one:\\n\\n``` json\\ndb.beer.aggregate([\\n  {\\"$group\\" : { \\"_id\\" : \\"$category\\",\\"count\\" : {\\"$sum\\" : 1 } } },\\n  {\\"$sort\\" : { \\"count\\" : -1 } },\\n  {\\"$project\\" : {\\t\\"category\\" : \\"$_id\\", \\"count\\" : 1, \\"_id\\" : 0 } }\\n]);\\n\\n{ \\"count\\" : 1996, \\"category\\" : \\"North American Ale\\" }\\n{ \\"count\\" : 1468, \\"category\\" : null }\\n{ \\"count\\" : 564, \\"category\\" : \\"North American Lager\\" }\\n{ \\"count\\" : 441, \\"category\\" : \\"German Lager\\" }\\n...\\n...\\n```\\n\\nNumber of beer of a specific ABV by brewery, for example: top 3 breweries with the most beer with an abv greather or equals to a value, let\'s say 5:\\n\\n``` json\\ndb.beer.aggregate([\\n... { \\"$match\\" : { \\"abv\\" : { \\"$gte\\" : 5 }  } },\\n... { \\"$group\\" : { \\"_id\\" : \\"$brewery_id\\", \\"count\\" : { \\"$sum\\" : 1} }},\\n... { \\"$sort\\" : { \\"count\\" : -1 } },\\n... { \\"$limit\\" : 3 }\\n... ])\\n\\n{ \\"_id\\" : \\"midnight_sun_brewing_co\\", \\"count\\" : 53 }\\n{ \\"_id\\" : \\"troegs_brewing\\", \\"count\\" : 33 }\\n{ \\"_id\\" : \\"rogue_ales\\", \\"count\\" : 31 }\\n```\\n\\n#### Geospatial queries\\n\\nThe first thing to do with the data is to change the data structure to save the various data into a GeoJSON format, for this we can simply use a script into the MongoDB Shell:\\n\\n``` json\\n>mongo\\n\\nuse beers\\n\\ndb.brewery.find().forEach(\\n  function( doc ) {\\n    var loc = { type : \\"Point\\" };\\n    if (doc.geo && doc.geo.lat && doc.geo.lon) {\\n      loc.coordinates = [ doc.geo.lon , doc.geo.lat  ];\\n      db.brewery.update( { _id : doc._id } , {$set : { loc : loc } }  );\\n    }\\n  }\\n);\\n\\ndb.brewery.ensureIndex( { \\"loc\\" : \\"2dsphere\\" } );\\n\\n```\\n\\nThis call take all the breweries and add a new attribute, name `loc` as a GeoJSON point. I could also chose to remove the old geo information using a \'$unset\', but I did not; let\'s imagine that some API/applications are using it. This is a good example of flexible schema.\\n\\nNow I can search for all the brewery that are at less than 30km from the Golden Gate in San Francisco: [-122.478255,37.819929]\\n\\n``` json\\ndb.brewery.find(\\n  { \\"loc\\" :\\n    { \\"$near\\" :\\n      { \\"$geometry\\" :\\n        {\\n          \\"type\\" : \\"Point\\",\\n          \\"coordinates\\" : [-122.478255,37.819929]\\n        },\\n        \\"$maxDistance\\" : 20000\\n\\n      }\\n    }\\n  }\\n  , { name : 1 }  \\n)\\n```\\n\\nYou can also use Geospatial indexes and operators in the aggregation queries used above\\n\\n\\n## Conclusion\\n\\nAs as said in the introduction, this week end project started as a joke on Twitter, and finished with a small blog post and Gitub repositories.\\n\\nMy goal here is not to compare the two solutions -I made my choice few months back-  but simply show how you can move from one to the other with almost no effort, not only the data but also the application code."},{"id":"/2015/01/23/everybody-says-hackathon","metadata":{"permalink":"/blog/2015/01/23/everybody-says-hackathon","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-01-23-everybody-says-hackathon.md","source":"@site/blog/2015-01-23-everybody-says-hackathon.md","title":"Everybody says \u201cHackathon\u201d!","description":"TLTR:","date":"2015-01-23T00:00:00.000Z","formattedDate":"January 23, 2015","tags":[],"readingTime":5.405,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Everybody says \u201cHackathon\u201d!","categories":"mongodb events hackathon"},"prevItem":{"title":"Moving my beers from Couchbase to MongoDB","permalink":"/blog/2015/02/01/moving-my-beers-from-couchbase-to-mongodb"},"nextItem":{"title":"Nantes MUG : Event #2","permalink":"/blog/2015/01/21/nantes-mug-event-number-2"}},"content":"#### TLTR:\\n* MongoDB & Sage organized an internal Hackathon\\n* We use the new X3 Platform based on MongoDB, Node.js and HTML to add cool features  to the ERP\\n* This shows that \u201cany\u201d enterprise can (should) do it to:\\n\\t* look differently at software development\\n\\t* build strong team spirit\\n\\t* have fun!\\n\\n\\n### Introduction\\n\\nI have like many of you participated to multiple Hackathons where developers, designer and entrepreneurs are working together to build applications in few hours/days. As you probably know more and more companies are running such events internally, it is the case for example at Facebook, Google, but also ING (bank), AXA (Insurance), and many more.\\n\\nLast week, I have participated to the first Sage Hackathon!\\n\\nIn case you do not know Sage is a 30+ years old ERP vendor. I have to say that I could not imagine that coming from such company\u2026 Let me tell me more about it.\\n\\n![](/images/posts/everybody-says-hackathon/00-logo.png )\\n\\n\x3c!-- truncate --\x3e\\n\\n### Sage Hackathon\\n\\n##### How did it start?\\n\\nI have met the development team few months back when I learned that Sage was using MongoDB. We discussed about use cases, architecture,... And this is when I was the most surprised! The new version of Sage ERP X3 is a mix of legacy components (RDBMS, C++ based proprietary middleware), and brand new layer based on Node.js, MongoDB and HTML/CSS/JS (AngularJS like). The Sage team has open sourced some of the JS libraries, see https://github.com/sage Pretty cool isn\u2019t?\\n\\nI was really excited to see how MongoDB and Node.js are used by Sage modernize the ERP. So I asked more and more questions about the product, looked at some demonstrations. This leads to a broader discussion to see how we can use this new architecture to develop more features using it.\\n\\nThis is how we started to talk about an internal hackathon. Everybody, developers, marketing and managers were very excited about the idea.\\n\\n\\n##### Hackathon Preparation\\n\\nSage and MongoDB teams worked together to organize the event, with the following constraints:\\n\\n* The hackathon will be a 24h (noon to noon) event, to allow Sage management to have corporate meetings before and after the event,\\n* 40 persons limit - 6 teams max - with a mix of developers, designer, product owner, quality engineer, \u2026 (coming from various countries),\\n* The event will occur offsite to *daily duty noise*\\n\\nAlso since the hackathon will be short, 24h!, we decided to propose in advance many subjects and teams. This to be able to focus on the implementation on D-Day and avoid \\"team and project\\" selection.\\n\\nSo we define a list of 6 ideas that would extend the ERP in a cool way, for example: notification platform, collaboration feature such as business discussion, caching layer with query capabilities, office tool integration, plug social network like LinkedIn and Twitter to ERP business objects,...\\n\\nOnce we had a good vision of the event, Sage marketing and product management organized an internal presentation to announce the event, and ask for more ideas. We were all surprised to see so many ideas coming out of this presentation!\\n\\n\\n##### Let\'s code!\\n\\nSo we all met at the location, a very nice conference center, [Les Fontaines](http://www.les-fontaines.com/), where fresh coffee was waiting for us!\\n\\nThe event started with a very short presentation of the teams, projects, and jury.\\n\\n![Presentation](/images/posts/everybody-says-hackathon/01-sage-hack-intro.png  )\\n\\nQuickly the teams started to draw things on whiteboard, discuss architecture, and design... The organizers, included myself, were\\nvery happy to see that everybody was diving into it.\\n\\n![Team Work](/images/posts/everybody-says-hackathon/02-team-work.png  ) \\n\\n![](/images/posts/everybody-says-hackathon/03-motto.png )\\n\\nMongoDB ([Alain](http://twitter.com/alainhelaili) and I), and Sage architects were here to help; so we did. I pushed hard to be sure all the teams start to develop, design\\nas early as possible. I also gave many advices around document design and other things around MongoDB and node.\\n\\nIn the evening we stopped for a nice dinner, this is the big difference between a startup event, and a corporate one, good wine, soup, duck confit, wine, and fantastic desserts. Yummy!\\n\\nLet\'s go back to the code thing, so all the teams were working like crazy on their project. I had lot of interesting discussions with all of them.\\n\\n![](/images/posts/everybody-says-hackathon/04-night-coding.png )  ![Night Coding](/images/posts/everybody-says-hackathon/05-night-coding.png  )\\n\\nI went to bed at 11:30pm, *yeah, I am a loser!*, while everybody was still working.\\n\\n![Night Coding](/images/posts/everybody-says-hackathon/06-night-coding.png ) ![Night Coding](/images/posts/everybody-says-hackathon/07-night-coding.png)\\n\\nI as back in the *war room* around 6:00am, and helped some team to finish their project.\\n\\nAll the teams used the morning to polish the feature and prepare the demonstration.\\n\\n\\n##### Let\'s vote!\\n\\nAt noon each team started to demonstrate their feature in 5mn. All the teams did a live demonstration, with the feature well integrated to the Sage X3 screens.\\nIt was really cool. A team even created a small video clip to explain the feature and vision.\\n\\n![](/images/posts/everybody-says-hackathon/08-demonstration.png )  ![](/images/posts/everybody-says-hackathon/10-demonstration.png ) \\n![](/images/posts/everybody-says-hackathon/11-demonstration.png ) ![](/images/posts/everybody-says-hackathon/12-demonstration.png ) \\n![](/images/posts/everybody-says-hackathon/13-demonstration.png ) ![](/images/posts/everybody-says-hackathon/14-demonstration.png ) \\n![](/images/posts/everybody-says-hackathon/15-demonstration.png ) ![](/images/posts/everybody-says-hackathon/09-demonstration.png )\\n\\nIt was very hard to chose a winner, but we agreed on a very rich and promising notification platform.\\n\\n\\n##### What\'s next?\\n\\nIt was really nice for me to see the excitement of all the teams, and the pride of being able to develop something that fast!\\n\\nI cannot talk for Sage, but I think they all realize something: they can do stuff a lot faster, and this should try to push this into the product now!\\nIt is also a good opportunity to see how to developer and deliver new addons for X3.\\n\\nOn my side, I am very happy of the result, and see that MongoDB and new technologies can really change the way we work with our data. And I hope to be able to do that again\\nwith other companies.\\n\\n\\n### What about you?\\n\\n\\nI believe that any organization that has an IT/Development team should organize such event, for example every year.\\n\\nHonestly, we can \\"stop\\" working on our daily duties for 1,2 or 3 days and do this. If you look at your agenda, I am sure that you have wasted\\nmore time on none productive meetings; remember :\\n\\n> If a picture is worth 1000 words\\n>\\n> A prototype is worth 1000 meetings!\\n\\nby @ideo\\n\\nYou will be surprised to see what can be done when you let the passionate people do what they love, but also it will be a good opportunity to motivate your team.\\n\\nI will be pleased to discuss that you will, so feel free to drop me a comment."},{"id":"/2015/01/21/nantes-mug-event-number-2","metadata":{"permalink":"/blog/2015/01/21/nantes-mug-event-number-2","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-01-21-nantes-mug-event-number-2.md","source":"@site/blog/2015-01-21-nantes-mug-event-number-2.md","title":"Nantes MUG : Event #2","description":"Last night the Nantes MUG (MongoDB Users Group) had its second event. More than 45 people signed up and joined us at the Epitech school (thanks for this!). &nbsp;We were lucky to have 2 talks from local community members:","date":"2015-01-21T00:00:00.000Z","formattedDate":"January 21, 2015","tags":[],"readingTime":2.515,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Nantes MUG : Event #2","categories":"events mongodb mug"},"prevItem":{"title":"Everybody says \u201cHackathon\u201d!","permalink":"/blog/2015/01/23/everybody-says-hackathon"},"nextItem":{"title":"How to create a pub/sub application with MongoDB ? Introduction","permalink":"/blog/2015/01/12/how-to-create-a-pub-slash-sub-application-with-mongodb-introduction"}},"content":"[Last night](http://www.meetup.com/Nantes-MongoDB-User-Group/events/218926859/) the Nantes MUG ([MongoDB Users Group](http://www.mongodb.org/user-groups)) had its second event. More than 45 people signed up and joined us at the [Epitech schoo](http://www.epitech.eu/nantes/ecole-informatique-nantes.aspx)l (thanks for this!). &nbsp;We were lucky to have 2 talks from local community members:\\n\\n\\n\\n* How \u201cMyScript Cloud\u201d uses MongoDB by [Mathieu Ruellan](https://twitter.com/mathieuruellan)\\n* Aggregation Framework by [Sebastien Prunier](https://twitter.com/sebprunier)\\n\\n\x3c!-- truncate --\x3e\\n\\n### How \u201cMyScript Cloud\u201d uses MongoDB\\n\\nFirst of all, if you do not know [MyScript](http://myscript.com/)&nbsp;I invite you to play with the [online demonstration](http://webdemo.myscript.com/#/home).&nbsp;I am pretty sure that you are already using this technology without noticing it, since it is embedded in many devices/applications including: your car look at the [Audi Touchpad](http://vimeo.com/49013364)!\\n\\nThat said Mathieu was not here to talk about the cool features and applications of MyScript but to explain how MongoDB is used to run their cloud product.&nbsp;\\nMathieu explained how you can use [MyScript SDK](https://dev.myscript.com/dev-kits/cloud-development-kit/) online. You just need to call a REST API to add Handwriting Recognition to your application. Let\'s make the long story short, and see how MongoDB was chosen and how it is used today:\\n\\n* The prototype was done with a single RDBMS instance\\n* With the success of the project MyScript Cloud the team had to move to a more flexible solution:\\n  * Flexible schema to support heterogeneous structures,\\n  * Highly available solution with automatic failover,\\n  * Multi datacenter supports with localized read,\\n* This is when Mathieu looked at different solution and selected MongoDB and deployed it on AWS.\\n\\nMathieu highlighted the following points:\\n\\n* Deploy and Manage a Replica Set is really easy, and it is done on multiple AWS data centers,\\n* Use the proper [read preference](http://docs.mongodb.org/manual/core/read-preference/)&nbsp; (nearest in this case) to deliver the data as fast as possible,\\n* Develop with JSON Documents provides lot of flexibility to the developers, that can add new features faster.\\n\\n![](http://4.bp.blogspot.com/-AWHn75hAyBY/VL9EMpRrFVI/AAAAAAAAAwA/CrDMkKL5A1Y/s1600/IMG_3743.jpg )\\n\\n\\n### Aggregation Framework\\n\\nSebastien \\"Seb\\" is software engineering at SERLI and working with MongoDB for more than 2 years now. Seb introduced the reasons why aggregations are needed in applications and the various ways of doing it with [MongoDB](http://docs.mongodb.org/manual/aggregation/): simple queries, map reduce, and aggregation pipeline; with a focus on a Aggregation Pipeline.\\n\\nUsing cool demonstrations, Seb explained in a step by step approach the key features and capabilities of MongoDB [Aggregation Pipeline](http://docs.mongodb.org/manual/core/aggregation-pipeline/):\\n\\n* $match, $group, ...\\n* $unwind arrays\\n* $sort and $limit\\n* $geonear\\n\\nTo close his presentation, Seb talked about aggregation best practices, and behavior&nbsp;[in a sharded cluster](http://docs.mongodb.org/manual/core/aggregation-pipeline-sharded-collections/#aggregation-pipeline-sharded-collection).\\n\\n![](http://4.bp.blogspot.com/-1fK-Q5SmL4s/VL9EQiaUIvI/AAAAAAAAAwI/AMVYrmQDPVg/s1600/IMG_3745.jpg )\\n\\n\\n### And...\\n\\nAs usual the event ended with some drinks and a late dinner!\\n\\nThis event was really great and I am very happy to see what people are doing with MongoDB, including storing _ink_ like MyScript, thanks again to the speakers!\\n\\nThis brings me to the last point : MUGs are driven by the community. You are using MongoDB and want to talk about what you, do not hesitate to reach out the organizers they will be more than happy to have you.\\n\\nYou can find a MUG near you, [look here](http://www.mongodb.org/user-groups)."},{"id":"/2015/01/12/how-to-create-a-pub-slash-sub-application-with-mongodb-introduction","metadata":{"permalink":"/blog/2015/01/12/how-to-create-a-pub-slash-sub-application-with-mongodb-introduction","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2015-01-12-how-to-create-a-pub-slash-sub-application-with-mongodb-introduction.md","source":"@site/blog/2015-01-12-how-to-create-a-pub-slash-sub-application-with-mongodb-introduction.md","title":"How to create a pub/sub application with MongoDB ? Introduction","description":"In this article we will see how to create a pub/sub application (messaging, chat, notification), and this fully based on MongoDB (without any message broker like RabbitMQ, JMS, ... ).","date":"2015-01-12T00:00:00.000Z","formattedDate":"January 12, 2015","tags":[{"label":"code","permalink":"/blog/tags/code"},{"label":"mongodb","permalink":"/blog/tags/mongodb"},{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"scala","permalink":"/blog/tags/scala"},{"label":"java","permalink":"/blog/tags/java"}],"readingTime":4.58,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"How to create a pub/sub application with MongoDB ? Introduction","tags":["code","mongodb","node.js","scala","java"]},"prevItem":{"title":"Nantes MUG : Event #2","permalink":"/blog/2015/01/21/nantes-mug-event-number-2"},"nextItem":{"title":"Big Data... Is Hadoop the good way to start?","permalink":"/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start"}},"content":"import Gist from \'react-gist\';\\n\\nIn this article we will see how to create a pub/sub application (messaging, chat, notification), and this fully based on MongoDB (without any message broker like RabbitMQ, JMS, ... ).\\n\\nSo, what needs to be done to achieve such thing:\\n\\n* an application \\"publish\\" a message. In our case, we simply save a document into MongoDB\\n* another application, or thread, subscribe to these events and will received message automatically. In our case this means that the application should automatically receive newly created document out of MongoDB\\n\\nAll this is possible with some very cool MongoDB features: [capped collections](http://docs.mongodb.org/manual/core/capped-collections/) and [tailable cursors](http://docs.mongodb.org/manual/tutorial/create-tailable-cursor/),\\n\\n\x3c!-- truncate --\x3e\\n\\n## Collections and Tailable Cursors\\n\\nAs you can see in the documentation, Capped Collections are fixed sized collections, that work in a way similar to circular buffers: once a collection fills its allocated space, it makes room for new documents by overwriting the oldest documents.\\n\\nMongoDB Capped Collections can be queried using Tailable Cursors, that are similar to the unix `tail -f` command. Your application continue to retrieve documents as they are inserted into the collection. I also like to call this a \\"continuous query\\".\\n\\nNow that we have seen the basics, let\'s implement it.\\n\\n## Building a very basic application\\n\\n\\n#### Create the collection\\n\\nThe first thing to do is to create a new capped collection:\\n\\n<Gist id=\\"f16b1d3b5bcc12a4270a\\" \\n      file=\'capped-collection\'  \\n/>\\n\\nFor simplicity, I am using the MongoDB Shell to create the `messages` collection in the `chat` database.\\n\\nYou can see on line #7 how to create a capped collection, with 2 options:\\n\\n* `capped : true` : this one is obvious\\n* `size : 10000` : this is a mandatory option when you create a capped collection. This is the maximum size in bytes. (will be raised to a multiple of 256)\\n\\nFinally, on line #9, I insert a dummy document, this is also mandatory to be able to get the tailable cursor to work.\\n\\n\\n#### Write an application\\n\\n\\nNow that we have the collection, let\'s write some code. First in *node.js*:\\n\\n<Gist id=\\"f16b1d3b5bcc12a4270a\\" \\n      file=\'app.js\'  \\n/>\\n\\n\\n\\nFrom lines #1 to 5 I just connect to my local mongoDB instance.\\n\\nThen on line #7, I get the `messages` collection.\\n\\nAnd on line #10, I execute a find, using a tailable cursor, using specific options:\\n\\n* `{}` : no filter, so all documents will be returned\\n* `tailable : true` : this one is clear, to say that we want to create a tailable cursor\\n* `awaitdata : true` : to say that we wait for data before returning no data to the client\\n* `numberOfRetries : -1` : The number of times to retry on time out, -1 is infinite, so the application will keep trying\\n\\nLine #11 just force the sort to the natural order.\\n\\nThen on line #12, the cursor returns the data, and the document is printed in the console each time it is inserted.\\n\\n\\n#### Test the Application\\n\\nStart the application\\n\\n`node app.js`\\n\\nInsert documents in the messages collection, from the shell or any other tool.\\n\\nYou can find below a screencast showing this very basic application working:\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/uSuiYvssKuo\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\nThe source code of this sample application in this Github repository, take the step-01 branch; clone this branch using:\\n\\n`git clone -b step-01 https://github.com/tgrall/mongodb-realtime-pubsub.git`\\n\\nI have also created a gist showing the same behavior in *Java*:\\n\\n<Gist id=\\"f16b1d3b5bcc12a4270a\\" \\n      file=\'MyApp.java\'  \\n/>\\n\\n\\nMathieu Ancelin has written it in *Scala*:\\n\\n<Gist id=\\"f16b1d3b5bcc12a4270a\\" \\n      file=\'App.scala\'  \\n/>\\n\\n\\n### Add some user interface\\n\\nWe have the basics of a publish subscribe based application:\\n\\n* publish by inserting document into MongoDB\\n* subscribe by reading document using a tailable cursor\\n\\nLet\'s now push the messages to a user using for example socket.io. For this we need to:\\n\\n* add socket.io dependency to our node project\\n* add HTML page to show messages\\n\\nThe following gists shows the updated version of the app.js and index.html, let\'s take a look:\\n\\n<Gist id=\\"d8c2acfdc416abcc5d18\\" \\n      file=\'app.js\'  \\n/>\\n\\nThe node application has been updated with the following features:\\n\\n* lines #4-7: import of http, file system and socket.io\\n* lines #10-21: configure and start the http server. You can see that I have created a simple handler to serve static html file\\n* lines #28-39: I have added support to Web socket using socket.io where I open the tailable cursor, and push/emit the messages on the socket.\\n\\nAs you can see, the code that I have added is simple. I do not use any advanced framework, nor manage exceptions, this for simplicity and readability.\\n\\nLet\'s now look at the client (html page).\\n\\n<Gist id=\\"d8c2acfdc416abcc5d18\\" \\n      file=\'index.html\'  \\n/>\\n\\nSame as the server, it is really simple and does not use any advanced libraries except socket.io client (line #18) and JQuery (line #19), and used:\\n\\n* on line #22 to received messages ans print them in the page using JQuery on line #23\\n\\n\\nI have created a screencast of this version of the application:\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/N9fDxuohdy8\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n\\nYou can find the source code in this Github repository, take the step-02 branch; clone this branch using:\\n\\n`git clone -b step-02 https://github.com/tgrall/mongodb-realtime-pubsub.git`\\n\\n\\n### Conclusion\\n\\nIn this first post, we have:\\n\\n* learned about tailable cursor and capped collection\\n* see how it can be used to develop a pub/sub application\\n* expose this into a basic web socket based application\\n\\nIn the next article we will continue to develop a bigger application using these features."},{"id":"/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start","metadata":{"permalink":"/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2014-11-25-big-data-dot-dot-dot-is-hadoop-the-good-way-to-start.md","source":"@site/blog/2014-11-25-big-data-dot-dot-dot-is-hadoop-the-good-way-to-start.md","title":"Big Data... Is Hadoop the good way to start?","description":"In the past 2 years, I have met many developers, architects that are working on \u201cbig data\u201d projects. This sounds amazing, but quite often the truth is not that amazing.","date":"2014-11-25T00:00:00.000Z","formattedDate":"November 25, 2014","tags":[],"readingTime":6.385,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Big Data... Is Hadoop the good way to start?","categories":"bigdata hadoop mongodb spark"},"prevItem":{"title":"How to create a pub/sub application with MongoDB ? Introduction","permalink":"/blog/2015/01/12/how-to-create-a-pub-slash-sub-application-with-mongodb-introduction"},"nextItem":{"title":"Introduction to MongoDB Geospatial feature","permalink":"/blog/2014/08/21/introduction-to-mongodb-geospatial-feature"}},"content":"In the past 2 years, I have met many developers, architects that are working on \u201cbig data\u201d projects. This sounds amazing, but quite often the truth is not that amazing.\\n\\n####TL;TR\\nYou believe that you have a big data project?\\n\\n* Do not start with the installation of an Hadoop Cluster -- the \\"*how*\\"\\n* Start to talk to business people to understand their problem -- the \\"*why*\\"\\n* Understand the data you must process\\n* Look at the volume -- very often it is not \\"that\\" big\\n* Then implement it, and take a simple approach, for example start with MongoDB + Apache Spark\\n\\n![\'Big Data\'](http://cdn.meme.am/instances/500x/47510205.jpg )\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n##The infamous \\"big data project\\"\\n\\nA typical discussion would look like:\\n\\nMe: *\u201cCan you tell me more about this project, what do you do with your data?\u201d*\\n\\nMr. Big Bytes: *\u201cSure, we have a 40 nodes Hadoop cluster...\\"*\\n\\nMe: *\u201cThis is cool but which type of data do you store, and what is the use case, business value?\\"*\\n\\nMr. Big Bytes: *\u201cWe store, all the logs of our applications, we have hundreds of gigabits\u2026\\"*\\n\\nAfter a long blank: *\u201cWe have not yet started to analyze these data. For now it is jut  \'us, the IT team,\' we store the data, like that soon we will be able to do interesting things with them\\"*\\n\\nYou can meet the same person few months later; the cluster is still sitting here, with no activity on it. I even met some consultants telling me they received calls from their customer asking the following:\\n\\n*\u201cHmmm, we have an Hadoop cluster installed, can you help us to find what to do with it?\\"*\\n\\n*Wrong! That is wrong!!!!!* This means that the IT Team has spent lot of time for nothing, at least for the business; and I am not even sure the team has learned something technically.\\n\\n###Start with the \\"Why\\" not with the \\"How\\"!\\n\\n\\nThe solution to this, could be obvious, start your \u201cbig data project\u201d answering the \u201cwhy/what\u201d questions first! The \u201chow\u201d, the implementation, will come later.\\n\\nI am sure that most of the enterprises will benefit of a so called \u201cbig data project\u201d, but it is really important to understand the problems first. *And these problems are not technical\u2026* at least at the beginning. So you must spend time with the business persons to understand what could help them. Let\'s take some examples.\\n\\n\\nYou are working in a bank or insurance, business people will be more than happy to predict when/why customer will be leaving the company by doing some *churn analysis*; or it will be nice to be able to see when it makes lot of sense *to sell new contracts*, service to existing customers.\\n\\nYou are working in retail/commerce, your business will be happy to see if they can <u>adjust the price</u> to the market, or <u>provide precise recommendations</u> to a user from an analysis of other customer behavior.\\n\\nWe can find many other examples. But as you can see we are not talking about technology, just business and possible benefits. In fact nothing new, compare with the applications you are building, you need first to have some requirements/ideas to build a product. Here we just need to have some \\"data input\\" to see <u>how we can enrich the data</u> with some business value.\\n\\nOnce you have started to ask all these questions you will start to see some input, and possible processing around them:\\n\\n* You are an insurance, you customers has no contact with your representative, or the customer satisfaction is medium/bad; you start to see some customer name in quotes coming from price comparison website\u2026. hmm you can guess that they are looking for a new insurance.\\n* Still in the insurance, when your customer are close to the requirement age, or has teenagers learning how to drive, moving to college, you know that you have some opportunity to sell new contract, or adapt existing ones to the new needs\\n* In retail, you may want to look to all customers and what they have ordered, and based on this be able to recommend some products to a customer that \\"looks\\" the same.\\n* Another very common use case these days, you want to do some sentiment analysis of social networks to see how your brand is perceived by your community\\n\\nAs you can see now, we can start to think about the data we have to use and the type of processing we have to do on them.\\n\\n###Let\'s now talk about the \\"How\\"\\n\\nNow that you have a better idea about what you want to do, it does not mean that you should dive into a large cluster installation.\\n\\nBefore that, you should continue to analyze the data:\\n\\n* What is the structure of the data that I have to analyze?\\n* How big is my dataset?\\n* How much data I have to ingest on a period of time (minute, hour, day, ...)\\n\\nAll these questions will help you to understand better your application. This is where it is often interesting too, and we realize that for most of us the \\"big data\\" is not that big!\\n\\nI was working the other day with a Telco company in Belgium, and I was talking about possible new project. I simply said:\\n\\n\\n* Belgium is what, 11+ millions of people\\n* If you store a 50kb object for each person this represent:\\n* Your full dataset will be 524Gb, yes not even a Terabyte!\\n\\nDo you need a large Hadoop cluster to store and process this? You can use it, but you do not need to! You can find something smaller, and easier to start with.\\n\\nAny database will do the job, starting with MongoDB. I think it is really interesting to start this project with a MongoDB cluster, not only because it will allow you to scale out as much as you need, but also because you will leverage the flexibility of the document model. This will allow you to store any type of data, and easily adapt the structure to the new data, or requirements.\\n\\nStoring the data is only one part of the equation. The other part is how you achieve the data processing. Lately I am playing a lot with [Apache Spark](https://spark.apache.org/). Spark provides a very powerful engine for large scale data processing, and it is a lot simpler than Map Reduce jobs. In addition to this, you can run Spark without Hadoop! This means you can connect you Spark to your MongoDB, with the [MongoDB Hadoop Connector](http://docs.mongodb.org/ecosystem/tools/hadoop/) and other data sources and directly execute job on your main database.\\n\\nWhat I like also about this approach, you can when you dataset starts to grow, and it become harder to process all the data on your operational database, you can easily add Hadoop; and keep most of your data processing layer intact, and only change the data sources information. In this case you will connect MongoDB and Hadoop to get/push the data into HDFS, once again using the MongoDB Hadoop Connector.\\n\\n###Conclusion\\n\\nToo many times, projects are driven by technology instead of focusing on the business value. This is particularly true around big data projects. So be sure you start by understanding the business problem, and find the data that could help to solve it.\\n\\nOnce you have the business problem and the data, select the good technology, that could be very simple, simple files and python scripts, or more often a database like MongoDB with a data processing layer like Spark. And start to move to Hadoop when it is really mandatory... a very, very, very, large dataset."},{"id":"/2014/08/21/introduction-to-mongodb-geospatial-feature","metadata":{"permalink":"/blog/2014/08/21/introduction-to-mongodb-geospatial-feature","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2014-08-21-introduction-to-mongodb-geospatial-feature.md","source":"@site/blog/2014-08-21-introduction-to-mongodb-geospatial-feature.md","title":"Introduction to MongoDB Geospatial feature","description":"This post is a quick and simple introduction to Geospatial feature of MongoDB 2.6 using simple dataset and queries.","date":"2014-08-21T00:00:00.000Z","formattedDate":"August 21, 2014","tags":[],"readingTime":8.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Introduction to MongoDB Geospatial feature","categories":"mongodb nosql json geo"},"prevItem":{"title":"Big Data... Is Hadoop the good way to start?","permalink":"/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start"},"nextItem":{"title":"db.person.find( { \'role\' : \'DBA\' } )","permalink":"/blog/2014/03/28/dbpersonfind-role-dba"}},"content":"This post is a quick and simple introduction to Geospatial feature of MongoDB 2.6 using simple dataset and queries.\\n\\n###  Storing Geospatial Informations\\n\\nAs you know you can store any type of data, but if you want to query them you need to use some coordinates, and create index on them. MongoDB supports three types of indexes for GeoSpatial queries:\\n\\n* [2d Index](http://docs.mongodb.org/manual/core/2d/) : uses simple coordinate (longitude, latitude). As stated in the documentation: _The 2d index is intended for legacy coordinate pairs used in MongoDB 2.2 and earlier_. For this reason, I won\'t detail anything about this in this post. Just for the record 2d Index are used to query data stored as points on a two-dimensional plane\\n* [2d Sphere Index](http://docs.mongodb.org/manual/core/2dsphere/) : support queries of any geometries on an-earth-like sphere, the data can be stored as GeoJSON and legacy coordinate pairs (longitude, latitude). For the rest of the article I will use this type of index and focusing on GeoJSON.\\n* [Geo Haystack](http://docs.mongodb.org/manual/core/geohaystack/) : that are used to query on very small area. It is today less used by applications and I will not describe it in this post.\\nSo this article will focus now on the 2d Sphere index with GeoJSON format to store and query documents.\\n\\n*So what is GeoJSON?*\\n\\nYou can look at the [http://geojson.org/](http://geojson.org/) site, let\'s do a very short explanation. GeoJSON is a format for encoding, in JSON, a variety of geographic data structures, and support the following types:  Point , LineString , Polygon , MultiPoint , MultiLineString , MultiPolygon and Geometry.\\n\\nThe GeoJSON format  is quite straightforward based, for the simple geometries, on two attributes: type and coordinates. Let\'s take some examples:\\n\\n\\nThe city where I spend all my childhood, Pleneuf Val-Andr\xe9, France, has the following coordinates (from Wikipedia)\\n\\n`48\xb0 35\u2032 30.12\u2033 N, 2\xb0 32\u2032 48.84\u2033 W`\\n\\nThis notation is a point, based on a latitude &amp; longitude using the [WGS 84](http://en.wikipedia.org/wiki/World_Geodetic_System) (Degrees, Minutes, Seconds) system. Not very easy to use by application/code, this is why it is also possible to represent the exact same point using the following values for latitude &amp; longitude:\\n\\n`48.5917, -2.5469`\\n\\nThis one uses the [WGS 84](http://en.wikipedia.org/wiki/World_Geodetic_System) (Decimal Degrees) system. This is the coordinates you see use in most of the application/API you are using as developer (eg: Google Maps/Earth for example)\\n\\nBy default GeoJSON, and MongoDB use these values but **the coordinates must be stored in the longitude, latitude order**, so this point in GeoJSON will look like:\\n\\n``` json\\n{\\n  \\"type\\": \\"Point\\",\\n  \\"coordinates\\": [\\n  -2.5469,  \\n  48.5917\\n  ]\\n}\\n```\\n\\n![](http://2.bp.blogspot.com/-0GfvAvSgLM8/U_NwAR_BCpI/AAAAAAAAArI/INweKtutfDQ/s1600/01-geojson-point.png )\\n\\nThis is a simple \\"Point\\", let\'s now for example look at a line, a very nice walk on the beach :\\n\\n``` json\\n{\\n  \\"type\\": \\"LineString\\",\\n  \\"coordinates\\": [\\n    [-2.551082,48.5955632],\\n    [-2.551229,48.594312],\\n    [-2.551550,48.593312],\\n    [-2.552400,48.592312],\\n    [-2.553677, 48.590898]\\n  ]\\n  }\\n```\\n\\n\\nhttp://1.bp.blogspot.com/-dg_myaJAG-c/U_Nv80jrncI/AAAAAAAAArA/utmCcBlQeqY/s1600/02-geojson-linestring.png )\\n\\n\\nSo using the same approach you will be able to create MultiPoint, MultiLineString, Polygon, MultiPolygon. It is also possible to mix all these in a single document using a GeometryCollection. The following example is a Geometry Collection of MultiLineString and Polygon over Central Park:\\n\\n``` json\\n{\\n  \\"type\\" : \\"GeometryCollection\\",\\n  \\"geometries\\" : [\\n    {\\n      \\"type\\" : \\"Polygon\\",\\n      \\"coordinates\\" : [\\n[\\n  [ -73.9580, 40.8003 ],\\n  [ -73.9498, 40.7968 ],\\n  [ -73.9737, 40.7648 ],\\n  [ -73.9814, 40.7681 ],\\n  [ -73.9580, 40.8003  ]\\n]\\n      ]\\n    },\\n    {\\n      \\"type\\" : \\"MultiLineString\\",\\n      \\"coordinates\\" : [\\n[ [ -73.96943, 40.78519 ], [ -73.96082, 40.78095 ] ],\\n[ [ -73.96415, 40.79229 ], [ -73.95544, 40.78854 ] ],\\n[ [ -73.97162, 40.78205 ], [ -73.96374, 40.77715 ] ],\\n[ [ -73.97880, 40.77247 ], [ -73.97036, 40.76811 ] ]\\n      ]\\n    }\\n  ]\\n}\\n```\\n\\n![](http://3.bp.blogspot.com/-tIxoUIeSMWw/U_SUsEJ_EDI/AAAAAAAAArY/2qelBrB1xRY/s1600/03-gejson-collection.png )\\n\\nNote: You can if you want test/visualize these JSON documents using the [http://geojsonlint.com/](http://geojsonlint.com/) service.\\n\\n\\n##### Now what? Let\'s store data!\\n\\nOnce you have a GeoJSON document you just need to store it into your document. For example if you want to store a document about JFK Airport with its location you can run the following command:\\n\\n``` js\\ndb.airports.insert(\\n{\\n  \\"name\\" : \\"John F Kennedy Intl\\",\\n  \\"type\\" : \\"International\\",\\n  \\"code\\" : \\"JFK\\",\\n  \\"loc\\" : {\\n    \\"type\\" : \\"Point\\",\\n    \\"coordinates\\" : [ -73.778889, 40.639722 ]\\n  }\\n}\\n```\\n\\nYes this is that simple! You just save the GeoJSON as one of the attribute of the document, `loc` in this example)\\n\\n\\n### Querying Geospatial Informations\\n\\n\\nNow that we have the data stored in MongoDB, it is now possible to use the geospatial information to do some interesting queries.\\n\\n\\n\\n\\nFor this we need a sample dataset. I have created one using some open data found in various places. This dataset contains the following informations:\\n\\n* airports collection with the list of US airport (Point)\\n* states collection with the list of US states (MultiPolygon)\\n\\nI have created this dataset from various OpenData sources ( [http://geocommons.com/](http://geocommons.com/) , [http://catalog.data.gov/dataset](http://catalog.data.gov/dataset) ) and use [toGeoJSON](https://github.com/mapbox/togeojson) to convert them into the proper format.\\n\\nLet\'s install the dataset:\\n\\n1.  Download it from [here](https://www.dropbox.com/s/yui7shcud2xbxt7/geo.zip)\\n2.  Unzip geo.zip file\\n3.  Restore the data into your mongoDB instance, using the following command\\n\\n```\\nmongorestore geo.zip\\n```\\n\\nMongoDB allows applications to do the following types of query on geospatial data:\\n\\n* inclusion\\n* intersection\\n* proximity\\n\\nObviously, you will be able to use all the other operator in addition to the geospatial ones. Let\'s now look at some concrete examples.\\n\\n\\n#### Inclusion\\n\\n\\n\\nFind all the airports in California. For this you need to get the California location (Polygon) and use the command $geoWithin in the query. From the shell it will look like :\\n\\n``` js\\nuse geo\\nvar cal = db.states.findOne(  {code : \\"CA\\"}  );\\n\\ndb.airports.find(\\n{\\n  loc : { $geoWithin : { $geometry : cal.loc } }\\n},\\n{ name : 1 , type : 1, code : 1, _id: 0 }\\n);\\n```\\n\\nResult:\\n\\n``` json\\n{ \\"name\\" : \\"Modesto City - County\\", \\"type\\" : \\"\\", \\"code\\" : \\"MOD\\" }\\n...\\n{ \\"name\\" : \\"San Francisco Intl\\", \\"type\\" : \\"International\\", \\"code\\" : \\"SFO\\" }\\n{ \\"name\\" : \\"San Jose International\\", \\"type\\" : \\"International\\", \\"code\\" : \\"SJC\\" }\\n...\\n```\\n\\nSo the query is using the \\"California MultiPolygon\\" and looks in the airports collection to find all the airports that are in these polygons. This looks like the following image on a map:\\n\\n![](http://1.bp.blogspot.com/-AO6C6fgsrYQ/U_Wyr2RHPWI/AAAAAAAAAro/hVn6YFJQtNI/s1600/04-geojson-cal-airport.png )\\n\\nYou can use any other query features or criteria, for example you can limit the query to international airport only sorted by name :\\n\\n``` js\\ndb.airports.find(\\n{\\n  loc : { $geoWithin : { $geometry : cal.loc } },\\n  type : \\"International\\"\\n},\\n{ name : 1 , type : 1, code : 1, _id: 0 }\\n).sort({ name : 1 });\\n```\\n\\nResult:\\n\\n``` json\\n{ \\"name\\" : \\"Los Angeles Intl\\", \\"type\\" : \\"International\\", \\"code\\" : \\"LAX\\" }\\n{ \\"name\\" : \\"Metropolitan Oakland Intl\\", \\"type\\" : \\"International\\", \\"code\\" : \\"OAK\\" }\\n{ \\"name\\" : \\"Ontario Intl\\", \\"type\\" : \\"International\\", \\"code\\" : \\"ONT\\" }\\n{ \\"name\\" : \\"San Diego Intl\\", \\"type\\" : \\"International\\", \\"code\\" : \\"SAN\\" }\\n{ \\"name\\" : \\"San Francisco Intl\\", \\"type\\" : \\"International\\", \\"code\\" : \\"SFO\\" }\\n{ \\"name\\" : \\"San Jose International\\", \\"type\\" : \\"International\\", \\"code\\" : \\"SJC\\" }\\n{ \\"name\\" : \\"Southern California International\\", \\"type\\" : \\"International\\", \\"code\\" : \\"VCV\\" }\\n```\\n\\nI do not know if you have looked in detail, but we are querying these documents with no index. You can run a query with the `explain()` to see what\'s going on. The `$geoWithin` operator does not need index but your queries will be more efficient with one so let\'s create the index:\\n\\n``` js\\ndb.airports.ensureIndex( { \\"loc\\" : \\"2dsphere\\" } );\\n```\\n\\nRun the explain and you will se the difference.\\n\\n#### Intersection\\n\\nSuppose you want to know what are all the adjacent states to California, for this we just need to search for all the states that have coordinates that \\"intersects\\" with California. This is done with the following query:\\n\\n``` js\\nvar cal = db.states.findOne(  {code : \\"CA\\"}  );\\ndb.states.find(\\n{\\n  loc : { $geoIntersects : { $geometry : cal.loc  }  } ,\\n  code : { $ne : \\"CA\\"  }  \\n},\\n{ name : 1, code : 1 , _id : 0 }\\n);\\n```\\n\\nResult:\\n\\n``` json\\n{ \\"name\\" : \\"Oregon\\", \\"code\\" : \\"OR\\" }\\n{ \\"name\\" : \\"Nevada\\", \\"code\\" : \\"NV\\" }\\n{ \\"name\\" : \\"Arizona\\", \\"code\\" : \\"AZ\\" }\\n```\\n\\n![](http://3.bp.blogspot.com/--Kh1AzmsaSU/U_XreY-tRlI/AAAAAAAAAr4/cS1pgjgF2Pc/s1600/05-geojson-intersect.png )\\n\\nSame as before `$geoIntersect` operator does not need an index to work, but it will be more efficient with the following index:\\n\\n``` js\\ndb.states.ensureIndex( { loc : \\"2dsphere\\" } );\\n```\\n\\n#### Proximity\\n\\nThe last feature that I want to highlight in this post is related to query with proximity criteria. Let\'s find all the international airports that are located at less than 20km from the reservoir in NYC Central Park. For this you will be using the `$near` operator.\\n\\n``` js\\ndb.airports.find(\\n{\\n  loc : {\\n    $near : {\\n      $geometry : {\\n        type : \\"Point\\" ,\\n        coordinates : [-73.965355,40.782865]  \\n      },\\n      $maxDistance : 20000\\n    }\\n  },\\n  type : \\"International\\"\\n},\\n{\\n  name : 1,\\n  code : 1,\\n  _id : 0\\n}\\n);\\n```\\n\\nResults:\\n\\n``` json\\n{ \\"name\\" : \\"La Guardia\\", \\"code\\" : \\"LGA\\" }\\n{ \\"name\\" : \\"Newark Intl\\", \\"code\\" : \\"EWR\\"}\\n```\\n\\nSo this query returns 2 airports, the closest being La Guardia, since the `$near` operator sorts the results by distance. Also it is important to raise here that the `$near` operator requires an index.\\n\\n### Conclusion\\n\\nIn this first post about geospatial feature you have learned:\\n\\n* the basic of GeoJSON\\n* how to query documents with inclusion, intersection and proximity criteria.\\n\\nYou can now play more with this for example integrate this into an application that expose data into some UI, or see how you can use the geospatial operators into a aggregation pipeline."},{"id":"/2014/03/28/dbpersonfind-role-dba","metadata":{"permalink":"/blog/2014/03/28/dbpersonfind-role-dba","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2014-03-28-dbpersonfind-role-dba.md","source":"@site/blog/2014-03-28-dbpersonfind-role-dba.md","title":"db.person.find( { \'role\' : \'DBA\' } )","description":"Wow! it has been a while since I posted something on my blog post. I have been very busy, moving to MongoDB, learning, learning, learning\u2026finally I can breath a little and answer some questions.","date":"2014-03-28T00:00:00.000Z","formattedDate":"March 28, 2014","tags":[],"readingTime":5.24,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"db.person.find( { \'role\' : \'DBA\' } )","categories":"nosql mongodb ops devops"},"prevItem":{"title":"Introduction to MongoDB Geospatial feature","permalink":"/blog/2014/08/21/introduction-to-mongodb-geospatial-feature"},"nextItem":{"title":"Pagination with Couchbase","permalink":"/blog/2013/10/01/pagination-with-couchbase"}},"content":"Wow! it has been a while since I posted something on my blog post. I have been very busy, moving to MongoDB, learning, learning, learning\u2026finally I can breath a little and answer some questions.\\n\\nLast week I have been helping my colleague Norberto to deliver a MongoDB Essentials Training in Paris. This was a very nice experience, and I am impatient to deliver it on my own. I was happy to see that the audience was well balanced between developers and operations, mostly DBA.\\n\\n### What! I still need DBA?\\n\\n![](http://ct.fra.bz/ol/fz/sw/i53/5/6/8/frabz-what-if-i-told-you-you-dont-need-to-know-sql-to-be-a-dba-85380e.jpg )\\n\\nThis is a good opportunity to raise a point, or comment a wrong idea: the fact that you are using MongoDB, or any other NoSQL datastore does not mean that you do not need a DBA\u2026 Like any project, an administrator is not mandatory, but if you have one it is better. So even when MongoDB is pushed by development team it is very important to understand the way the database works, and how to administer, monitor it.\\n\\nIf you are lucky enough to have real operations teams, with good system and database administrators use them! They are very important for your application.\\n\\nMost DBA/System Administrators have been maintaining systems in production for many years. They know how to keep your application up and running. They also most of the time experienced many \u201cdisasters\u201d, and then recover (I hope).\\n\\nWho knows, you may encounter big issues with your application and you will be happy to have them on your side at this moment.\\n\\n### \\"Great, but the DBA is slowing down my development!\\"\\n\\nI hear this, sometimes, and I had this feeling in the past to as developer in large organization. Is it true?\\n\\nDevelopers and DBA are today, not living in the same worlds:\\n\\n* Developers want to integrate new technologies as soon as possible, not only because it is fun and they can brag about it during meetups/conferences; but because these technologies, most of the time, are making them more productive, and offer better service/experience to the consumer\\n* DBA, are here to keep the applications up and running! So every time they do not feel confident about a technology they will push back. I think this is natural and I would be probably the same in their position. Like all geeks, they would love to adopt new technologies but they need to understand and trust it before.\\n\\nSystem administrators, DBAS look at the technology with a different angle than developers.\\n\\nBased on this assumption, it is important to bring the operation team as early as possible when  the development team wants to integrate MongoDB or any new data store. Having the operation team in the loop early will ease the global adoption of MongoDB in the company.\\n\\nPersonally, and this will show my age, I have seen a big change in the way developers and DBAs are working together.\\n\\nBack in the 90\'s, when the main architecture was based on client/server architecture  developers and DBAs where working pretty well togethers; probably because they were speaking the same language: SQL was everywhere.  I had regular meetings wit\\n\\nThen, since mid 2000, mots of applications have moved to a web based architecture , with for example Java middleware, and the developers stopped working with DBAs. Probably because the abstraction data layer provided by the ORM exposed the database as a \\"commodity\\" service that is supposed to work: \\"Hey Mr DBA, my application has been written with the best middleware technology on the market, so now deal with the performance and scalability! I am done!\\"\\n\\nYes it is a clich\xe9, but I am sure that some of you will recognize that.\\n\\nNevertheless each time I can, I have been pushing developers to talk more to administrators and look closely to their database!\\n\\n### A new era for operations and development teams\\n\\nThe fast adoption of MongoDB by developers, is a great opportunity to fix what we have broken 10 years ago in large information systems:\\n\\n* Let\'s talk again!\\n\\nMongoDB has been built first for developers. The document oriented approach gives lot of flexibility to quickly adapt to change. So anytime your business users need a new feature you can implement it, even if this change impact the data structure. Your data model is now driven and controlled by the application, not the database engine.\\n\\nHowever, the applications still need to be available 24x7, and performs well. These topics are managed - and shared- by administrator and developers! This has been always the case but, as I described it earlier, it looks like some of us have forgotten that.\\n\\nSchemas design, change velocity, are driven by the application, so the business and development teams, but all this impacts the database, for example:\\n\\n* How storage will grow ?\\n* Which indexes must be created to speed up my application?\\n* How to organize my cluster to leverage the infrastructure properly:\\n    * Replica-Set organization (and related write concerns, managed by developer)\\n    * Sharding options\\n* And the most important of them : backup/recovery strategies\\n\\nSo many things that could be managed by the project team, but if you have an operation team with you, it will be better to do that as a single team.\\n\\nYou, the developer, are convinced that MongoDB is the best database for your projects! Now it is time to work with the ops team and convince them too.  So you should for sure explain why MongoDB is good for you as developer, but also you should highlight all the benefits for the operations, starting with built-in high-availability with replica sets, and easy scalability with sharding. MongoDB is also here to make the life of the administrator easier! I have shared in the next paragraph a lit of resources that are interesting for operations people.\\n\\nLet\u2019s repeat it another time, try to involve the operation team as soon as possible, and use that as an opportunity to build/rebuild the relationship between developers and system administrators!\\n\\n### Resources\\n\\nYou can find many good resources on the Site to helps operations or learn about this:\\n\\n* Documentation : [Operations](http://docs.mongodb.org/manual/administration/)\\n* Online Training :\\n    * [M102: MongoDB for DBAs](https://education.mongodb.com/courses/10gen/M102/2014_May/about)\\n    * [M202: MongoDB Advanced Deployment and Operations](https://education.mongodb.com/courses/10gen/M202/2014_April/about)\\n* And many others such as White Papers and [Webinars](http://www.mongodb.com/webinars)."},{"id":"/2013/10/01/pagination-with-couchbase","metadata":{"permalink":"/blog/2013/10/01/pagination-with-couchbase","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-10-01-pagination-with-couchbase.md","source":"@site/blog/2013-10-01-pagination-with-couchbase.md","title":"Pagination with Couchbase","description":"If you have to deal with a large number of documents when doing queries against a Couchbase cluster it is important to use pagination to get rows by page. You can find some information in the documentation in the chapter \\"Pagination\\", but I want to go in more details and sample code in this article.","date":"2013-10-01T00:00:00.000Z","formattedDate":"October 1, 2013","tags":[],"readingTime":7.125,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Pagination with Couchbase","categories":"couchbase nosql json"},"prevItem":{"title":"db.person.find( { \'role\' : \'DBA\' } )","permalink":"/blog/2014/03/28/dbpersonfind-role-dba"},"nextItem":{"title":"How to implement Document Versioning with Couchbase","permalink":"/blog/2013/07/18/how-to-implement-document-versioning-with-couchbase"}},"content":"import Gist from \'react-gist\';\\n\\n\\nIf you have to deal with a large number of documents when doing queries against a Couchbase cluster it is important to use pagination to get rows by page. You can find some information in the documentation in the chapter \\"[Pagination](http://docs.couchbase.com/couchbase-manual-2.2/#pagination)\\", but I want to go in more details and sample code in this article.\\n\\nFor this example I will start by creating a simple view based on the `beer-sample` dataset, the view is used to find brewery by country:\\n\\n``` js\\nfunction (doc, meta) {\\n  if (doc.type == \\"brewery\\" &amp;&amp; doc.country){\\n    emit(doc.country);\\n  }\\n}\\n```\\n\\n\\nThis view list all the breweries by country, the index looks like:\\n\\n\\n<table>\\n  <tbody>\\n    <tr>\\n      <th>Doc id</th>\\n      <th>Key</th>\\n      <th>Value</th>\\n    </tr>\\n    <tr>\\n      <td>bersaglier</td>\\n      <td>Argentina</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>cervecera_jerome</td>\\n      <td>Argentina</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>brouwerij_nacional_balashi</td>\\n      <td>Aruba</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>australian_brewing_corporation</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>carlton_and_united_breweries</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>coopers_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>foster_s_australia_ltd</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>gold_coast_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>lion_nathan_australia_hunter_street</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>little_creatures_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>malt_shovel_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>matilda_bay_brewing</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <td>yellowstone_valley_brewing</td>\\n      <td>United States</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>yuengling_son_brewing</td>\\n      <td>United States</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>zea_rotisserie_and_brewery</td>\\n      <td>United States</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>fosters_tien_gang</td>\\n      <td>Viet Nam</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>hue_brewery</td>\\n      <td>Viet Nam</td>\\n      <td>null</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n\\n\\nSo now you want to navigate in this index with a page size of 5 rows.\\n\\n\x3c!--more--\x3e\\n\\n### Using skip / limit Parameters\\n\\nThe most simplistic approach is to use `limit` and `skip` parameters for example:\\n\\nPage 1 : `?limit=5&amp;skip0`  \\nPage 2 : `?limit=5&amp;skip=5`\\n...\\nPage x : `?limit=5&amp;skip(limit*(page-1))`\\n\\nYou can obviously use any other parameters you need to do range or key queries (`startkey/endkey, key, keys`) and sort option (`descending`).\\n\\nThis is simple but not the most efficient way, since the query engine has to read all the rows that match the query, until the skip value is reached.\\n\\nSome code sample in python that paginate using this view :\\n\\n\\n<Gist id=\\"6174762\\" />\\n\\nThis application loops on all the pages until the end of the index.\\n\\nAs I said before this is not the best approach since the system must read all the values until the skip is reached. The following example shows a better way to deal with this.\\n\\n### Using startkey / startkey_docid parameters\\n\\nTo make this pagination more efficient it is possible to take another approach. This approach uses the  `startkey` and `startkey_docid`  to select the proper documents.\\n\\n* `The startkey` parameter will be the value of the key where the query should start to read (based on the last key of the \\"previous page\\"\\n* Since for a key for example \\"Germany\\" you may have one or more ids (documents) it is necessary to say to Couchbase query engine where to start, for this you need to use the `startkey_docid` parameter, and ignore this id since it is the last one of the previous page.\\n\\nSo if we look at the index, and add a row number to explain the pagination\\n\\n<table>\\n  <tbody>\\n    <tr>\\n      <th>Row num</th>\\n      <th>Doc id</th>\\n      <th>Key</th>\\n      <th>Value</th>\\n    </tr>\\n    <tr>\\n      <td colspan=\\"4\\">\\n        <br />\\n        Query for page 1<br />\\n        `?limit=5`\\n      </td>\\n    </tr>\\n    <tr>\\n      <td>1</td>\\n      <td></td>\\n      <td>bersaglier</td>\\n      <td>Argentina</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>2</td>\\n      <td></td>\\n      <td>cervecera_jerome</td>\\n      <td>Argentina</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>3</td>\\n      <td></td>\\n      <td>brouwerij_nacional_balashi</td>\\n      <td>Aruba</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>4</td>\\n      <td></td>\\n      <td>australian_brewing_corporation</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>5</td>\\n      <td></td>\\n      <td>carlton_and_united_breweries</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td colspan=\\"4\\">\\n        Query for page 2<br />\\n        `?limit=5&amp;startkey=\\"Australia\\"&amp;startkey_docid=carlton_and_united_breweries&amp;skip=1`\\n      </td>\\n    </tr>\\n    <tr>\\n      <td>6</td>\\n      <td></td>\\n      <td>coopers_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>7</td>\\n      <td></td>\\n      <td>foster_s_australia_ltd</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>8</td>\\n      <td></td>\\n      <td>gold_coast_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>9</td>\\n      <td></td>\\n      <td>lion_nathan_australia_hunter_street</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>10</td>\\n      <td></td>\\n      <td>little_creatures_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td colspan=\\"4\\">\\n        <br />\\n        Query for page 3<br />\\n        `?limit=5&amp;startkey=\\"Australia\\"&amp;startkey_docid=little_creatures_brewery``&amp;skip=1`\\n      </td>\\n    </tr>\\n    <tr>\\n      <td>11</td>\\n      <td></td>\\n      <td>malt_shovel_brewery</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>12</td>\\n      <td></td>\\n      <td>matilda_bay_brewing</td>\\n      <td>Australia</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td></td>\\n      <td>yellowstone_valley_brewing</td>\\n      <td>United States</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td></td>\\n      <td>yuengling_son_brewing</td>\\n      <td>United States</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td></td>\\n      <td>zea_rotisserie_and_brewery</td>\\n      <td>United States</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td></td>\\n      <td>fosters_tien_gang</td>\\n      <td>Viet Nam</td>\\n      <td>null</td>\\n    </tr>\\n    <tr>\\n      <td>...</td>\\n      <td></td>\\n      <td>hue_brewery</td>\\n      <td>Viet Nam</td>\\n      <td>null</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n\\nSo as you can see in the examples above, the query uses the startkey, a document id, and just passes it using skip=1.\\n\\nLet\'s now look at the application code, once again in Python\\n\\n```python\\nfrom couchbase import Couchbase\\ncb = Couchbase.connect(bucket=\'beer-sample\')\\n\\nhasRow = True\\nrowPerPage = 5\\npage = 0\\ncurrentStartkey=\\"\\"\\nstartDocId=\\"\\"\\n\\nwhile hasRow :\\n\\thasRow = False\\n\\tskip = 0 if page == 0 else 1\\n\\tpage = page + 1\\n\\tprint \\"-- Page %s --\\" % (page)\\n\\trows = cb.query(\\"test\\", \\"by_country\\", limit=rowPerPage, skip=skip, startkey=currentStartkey, startkey_docid=startDocId)\\n\\tfor row in rows:\\n\\t\\thasRow = True\\n\\t\\tprint \\"Country: \\\\\\"%s\\\\\\" \\\\t Id: \'%s\'\\" % (row.key, row.docid)\\n\\t\\tcurrentStartkey = row.key\\n\\t\\tstartDocId = row.docid\\n\\tprint \\" -- -- -- -- \\\\n\\"\\n```\\n\\nThis application loops on all the pages until the end of the index\\n\\nUsing this approach, the application start to read the index at a specific key (`startkey` parameter), and only loop on the necessary entry in the index. This is more efficient than using the simple skip approach.  \\n#### Views with Reduce function\\n\\nWhen your view is using a reduce function, if you want to paginate on the various keys only (with the reduce function) you need to use the `skip` and `limit` parameters.\\n\\nWhen you are using the  paramater `startkey_docid` with a reduce function it will calculate the reduce only to the subset of document ids that are part of your query.\\n\\n### Couchbase Java SDK Paginator\\n\\nIn the previous examples, I have showed how to do pagination using the various query parameters. The Java SDK provides a Paginator object to help developers to deal with pagination. The following example is using the same view with the Paginator API.\\n\\n```java\\npackage com.couchbase.devday;\\n\\nimport com.couchbase.client.CouchbaseClient;\\nimport com.couchbase.client.protocol.views.*;\\nimport java.net.URI;\\nimport java.util.HashMap;\\nimport java.util.LinkedList;\\nimport java.util.List;\\nimport java.util.Properties;\\nimport java.util.concurrent.TimeUnit;\\nimport java.util.logging.ConsoleHandler;\\nimport java.util.logging.Handler;\\nimport java.util.logging.Level;\\nimport java.util.logging.Logger;\\n\\npublic class JavaPaginatorSample {\\n\\npublic static void main(String[] args) {\\n\\n\\tconfigure();\\n\\tSystem.out.println(\\"--------------------------------------------------------------------------\\");\\n\\tSystem.out.println(\\"\\\\tCouchbase - Paginator\\");\\n\\tSystem.out.println(\\"--------------------------------------------------------------------------\\");\\n\\n    List<URI> uris = new LinkedList<URI>();\\n    uris.add(URI.create(\\"http://127.0.0.1:8091/pools\\"));\\n\\n    CouchbaseClient cb = null;\\n    try {\\n    \\tcb = new CouchbaseClient(uris, \\"beer-sample\\", \\"\\");\\n\\t  \\tSystem.out.println(\\"--------------------------------------------------------------------------\\");\\n\\t  \\tSystem.out.println(\\"Breweries (by_name) with docs & JSON parsing\\");\\n\\t\\tView view = cb.getView(\\"test\\", \\"by_country\\");\\n\\t\\tQuery query = new Query();\\n\\t\\tint docsPerPage = 5;\\n\\n\\t\\tPaginator paginatedQuery = cb.paginatedQuery(view, query, docsPerPage);\\n\\t\\tint pageCount = 0;\\n\\t\\twhile(paginatedQuery.hasNext()) {\\n\\t\\t\\tpageCount++;\\n\\t\\t\\tSystem.out.println(\\" -- Page \\"+ pageCount +\\" -- \\");\\n\\t\\t\\tViewResponse response = paginatedQuery.next();\\n\\t\\t\\tfor (ViewRow row : response) {\\n\\t\\t\\t\\tSystem.out.println(row.getKey() + \\" : \\" + row.getId());\\n\\t\\t\\t}\\n\\t\\t\\tSystem.out.println(\\" -- -- -- \\");\\n\\t\\t}\\n\\t\\t\\n\\t\\tSystem.out.println(\\"\\\\n\\\\n\\");\\n    \\tcb.shutdown(10, TimeUnit.SECONDS);\\n    } catch (Exception e) {\\n    \\tSystem.err.println(\\"Error connecting to Couchbase: \\" + e.getMessage());\\n    }\\n}\\n\\n\\n\\nprivate static void configure() {\\n\\n\\tfor(Handler h : Logger.getLogger(\\"com.couchbase.client\\").getParent().getHandlers()) {\\n\\t\\tif(h instanceof ConsoleHandler) {\\n\\t\\t\\th.setLevel(Level.OFF);\\n\\t\\t}\\n\\t}\\n\\tProperties systemProperties = System.getProperties();\\n\\tsystemProperties.put(\\"net.spy.log.LoggerImpl\\", \\"net.spy.memcached.compat.log.SunLogger\\");\\n\\tSystem.setProperties(systemProperties);\\n\\n\\tLogger logger = Logger.getLogger(\\"com.couchbase.client\\");\\n\\tlogger.setLevel(Level.OFF);\\n\\tfor(Handler h : logger.getParent().getHandlers()) {\\n\\t\\tif(h instanceof ConsoleHandler){\\n\\t\\t\\th.setLevel(Level.OFF);\\n\\t\\t}\\n\\t}\\n}\\n\\n}\\n```\\n\\n\\nSo as you can see you can easily paginate on the results of a Query using the Java Paginator.\\n\\n* At the line #37, the Paginator is created from using the view and query objects and a page size is specified\\n* Then you just need to use the hasNext() and next() methods to navigate in the results.\\n\\nThe Java Paginator  is aware of the fact that they query is using a reduce or not, so you can use it with all type of queries - Internally it will switch between the skip/limit approach and the doc_id approaches. You can [see how it is done in the Paginator class](https://github.com/couchbase/couchbase-java-client/blob/1.1.9/src/main/java/com/couchbase/client/protocol/views/Paginator.java#L176-L195).\\n\\nNote that if you want to do that in a Web application between HTTP request you must keep the Paginator object in the user session since the current API keeps the current page in its state.\\n\\n### Conclusion\\n\\nIn this blog post you have  learned how to deal with pagination in Couchbase views; to summarize\\n\\n* The pagination is based on some specific parameters that you send when executing a query.\\n* Java developers can use the [Paginator](http://www.couchbase.com/autodocs/couchbase-java-client-1.2.0/com/couchbase/client/protocol/views/Paginator.html) class that simplifies pagination.\\n\\nI am inviting you to look at the new Couchbase Query Language N1QL, still under development, that will provide more options to developers including pagination, using LIMIT &amp; OFFSET parameters, for example:\\n\\n``` sql\\nSELECT fname, age\\nFROM tutorial\\nWHERE age > 30\\nLIMIT 2\\nOFFSET 2\\n```\\n\\nIf you want to learn more about N1QL:\\n\\n* [N1QL on Couchbase Community Portal](http://query.couchbase.com/)\\n* [N1QL Online Tutorial](http://query.pub.couchbase.com/tutorial/)"},{"id":"/2013/07/18/how-to-implement-document-versioning-with-couchbase","metadata":{"permalink":"/blog/2013/07/18/how-to-implement-document-versioning-with-couchbase","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-07-18-how-to-implement-document-versioning-with-couchbase.md","source":"@site/blog/2013-07-18-how-to-implement-document-versioning-with-couchbase.md","title":"How to implement Document Versioning with Couchbase","description":"Introduction","date":"2013-07-18T00:00:00.000Z","formattedDate":"July 18, 2013","tags":[],"readingTime":6.54,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"How to implement Document Versioning with Couchbase","categories":"couchbase java nosql json"},"prevItem":{"title":"Pagination with Couchbase","permalink":"/blog/2013/10/01/pagination-with-couchbase"},"nextItem":{"title":"Deploy your Node/Couchbase application to the cloud with Clever Cloud","permalink":"/blog/2013/07/11/deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud"}},"content":"###  Introduction\\n\\nDevelopers are often asking me how to \\"version\\" documents with Couchbase 2.0. The short answer is: the clients and server do not expose such feature, but it is quite easy to implement.\\n\\nIn this article I will use a basic approach, and you will be able to extend it depending of your business requirements.\\n\\n\x3c!-- truncate --\x3e\\n\\n###  Design\\n\\nThe first thing to do is to select how to \\"store/organize\\" the versions of your document, and for this you have different designs:\\n\\n*   copy the versions the document into new documents\\n*   copy the versions of the document into a list of embedded documents\\n*   store the list of attributes that have been changed into a embedded element (or new documents)\\n*   store the \\"delta\\"\\n*   \u2026\\n\\nYou will have to chose the design based on your application requirements (business logic, size of the dataset, ...).  For this article, let\'s use one of the most simplistic approach: create new document for each version with the following rules for the keys:\\n\\n1.  The current version is is a simple Key/Document, no change to the key.\\n2.  The version is a copy of the document, and the version number is added to the key.\\n\\nThis looks like:\\n\\n```\\nCurrent Version   mykey\\nVersion 1         mykey::v1\\nVersion 2         mykey::v2\\n...\\t              ...\\n```\\n\\nWith this approach, existing applications will always use the current version of the document, since the key is not changed. But this approach creates new documents that will be indexed by existing views.\\n\\nFor example, in the Beer Sample application, the following view is used to list the beer by name:\\n\\n``` js\\nfunction (doc, meta) {\\n  if(doc.type &amp;&amp; doc.type == \\"beer\\") {\\n    emit(doc.name);\\n  }\\n}\\n```\\n\\nIt is quite simple to \\"support\\" versioning without impacting the existing code, except the view itself. The new view needs to emit keys,value only for the current version of the document. This is the new view code:\\n\\n```\\nfunction (doc, meta) {\\n  if(doc.type &amp;&amp; doc.type == \\"beer\\" &amp;&amp; (meta.id).indexOf(\\"::v\\") == -1   ) {\\n    emit(doc.name);\\n  }\\n}\\n```\\n\\nWith this change the existing applications that are using this view will continue to work with the same behavior.\\n\\n### Implementing the versioning\\n\\nBased on this design, when the application needs to version the document, the following logic should happen:\\n\\n1.  Get the current version of the document\\n2.  Increment the version number (for example using another key that maintains the version number for each document)\\n3.  Create the version with the new key  \\"mykey::v1\\"\\n4.  Save the document current version\\n\\nLet\'s look at the code in Java\\n\\n\\n``` java\\nObject obj = client.get(key);\\nif (obj != null) {\\n  // get the next version, create or use the key: mykey_version\\n  long version = client.incr(key + \\"_version\\", 1, 1);\\n  String keyForVersion = key + \\"::v\\" + version; // mykey::v1\\n  try {\\n    client.set(keyForVersion, obj).get();\\n    } catch (Exception e) {\\n      logger.severe(\\"Cannot save version \\"+ version + \\" for key \\"+ key +\\" - Error:\\"+ e.getMessage() );\\n    }\\n  }\\n  client.set(key, value);\\n```\\n\\nQuite simple isn\'t?\\n\\nThe application can access the document using the key, but also get one version or the list of all versions, this is one of the reasons why it is interesting to create a key (`mykey_version`), and use it also to delete documents and related versions.\\n\\nBased on the previous comment, the delete operation looks like:\\n\\n``` java\\nObject obj = client.get(key);\\n// need to delete all the version first\\nObject vObject = this.get(key + \\"_version\\");\\nif (vObject != null) {\\n  long biggerVersion = Long.parseLong((String) vObject);\\n  try {\\n    // delete all the versions\\n    for (int i = 1; i <= biggerVersion; i++) {\\n      String versionKey = key + \\"::v\\" + i;\\n      client.delete(versionKey).get();\\n    }\\n    // delete the counter\\n    client.delete(key + \\"_version\\").get();\\n  } catch (InterruptedException e) {\\n    e.printStackTrace();\\n  } catch (ExecutionException e) {\\n    e.printStackTrace();\\n  }\\n}\\nclient.delete(key);\\n```\\n\\n#### Use versioning\\n\\nAs an example, I have created a small library available on GitHub [https://github.com/tgrall/couchbase-how-to-versioning](https://github.com/tgrall/couchbase-how-to-versioning), this library extends the Couchbase Client and overrides some of the operations : set, replace and delete. (the basic one: no TLL, no durability) As I said before this is just an example.\\n\\n\\n*Build and Install*\\n\\n```\\ngit clone https://github.com/tgrall/couchbase-how-to-versioning.git\\ncd how-to-versioning\\nmvn clean install\\n```\\n\\nThen add this library to your project in addition to Couchbase Java Client, for example in your pom.xml\\n\\n``` xml\\n...\\n<dependency>\\n  <groupid>com.couchbase.howtos</groupid>\\n  <artifactid>couchbase-how-to-versioning</artifactid>\\n  <version>1.0-SNAPSHOT</version>\\n</dependency>\\n<dependency>\\n  <groupid>couchbase</groupid>\\n  <artifactid>couchbase-client</artifactid>\\n  <version>1.1.8</version>\\n</dependency>\\n\\n...\\n```\\n\\n*Code your application*\\n\\nCreate a document and version it:\\n\\n``` java\\nList<uri> uris = new LinkedList<uri>();\\nuris.add(URI.create(\\"http://127.0.0.1:8091/pools\\"));\\nCouchbaseClientWithVersioning client = null\\ntry {\\n  client = new CouchbaseClientWithVersioning(uris, \\"default\\", \\"\\");\\n  String key = \\"key-001\\";\\n  client.set(key, \\"This is the original version\\");\\n  System.out.printf(\\"Original \'%s\' .\\\\n\\", client.get(key));\\n  client.set(key, \\"This is a new version\\", true); // create a new version\\n  System.out.printf(\\"Current Version \'%s\' .\\\\n\\", client.get(key));\\n  System.out.printf(\\"Version 1 \'%s\' .\\\\n\\", client.get(key, 1));\\n  client.set(key, \\"This is another version\\", true); // create a new version\\n  System.out.printf(\\"All versions %s .\\\\n\\", client.getAllVersions(key));\\n  client.deleteVersion(key, 1); // create a new version\\n  System.out.printf(\\"All versions %s (after delete 1 version).\\\\n\\", client.getAllVersions(key));\\n  client.delete(key); // create a new version\\n  System.out.printf(\\"All versions %s (after delete the main key).\\\\n\\", client.getAllVersions(key));\\n} catch (Exception e) {\\n  e.printStackTrace();\\n}\\nif (client !=null) {\\n  client.shutdown();\\n}\\n```\\n\\nQuick explanation:\\n\\n*   Line 5: instead of using the `CouchbaseClient`, the application uses the extended `CouchbaseClientWithVersioning` class.\\n*   Line 7: create a new entry\\n*   Line 9: create a new version, the boolean value to \\"true\\" force the versioning of the document\\n*   The application use other methods such as get a specific version (line 11), get all versions (line 13), delete a specific version (line 14), and finally delete the key and all versions (line 16).\\n\\n\\nSo using this approach the developer controls explicitly when to create a version, since he has to add the boolean parameter in the set operation. In this small sample library it is also possible to do auto versioning, in this case all set and replace calls will create a version, to achieve that the developer just needs to call the setAutoVersioning(true) method. Something like:\\n\\n``` java\\nclient = new CouchbaseClientWithVersioning(uris, \\"default\\", \\"\\");\\nclient.setAutomaticVersionning(true);\\n```\\n\\nWith this approach you can provide versioning to your application with minimal code change. You can test it in the Beer Sample application, just do not forget to change the views as documenter above to only return _current_ version of the documents.\\n\\n### Conclusion\\n\\nAs you can see doing versioning in Couchbase is not that complicated, but it is something that must be done by your application based on its requirements and constraints. You have many different solution and none of these options is perfect for all use cases.\\n\\nIn this specific sample code, I am working with a simple design where I create a copy of the documents for each version. With this approach also, it is interesting to mention that you can version \\"anything\\", not only JSON document but also any values.  As I said before, this is one possible approach, and like any design, it has some impact on the application or database, in this case most the database:\\n\\n*   Increase the number of keys and documents\\n*   Double - or more- the number of operations, for example when updating a document, the application needs to get the current value, create a version, save the current version.\\n*   Consistency management when adding new version and incrementing the version number (need to deal with errors when creating a new version, deleting the versions and counter....)\\n\\nMany features could be added to this easily, for example:\\n\\n*   Limit to a specific number of version,\\n*   Enable the versioning only of replace() operation\\n*   Add specific attribute about versions in JSON document (for example date of the version)\\n*   ....\\n\\nIf you are using versioning in your Couchbase application feel free to comment or write a small article that describes the way your are doing it."},{"id":"/2013/07/11/deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud","metadata":{"permalink":"/blog/2013/07/11/deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-07-11-deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud.md","source":"@site/blog/2013-07-11-deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud.md","title":"Deploy your Node/Couchbase application to the cloud with Clever Cloud","description":"Introduction","date":"2013-07-11T00:00:00.000Z","formattedDate":"July 11, 2013","tags":[],"readingTime":5.285,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Deploy your Node/Couchbase application to the cloud with Clever Cloud","categories":"couchbase nosql nodejs cloud clevercloud"},"prevItem":{"title":"How to implement Document Versioning with Couchbase","permalink":"/blog/2013/07/18/how-to-implement-document-versioning-with-couchbase"},"nextItem":{"title":"SQL to NoSQL : Copy your data from MySQL to Couchbase","permalink":"/blog/2013/07/03/sql-to-nosql-copy-your-data-from-mysql-to-couchbase"}},"content":"### Introduction\\n\\n[Clever Cloud](http://www.clever-cloud.com/en/) is the first PaaS to provide Couchbase as a service allowing developers to run applications in a fully managed environment. This article shows how to deploy an existing application to Clever Cloud.\\n\\n![]( http://f.cl.ly/items/2L2M2k2O000e3g2N1z3z/couchbase_gradient_clever.png )\\n\\nI am using a very simple Node application that I have documented in a previous article: \u201c[Easy application development with Couchbase, Angular and Node](http://tugdualgrall.blogspot.fr/2013/03/easy-application-development-with.html)\u201d.\\n\\nClever Cloud provides support for various databases MySQL, PostgreSQL, but also and this is most important for me [Couchbase](http://www.clever-cloud.com/en/services/couchbase.html). No only Clever Cloud allows you to use database services but also you can deploy and host your application that could be developed in the language/technology of your choice : Java, Node, Scala, Python, PHP, \u2026 and all this in a secure, scalable and managed environment.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Setting up your Clever Cloud environment\\n\\n#### Create your account\\n\\n1.  Go to the Clever Cloud site :[ http://www.clever-cloud.com/](http://www.clever-cloud.com/)\\n2.  Click on \u201cLogin\u201d link and follow the steps to create your account.\\n3.  After few seconds you will received an email and be redirected to the Clever Cloud Console.\\n\\n\\n#### Create a Couchbase instance\\n\\nThe [Clever Cloud Console](https://console.clever-cloud.com/) allows you to create your Couchbase Bucket in few clicks:\\n\\n1-  Cick on \u201cServices\u201d in the left menu\\n\\n2-  Click on \u201cAdd a Service\u201d in the left menu \\n\\n![]( http://4.bp.blogspot.com/-He8scPOrH5I/Uac5B_O2k3I/AAAAAAAAAcE/OZyn8jW-bV8/s320/clever-cloud-add-couchbase.png )\\n\\n3- Click on \u201cCouchbase\u201d button.\\n\\n4. Select the size of the RAM quota for your bucket\\n\\n![]( http://4.bp.blogspot.com/-V_GMolXLClI/Uac5CMdKQoI/AAAAAAAAAcM/vvGKJUXW-xQ/s320/Screen+Shot+2013-05-30+at+9.19.59+AM.png )\\n\\nThe size of the RAM Quota for your bucket will have an impact on performance but also on the pricing.\\n\\n5- Click \u201cAdd this Service\u201d\\n\\nYou are done, you should receive an email with all the information to access your newly created bucket.\\n\\nThe mail from Clever Cloud contains the following information:\\n\\n```\\ndb_host = xxxxxxxx.couchbase.clvrcld.net\\tLocation of the database, this is where the endpoint is located.\\ndb_name = yyyyyyyy\\tName of the Couchbase bucket\\ndb_username = xxxxxxxx\\tNot used in Couchbase context\\ndb_password = zzzzzzzz\\tPassword to connect to the Couchbase Bucket\\n```\\n\\nSo you are now ready to use your bucket.\\n\\nNote: In the current version of the Clever Cloud Couchbase Service you do not have access to a management console. If you want to get some information about the database or create views you need to do it from you application code.\\n\\n\\n\\n#### Connect your Application to Couchbase@Clever-Cloud\\n\\nThe first step is to get some code, so let\u2019s clone the \u201cCouchbase Ideas Sample Application\u201d, and install the dependencies, using the following commands:\\n\\n```\\ngit clone -b 03-vote-with-value https://github.com/tgrall/couchbase-node-ideas.git\\n\\ncd couchbase-node-ideas\\n\\ngit branch mybranch\\n\\ngit checkout mybranch\\n\\nnpm install\\n```\\n\\nOpen the app.js and edit the connection info to point your application to the Couchbase instance and modify the HTTP port of your application to 8080 - this is a mandatory step documented [here](http://doc.clever-cloud.com/nodejs/nodejs/#requirements) :\\n\\n``` js\\ndbConfiguration = {\\n  \\"hosts\\": [\\"xxxxxxxxxxx.couchbase.clvrcld.net:8091\\"],\\n  \\"bucket\\": \\"xxxxxxxxxxx\\",\\n  \\"user\\": \\"xxxxxxxxxx\\",\\n  \\"password\\": \\"yyyyyyyyyyyyyyyyyyyyyyyyy\\"\\n};\\n...\\n...\\n\\nappServer = app.listen(8080, function() {\\n  console.log(\\"Express server listening on port %d in %s mode\\", appServer.address().port, app.settings.env);\\n});\\n```\\n\\nLaunch your application using\\n\\n```\\nnode app.js\\n```\\n\\nGo to http://localhost:8080\\n\\nYour application is now using Couchbase on the cloud powered by Clever Cloud. Let\u2019s now deploy the application itself on Clever Cloud\\n\\n### Deploy your application on Clever Cloud\\n\\nThe easiest way to deploy an application to Clever Cloud is using git. The first thing to do is to add your SSH public key into Clever Cloud Console. If you do not have any SSH yet, follow the steps described on Github : \u201c[Generating SSH Keys](https://help.github.com/articles/generating-ssh-keys)\u201d.\\n\\n#### Add your SSH key\\n\\nNote: As you can guess this should be done only once\\n\\nOpen the id_rsa.pub file with a text editor. This is your SSH key. Select all and copy to your clipboard.\\n\\n1.  Go to the Clever Cloud Console\\n2.  Click on \u201cProfile\u201d entry in the left menu\\n3.  Click on \u201cSSH Keys\u201d\\n4.  Click on \u201cAdd a SSH Key\u201d\\n5.  Enter a name (anything you want) and paste your key\\n6.  Click \u201cAdd\u201d button\\n\\nYou are now ready to deploy applications to Clever Cloud. The next thing to do, is to create a new node application in Clever Cloud.\\n\\n#### Create your Application\\n\\n1.  Click \u201cAdd an app\u201d in the Application menu in the top menu.\\n2.  Give a name and description to this application\\n3.  Select the Instance type, in this case \u201cNode.js\u201d\\n4.  Configure your instances, you can keep the default values for now, click \u201cNext\u201d\\n5.  Check the configuration, and click \u201cCreate\u201d\\n\\nYour application is created, you are redirected to the generic information page, where you can find a Git URL that we will use to deploy the application.\\n\\nYou can navigate into the entries in the left menu to see more information about your application. In addition to the Information page, you can look at the following entries:\\n\\n1.  \u201cDomain Names\u201d to configure the URL to access your application\\n2.  \u201cLogs\u201d to view the application logs\\n\\n#### Deploy the Application\\n\\nSo we are almost there!\\n\\nThe deployment to Clever Cloud is done using a Git push command, so you need to add the deployment URL as a remote repository to your application, using the following command:\\n\\n```\\ngit remote add clever git+ssh://git@push.clever-cloud.com/app_[your_app_id].git\\n\\ngit commit -a -m \u201cCouchbase on Clever Cloud connection\u201d\\n\\ngit push clever mybranch:master\\n```\\n\\nOnce you have added the application as remote repository you can commit and push your application.\\n\\nThe last command pushes the application  to Clever Cloud. It is important to note that Clever Cloud will always deploy the application on the \u201cmaster\u201d branch on the remote repository. The notation mybranch:master is used to mention it. If you work locally on your master branch just use \u201cmaster\u201d.\\n\\nYou can now go to the Clever Cloud console and look in the log and click on the URL in the \u201cDomain Names\u201d section to test your application.\\n\\nYou should be able to see your application, that is running on the Clever Cloud PaaS.\\n\\nWhen you update your application, you just need to do a  git push and git commit.\\n\\n### Conclusion\\n\\nIn this tutorial you have learned how to:\\n\\n*   Create your Clever Cloud account\\n*   Create a Couchbase instance\\n*   Create and deploye a Node.js application\\n\\nFeel free to test this yourself, with Node or other technology, as you can see it is quite easy to setup."},{"id":"/2013/07/03/sql-to-nosql-copy-your-data-from-mysql-to-couchbase","metadata":{"permalink":"/blog/2013/07/03/sql-to-nosql-copy-your-data-from-mysql-to-couchbase","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-07-03-sql-to-nosql-copy-your-data-from-mysql-to-couchbase.md","source":"@site/blog/2013-07-03-sql-to-nosql-copy-your-data-from-mysql-to-couchbase.md","title":"SQL to NoSQL : Copy your data from MySQL to Couchbase","description":"TL;DR: Look at the project on Github.","date":"2013-07-03T00:00:00.000Z","formattedDate":"July 3, 2013","tags":[],"readingTime":5.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"SQL to NoSQL : Copy your data from MySQL to Couchbase","categories":"couchbase nosql sql"},"prevItem":{"title":"Deploy your Node/Couchbase application to the cloud with Clever Cloud","permalink":"/blog/2013/07/11/deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud"},"nextItem":{"title":"Create a Couchbase cluster in less than a minute with Ansible","permalink":"/blog/2013/05/31/create-a-couchbase-cluster-in-less-than-a-minute-with-ansible"}},"content":"**TL;DR:** Look at the [project on Github](https://github.com/tgrall/couchbase-sql-importer).\\n\\n### Introduction\\n\\nDuring my last interactions with the Couchbase community, I had the question how can I easily import my data from my current database into Couchbase. And my answer was always the same:\\n\\n*   Take an ETL such as Talend to do it\\n*   Just write a small program to copy the data from your RDBMS to Couchbase...\\n\\nSo I have written this small program that allows you to import the content of a RDBMS into Couchbase. This tools could be used as it is, or you can look at the code to adapt it to your application.\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/xzqBjhYKCLY\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n\\n### The Tool: Couchbase SQL Importer\\n\\nThe Couchbase SQL Importer, available [here](http://github.com/tgrall/couchbase-sql-importer), allows you with a simple command line to copy all -or part of- your SQL schema into Couchbase. Before explaining how to run this command, let\'s see how the data are stored into Couchbase when they are imported:\\n\\n\\n*   Each table row is imported a single JSON document\\n    *   where each table column becomes a JSON attribute\\n*   Each document as a key made of the name of the table and a counter (increment)\\n\\n\\nThe following concrete example, based on the [MySQL World sample database](http://dev.mysql.com/doc/world-setup/en/index.html), will help you to understand how it works. This database contains 3 tables : City, Country, CountryLanguage. The City table looks like:\\n\\n```\\n+-------------+----------+------+-----+---------+----------------+\\n| Field       | Type     | Null | Key | Default | Extra          |\\n+-------------+----------+------+-----+---------+----------------+\\n| ID          | int(11)  | NO   | PRI | NULL    | auto_increment |\\n| Name        | char(35) | NO   |     |         |                |\\n| CountryCode | char(3)  | NO   |     |         |                |\\n| District    | char(20) | NO   |     |         |                |\\n| Population  | int(11)  | NO   |     | 0       |                |\\n+-------------+----------+------+-----+---------+----------------+\\n```\\n\\nThe JSON document that matches this table looks like the following:\\n\\n``` json\\ncity:3805\\n{\\n  \\"Name\\": \\"San Francisco\\",\\n  \\"District\\": \\"California\\",\\n  \\"ID\\": 3805,\\n  \\"Population\\": 776733,\\n  \\"CountryCode\\": \\"USA\\"\\n}\\n```\\n\\nYou see that here I am simply taking all the rows and \\"moving\\" them into Couchbase. This is a good first step to play with your dataset into Couchbase, but it is probably not the final model you want to use for your application; most of the time you will have to see when to use embedded documents, list of values, .. into your JSON documents.\\n\\nIn addition to the JSON document the tool create views based on the following logic:\\n\\n*   a view that list all imported documents with the name of the \\"table\\" (aka type) as key\\n*   a view for each table with the primary key columns\\n\\n\\nView: all/by_type\\n\\n``` json\\n{\\n  \\"rows\\": [\\n  {\\"key\\": \\"city\\", \\"value\\": 4079},\\n  {\\"key\\": \\"country\\", \\"value\\": 239},\\n  {\\"key\\": \\"countrylanguage\\", \\"value\\": 984}\\n  ]\\n}\\n```\\n\\nAs you can see this view allows you to get with a single Couchbase query the number of document by type.\\n\\nAlso for each table/document type, a view is created where the key of the index is built from the table primary key. Let\'s for example query the \\"City\\" documents.\\n\\n\\n\\nView: city/by_pk?reduce=false&amp;limit=5\\n\\n``` json\\n{\\n  \\"total_rows\\": 4079,\\n  \\"rows\\": [\\n  {\\"id\\": \\"city:1\\", \\"key\\": 1, \\"value\\": null},\\n  {\\"id\\": \\"city:2\\", \\"key\\": 2, \\"value\\": null},\\n  {\\"id\\": \\"city:3\\", \\"key\\": 3, \\"value\\": null},\\n  {\\"id\\": \\"city:4\\", \\"key\\": 4, \\"value\\": null},\\n  {\\"id\\": \\"city:5\\", \\"key\\": 5, \\"value\\": null}\\n  ]\\n}\\n```\\n\\nThe index key matches the value of the `City.ID` column.  When the primary key is made of multiple columns the key looks like:\\n\\nView: CountryLanguage/by_pk?reduce=false&amp;limit=5\\n``` json\\n{\\n  \\"total_rows\\": 984,\\n  \\"rows\\": [\\n  {\\"id\\": \\"countrylanguage:1\\", \\"key\\": [\\"ABW\\", \\"Dutch\\"], \\"value\\": null},\\n  {\\"id\\": \\"countrylanguage:2\\", \\"key\\": [\\"ABW\\", \\"English\\"], \\"value\\": null},\\n  {\\"id\\": \\"countrylanguage:3\\", \\"key\\": [\\"ABW\\", \\"Papiamento\\"], \\"value\\": null},\\n  {\\"id\\": \\"countrylanguage:4\\", \\"key\\": [\\"ABW\\", \\"Spanish\\"], \\"value\\": null},\\n  {\\"id\\": \\"countrylanguage:5\\", \\"key\\": [\\"AFG\\", \\"Balochi\\"], \\"value\\": null}\\n  ]\\n}\\n```\\n\\n\\nThis view is built from the CountryLanguage table primary key made of `CountryLanguage.CountryCode and  `CountryLanguage.Language` columns.\\n\\n```\\n+-------------+---------------+------+-----+---------+-------+\\n| Field       | Type          | Null | Key | Default | Extra |\\n+-------------+---------------+------+-----+---------+-------+\\n| CountryCode | char(3)       | NO   | PRI |         |       |\\n| Language    | char(30)      | NO   | PRI |         |       |\\n| IsOfficial  | enum(\'T\',\'F\') | NO   |     | F       |       |\\n| Percentage  | float(4,1)    | NO   |     | 0.0     |       |\\n+-------------+---------------+------+-----+---------+-------+\\n```\\n\\n\\n**How to use Couchbase SQL Importer tool? **\\n\\nThe importer is a simple Java based command line utility, quite simple to use:\\n\\n\\n1- Download the [CouchbaseSqlImporter.jar file from here](http://goo.gl/IF89e). This file is contains all the dependencies to work with Couchbase: the Java Couchbase Client, and GSON.\\n2- Download the JDBC driver for the database you are using as data source. For this example I am using MySQL and I have download the driver for MySQL Site.\\n3- Configure the import using a properties file.\\n\\n```\\n## SQL Information ##\\nsql.connection=jdbc:mysql://192.168.99.19:3306/world\\nsql.username=root\\nsql.password=password\\n\\n## Couchbase Information ##\\ncb.uris=http://localhost:8091/pools\\ncb.bucket=default\\ncb.password=\\n\\n## Import information\\nimport.tables=ALL\\nimport.createViews=true\\nimport.typefield=type\\nimport.fieldcase=lower\\n```\\n\\n\\nThis sample properties file contains three sections :\\n\\n*   The two first sections are used to configure the connections to your SQL database and Couchbase cluster (note that the bucket must be created first)\\n*   The third section allow you to configure the import itself\\n\\n\\n4- Run the tool !\\n\\n```\\njava -cp \\"./CouchbaseSqlImporter.jar:./mysql-connector-java-5.1.25-bin.jar\\" com.couchbase.util.SqlImporter import.properties\\n```\\nSo you run the Java command with the proper classpath (-cp parameter).\\n\\nAnd you are done, you can get your data from your SQL database into Couchbase.\\n\\nIf you are interested to see how it is working internally, you can take a look to the next paragraph.\\n\\n\\n### The Code: How it works?\\n\\nThe main class of the tool is really simple  [com.couchbase.util.SqlImporter](https://github.com/tgrall/couchbase-sql-importer/blob/master/sql-importer-lib/src/main/java/com/couchbase/util/SqlImporter.java), the process is:\\n\\n1. Connect to the SQL database\\n2. Connect to Couchbase\\n3. Get the list of tables\\n4. For each tables execute a \\"select * from table\\"\\n    4.1. Analyze the ResultSetMetadata to get the list of columns\\n    4.2. Create a Java map for each rows where the key is the name of the columns and the value\u2026is the value\\n    4.3. Serialize this Map into a GSON document and save it into Couchbase\\n\\nThe code is available in the [ImportTable(String table)](https://github.com/tgrall/couchbase-sql-importer/blob/master/sql-importer-lib/src/main/java/com/couchbase/util/SqlImporter.java#L212) Java method.\\n\\nOne interesting point is that you can use and extend the code to deal with your application.\\n\\n\\n### Conclusion\\n\\nI have created this tool quickly to help some people in the community, if you are using it and need new features, let me know, using comment or pull request."},{"id":"/2013/05/31/create-a-couchbase-cluster-in-less-than-a-minute-with-ansible","metadata":{"permalink":"/blog/2013/05/31/create-a-couchbase-cluster-in-less-than-a-minute-with-ansible","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-05-31-create-a-couchbase-cluster-in-less-than-a-minute-with-ansible.md","source":"@site/blog/2013-05-31-create-a-couchbase-cluster-in-less-than-a-minute-with-ansible.md","title":"Create a Couchbase cluster in less than a minute with Ansible","description":"TL;DR: Look at the Couchbase Ansible Playbook on my Github.","date":"2013-05-31T00:00:00.000Z","formattedDate":"May 31, 2013","tags":[],"readingTime":5.315,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Create a Couchbase cluster in less than a minute with Ansible","categories":"couchbase nosql devops"},"prevItem":{"title":"SQL to NoSQL : Copy your data from MySQL to Couchbase","permalink":"/blog/2013/07/03/sql-to-nosql-copy-your-data-from-mysql-to-couchbase"},"nextItem":{"title":"Six months as Technical Evangelist at Couchbase","permalink":"/blog/2013/05/28/six-months-as-technical-evangelist-at-couchbase"}},"content":"**TL;DR:** Look at the Couchbase Ansible Playbook on my [Github](https://github.com/tgrall/couchbase-ansible-playbook).\\n\\n\\n### Introduction  \\n\\nWhen I was looking for a more effective way to create my cluster I asked some sysadmins which tools I should use to do it. The answer I got during [OSDC](http://www.netways.de/osdc) was not [Puppet](https://puppetlabs.com/), nor [Chef](http://www.opscode.com/chef/), but was [Ansible](http://ansible.cc/).\\n\\nThis article shows you how you can easily configure and create a Couchbase cluster deployed and many linux boxes...and the only thing you need on these boxes is an SSH Server!\\n\\nThanks to [Jan-Piet Mens](http://jpmens.net/) that was one of the person that convinced me to use Ansible and answered questions I had about Ansible.\\n\\nYou can watch the demonstration below, and/or look at all the details in the next paragraph.\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/COb6y89xcYY\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n### Ansible\\n\\n#### Ansible is an open-source software that allows administrator to configure and manage many computers over SSH.\\n\\n\\nI won\'t go in all the details about the installation, just follow the steps documented in the [Getting Started Guide](http://ansible.cc/docs/gettingstarted.html). As you can see from this guide, you just need Python and few other libraries and clone Ansible project from Github. So I am expecting that you have Ansible working with your various servers on which you want to deploy Couchbase.\\n\\nAlso for this first scripts I am using **root** on my server to do all the operations. So be sure you have register the root ssh keys to your administration server, from where you are running the Ansible scripts.\\n\\n\\n### Create a Couchbase Cluster\\n\\nSo before going into the details of the Ansible script it is interesting to explain how you create a Couchbase Cluster. So here are the 5 steps to create and configure a cluster:\\n\\n1.  Install Couchbase on each nodes of the cluster, as documented [here](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-getting-started-install-ubuntu.html).\\n2.  Take one of the node and \\"initialize\\" the cluster,  using [cluster-init command](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-cli-other-examples.html).\\n3.  Add the other nodes to the cluster, using [server-add command](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-cli-other-examples.html).\\n4.  Rebalance, using [rebalance command](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-cli-other-examples.html).\\n5.  Create a Bucket, using [bucket-create command](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-cli-other-examples.html).\\n\\nSo the goal now is to create an Ansible Playbook that does these steps for you.\\n\\n\\n\\n\\n#### Ansible Playbook for Couchbase\\n\\n\\nThe first think you need is to have the list of hosts you want to target, so I have create a [hosts file](https://github.com/tgrall/couchbase-ansible-playbook/blob/master/hosts) that contains all my server organized in 2 groups:\\n\\n```\\n[couchbase-main]\\nvm1.grallandco.com\\n\\n[couchbase-nodes]\\nvm2.grallandco.com\\nvm3.grallandco.com\\n```\\n\\n\\nThe group [couchbase-main] group is just one of the node that will drive the installation and configuration, as you probably already know, Couchbase does not have any master... All nodes in the cluster are identical.\\n\\nTo ease the configuration of the cluster, I have create another file that contains all parameters that must be sent to all the various commands. This file is located in the [group_vars/all](https://github.com/tgrall/couchbase-ansible-playbook/blob/master/group_vars/all) see the section [Splitting Out Host and Group Specific Data](http://ansible.cc/docs/patterns.html#splitting-out-host-and-group-specific-data) in the documentation.\\n\\n```\\n# Adminisrator user and password\\nadmin_user: Administrator\\nadmin_password: password\\n\\n# ram quota for the cluster\\ncluster_ram_quota: 1024\\n\\n# bucket and replicas\\nbucket_name: ansible\\nbucket_ram_quota: 512\\nnum_replicas: 2\\n```\\n\\nUse this file to configure your cluster.\\n\\n\\n\\n\\nLet\'s describe the [playbook file](https://github.com/tgrall/couchbase-ansible-playbook/blob/master/couchbase.yml) :\\n\\n```\\n- name: Couchbase Installation\\nhosts: all\\nuser: root\\n\\ntasks:\\n\\n- name: download Couchbase package\\nget_url: url=http://packages.couchbase.com/releases/2.0.1/couchbase-server-enterprise_x86_64_2.0.1.deb dest=~/.\\n\\n- name: Install dependencies\\napt: pkg=libssl0.9.8 state=present\\n\\n- name: Install Couchbase .deb file on all machines\\nshell: dpkg -i ~/couchbase-server-enterprise_x86_64_2.0.1.deb\\n```\\n\\nAs expected, the installation has to be done on **all** servers as **root** then we need to execute 3 tasks:\\n\\n1.  Download the product, the get_url command will only download the file if not already present\\n2.  Install the dependencies with the apt command, the state=present allows the system to only install this package if not already present\\n3.  Install Couchbase with a simple shell command. (here I am not checking if Couchbase is already installed)\\n\\nSo we have now installed Couchbase on all the nodes. Let\'s now configure the first node and add the others:\\n\\n```\\n- name: Initialize the cluster and add the nodes to the cluster\\nhosts: couchbase-main\\nuser: root\\n\\ntasks:\\n- name: Configure main node\\nshell: /opt/couchbase/bin/couchbase-cli cluster-init -c 127.0.0.1:8091  --cluster-init-username=${admin_user} --cluster-init-password=${admin_password} --cluster-init-port=8091 --cluster-init-ramsize=${cluster_ram_quota}\\n\\n- name: Create shell script for configuring main node\\naction: template src=couchbase-add-node.j2 dest=/tmp/addnodes.sh mode=750\\n\\n- name: Launch config script\\naction: shell /tmp/addnodes.sh\\n\\n- name: Rebalance the cluster\\nshell: /opt/couchbase/bin/couchbase-cli rebalance -c 127.0.0.1:8091 -u ${admin_user} -p ${admin_password}\\n\\n- name: create bucket ${bucket_name} with ${num_replicas} replicas\\nshell: /opt/couchbase/bin/couchbase-cli bucket-create -c 127.0.0.1:8091 --bucket=${bucket_name} --bucket-type=couchbase --bucket-port=11211 --bucket-ramsize=${bucket_ram_quota}  --bucket-replica=${num_replicas} -u ${admin_user} -p ${admin_password}\\n```\\n\\nNow we need to execute specific taks on the \\"main\\" server:\\n\\n* Initialization of the cluster using the Couchbase CLI, on line 06 and 07\\n\\nThen the system needs to ask all other server to join the cluster. For this the system needs to get the various IP and for each IP address execute the add-server command with the IP address. As far as I know it is not possible to get the IP address from the main playbook YAML file, so I ask the system to generate a shell script to add each node and execute the script.\\n\\nThis is done from the line 09 to 13.\\n\\n\\n\\n\\n\\nTo generate the shell script, I use [Ansible Template](http://ansible.cc/docs/modules.html#template), the template is available in the [couchbase-add-node.j2](https://github.com/tgrall/couchbase-ansible-playbook/blob/master/couchbase-add-node.j2) file.\\n\\n\\n```sh\\n{% for host in groups[\'couchbase-nodes\'] %}\\n/opt/couchbase/bin/couchbase-cli server-add -c 127.0.0.1:8091 -u ${admin_user} -p ${admin_password} --server-add={{ hostvars[host][\'ansible_eth0\'][\'ipv4\'][\'address\'] }}:8091 --server-add-username=${admin_user} --server-add-password=${admin_password}\\n{% endfor %}\\n```\\n\\nAs you can see this script loop on each server in the [couchbase-nodes] group and use its IP address to add the node to the cluster.\\n\\nFinally the script rebalance the cluster (line 16) and add a new bucket (line 19).\\n\\nYou are now ready to execute the playbook using the following command :\\n\\n```\\n./bin/ansible-playbook -i ./couchbase/hosts ./couchbase/couchbase.yml -vv\\n```\\n\\nI am adding the -vv parameter to allow you to see more information about what\'s happening during the execution of the script.\\n\\nThis will execute all the commands described in the playbook, and after few seconds you will have a new cluster ready to be used! You can for example open a browser and go to the Couchase Administration Console and check that your cluster is configured as expected.\\n\\n ![]( http://1.bp.blogspot.com/-L-3yeJZECwY/Uaj_PA_aVUI/AAAAAAAAAcg/fKBZ47Nhd4M/s320/Screen+Shot+2013-05-31+at+9.50.44+PM.png )\\n\\nAs you can see it is really easy and fast to create a new cluster using Ansible.\\n\\nI have also create a script to uninstall properly the cluster.. just launch\\n\\n```\\n./bin/ansible-playbook -i ./couchbase/hosts ./couchbase/couchbase-uninstall.yml\\n```"},{"id":"/2013/05/28/six-months-as-technical-evangelist-at-couchbase","metadata":{"permalink":"/blog/2013/05/28/six-months-as-technical-evangelist-at-couchbase","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-05-28-six-months-as-technical-evangelist-at-couchbase.md","source":"@site/blog/2013-05-28-six-months-as-technical-evangelist-at-couchbase.md","title":"Six months as Technical Evangelist at Couchbase","description":"Already 6 months! Already 6 months that I have joined Couchbase as Technical Evangelist. This is a good opportunity to take some time to look back.","date":"2013-05-28T00:00:00.000Z","formattedDate":"May 28, 2013","tags":[],"readingTime":5.435,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Six months as Technical Evangelist at Couchbase","categories":"couchbase nosql"},"prevItem":{"title":"Create a Couchbase cluster in less than a minute with Ansible","permalink":"/blog/2013/05/31/create-a-couchbase-cluster-in-less-than-a-minute-with-ansible"},"nextItem":{"title":"Screencast : Fun with Couchbase MapReduce and Twitter","permalink":"/blog/2013/04/29/screencast-fun-with-couchbase-mapreduce-and-twitter"}},"content":"Already 6 months! Already 6 months that I have joined Couchbase as Technical Evangelist. This is a good opportunity to take some time to look back.\\n\\n\\nSo first of all what is a Developer/Technical Evangelist?\\n\\nHmm it depends of each company/product, but let me tell you what it is for me, inside Couchbase. This is one of the most exciting job I ever had. And I think it is the best job you can have when you are passionate about technology, and you like to share this passion with others. So my role as Technical Evangelist is to help the developers to adopt NoSQL technologies in general, and as you can guess Couchbase in particular.\\n\\n\\nLet\'s now see in more details what I have done during these past six months and why I am so happy about it. I have organized the different activities in three types:\\n\\n* Outbound activities : meet the developers\\n* Online activities : reach even more developers\\n* Inbound Activities : make the product better !\\n\\n\\n### Outbound activities : meet the developers !\\n\\nA large part of my activities for this first semester was made of conferences and meetups. All these events are great opportunities for me to talk about NoSQL and get more people to use Couchbase Server 2.0, here a short list of what I have done:\\n\\n* participated to many Couchbase Developer Days in various cities (Portland, Seattle, Vancouver, Oslo, Copenhagen, Stockholm, Munich, Amsterdam, Barcelona, Paris, ...), these are one day workshops where I am helping developers to get their hands dirty on Couchbase\\n* participated to Couchconf Berlin and Couchbase [UK] our main European events where I met many Customer and key members of the community\\n* submitted talks to conferences and adapt them to the conference, then spoken in various conferences about NoSQL and Couchbase ([33Degree Warsaw,](http://33degree.org/)  [NoSQL &amp; Big Data Israel](http://www.johnbryce.co.il/events/nosql-big-data), [Devoxx France](http://devoxx.fr/), [NoSQL Matters](http://nosql-matters.org/), and many others).\\n* met many developers during user groups and meetups. I have to say that I have been very active there, and quite happy to see that NoSQL is a very hot topic for developers, and this in all languages.\\n* delivered [BrowBagLunch](http://en.wikipedia.org/wiki/Brown_bag_seminar)es to various technical teams in companies.\\n\\nYes! Be a Technical Evangelist means, at least for me, be on the road. It is very nice to meet developers from various countries, different cultures, languages, and\u2026 this also means tasting many different types of food!\\n\\n\\nAnother interesting thing when you work on a database/infrastructure layer is the fact that it is *technology agnostic*; you can access Couchbase with multiple programming languages: Java, .Net,Javascript/Node, Ruby, PHP, Python, C, \u2026 and even Go. So with this job I met developers with different backgrounds and views about application development. So yes when I am at a conference or meetup, I am suppose to \\"teach\\" something to people, but I have also learned a lot of things, and still doing it.\\n\\n\\n### Online activities : reach even more developers!\\n\\n\\nMeeting developers during conferences is great but it, it is also very important to produce content to reach even more people, so I have :\\n\\n* written blog post about Couchbase usage, most of them based on feedback/questions from the community\\n* created sample code to show how it works\\n* monitored and answered questions on various sites and mailing lists, from Couchbase discussion forums, mailing lists, Stack Overflow, Quora and others...\\n\\nThis task is quite interesting because it is the moment where you can reach many developers and also get feedback from users, and understand how they are using the product. I have to say that I was not as productive as I was expected, mainly because I was traveling a lot during this period.\\n\\n\\nAnother important thing about online activities, is the \\"Couchbase Community\\" itself, many users of Couchbase are creating content : blog posts, samples, new applications, or features - for example I am talking with a person that is developing a [Dart Client for Couchbase](http://blog.rikulo.org/posts/2013/May/General/couchclient/), so as Technical Evangelist I am also working closely with the most active contributor.\\n\\n\\n### Inbound Activities : make the product better !\\n\\nSo the ultimate goal of a Technical Evangelist at Couchbase is to \\"convert\\" developers to NoSQL/Couchbase and get them to talk about Couchbase. Meeting them online or during events is a way of achieving this; but it is also great to do it directly _with_ the product. This means participating to the \\"development\\" of the product or its ecosystem. Here some of the things that I have done on this topic:\\n\\n* talked a lot with the development team, core developers, product managers, architects, \u2026 Quite exciting to work with so much smart people and have access to them. During this discussions I was able to comment the roadmap, influence features, but also it is all the time an opportunity to learn new things about Couchbase - and many other things around architecture, programming languages, take a look for example to [this nice post from Damien Katz](http://damienkatz.net/2013/01/the_unreasonable_effectiveness_of_c.html) .\\n* contributed some code, yes remember Couchbase is an open source project and it is quite easy to participate to the development. Obviously based on my skills I have only help a little bit with the Java and the Javascript SDK. So if like me you are interested to contribute to the project, take a look to this page: \\"[Contributing Changes](http://www.couchbase.com/wiki/display/couchbase/Contributing+Changes)\\"\\n* but the biggest contributions to the products are such like doc reviews, testing and writing bug reports, and this is very important and interesting, since once again it helps a lot with the product adoption by the developers.\\n\\n### So what?\\n\\nAs you can see the Technical Evangelist job is a quite exciting job, and one of the reason I really love it, it is simply because it allows me to do many different things, that are all related to the technology. Six months is still a very short period, I still have many things to learn and to with the team to be successful, such as be more present online (blog, sample code, technical article, screencast, ..), be accepted in more conferences, and code a little more (I have to finish for example the Couchbase Data Provider for Hibernate OGM, and many other ideas around application development experience)\\n\\nFinally, Couchbase needs you ! This is a good opportunity to say that Couchbase is always looking for talents, especially in the Technical/Developer Evangelist team, so do not hesitate to look at [the different job openings](http://www.couchbase.com/careers) and join the team !"},{"id":"/2013/04/29/screencast-fun-with-couchbase-mapreduce-and-twitter","metadata":{"permalink":"/blog/2013/04/29/screencast-fun-with-couchbase-mapreduce-and-twitter","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-04-29-screencast-fun-with-couchbase-mapreduce-and-twitter.md","source":"@site/blog/2013-04-29-screencast-fun-with-couchbase-mapreduce-and-twitter.md","title":"Screencast : Fun with Couchbase MapReduce and Twitter","description":"I have created this simple screencast to show how you can, using Couchbase do some realtime analysis based on Twitter feed.","date":"2013-04-29T00:00:00.000Z","formattedDate":"April 29, 2013","tags":[],"readingTime":0.46,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Screencast : Fun with Couchbase MapReduce and Twitter","categories":"couchbase twitter nosql"},"prevItem":{"title":"Six months as Technical Evangelist at Couchbase","permalink":"/blog/2013/05/28/six-months-as-technical-evangelist-at-couchbase"},"nextItem":{"title":"Easy application development with Couchbase, Angular and Node","permalink":"/blog/2013/03/06/easy-application-development-with-couchbase-angular-and-node"}},"content":"import Gist from \'react-gist\';\\n\\nI have created this simple screencast to show how you can, using Couchbase do some realtime analysis based on Twitter feed.\\n\\nThe key steps of this demonstration are\\n\\n1.  Inject Tweets using a simple program available on my Github [Couchbase-Twitter-Injector](https://github.com/tgrall/couchbase-twitter-injector)\\n2.  Create views to index and query the Tweets by\\n    * User name\\n    * Tags\\n    * Date\\n\\nThe views that I used in this demonstration are available at the bottom of this post.\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/X167R0TV5QE\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\nViews:\\n\\n<Gist id=\\"1df10b10c9dd387995cb\\" />"},{"id":"/2013/03/06/easy-application-development-with-couchbase-angular-and-node","metadata":{"permalink":"/blog/2013/03/06/easy-application-development-with-couchbase-angular-and-node","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-03-06-easy-application-development-with-couchbase-angular-and-node.md","source":"@site/blog/2013-03-06-easy-application-development-with-couchbase-angular-and-node.md","title":"Easy application development with Couchbase, Angular and Node","description":"Note : This article has been written in March 2013, since Couchbase and its drivers have a changed a lot. I am not working with/for Couchbase anymore, with no time to update the code.","date":"2013-03-06T00:00:00.000Z","formattedDate":"March 6, 2013","tags":[],"readingTime":13.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Easy application development with Couchbase, Angular and Node","categories":"couchbase node nosql angular"},"prevItem":{"title":"Screencast : Fun with Couchbase MapReduce and Twitter","permalink":"/blog/2013/04/29/screencast-fun-with-couchbase-mapreduce-and-twitter"},"nextItem":{"title":"How to get the latest document by date/time field?","permalink":"/blog/2013/02/18/how-to-get-the-latest-document-by-date-slash-time-field"}},"content":"import Gist from \'react-gist\';\\n\\n\\n> Note : This article has been written in March 2013, since Couchbase and its drivers have a changed a lot. I am not working with/for Couchbase anymore, with no time to update the code.\\n\\nA friend of mine wants to build a simple system to capture ideas, and votes. Even if you can find many online services to do that, I think it is a good opportunity to show how easy it is to develop new application using a Couchbase and Node.js.\\n\\nSo how to start?\\n\\nSome of us will start with the UI, other with the data, in this example I am starting with the model. The basics steps are :\\n\\n1.  Model your documents\\n2.  Create Views\\n3.  Create Services\\n4.  Create the UI\\n5.  Improve your application by iteration\\n\\nThe sources of this sample application are available in Gihub :\\n\\n[https://github.com/tgrall/couchbase-node-ideas](https://github.com/tgrall/couchbase-node-ideas)\\n\\nUse the following command to clone the project locally :\\n\\n```\\ngit clone https://github.com/tgrall/couchbase-node-ideas.git\\n```\\n\\n*Note:* my goal is not to provide a complete application, but to describe the key steps to develop an application.\\n\\n\x3c!--more--\x3e\\n\\n### Model your documents\\n\\nFor this application you need 3 types of document :\\n\\n* Ideas : describes the idea with a author, title and description\\n* Vote : the author and a comment - note that it is a choice to not put a value for the vote, in this first version if the vote exists this means user like the idea.\\n* User : contains all the information about the user (not used in this first version of the application)\\n\\nYou can argue that it is possible to put the votes as a list of element inside the idea document. In this case I prefer to use different document and reference the idea in the vote since we do not know how many votes/comments will have. Using different documents is also interesting in this case for the following reasons :\\n\\n* No \\"concurrent\\" access, when a user wants to vote he does not change the idea document itself, so no need to put an optimistic locking in place.\\n* The size of the document will be smaller and easier to cache in memory.\\n\\nSo documents will look like:\\n\\n<Gist id=\\"79f57b13e7a637c7e62e\\" />\\n\\nWhat I really like is the fact that I can quickly create a small dataset to validate that it is correct and help me to design the view. The way I do it, I start my server, launch the Couchbase Administration Console, create a bucket, and finally insert document manually and validate the model and views.\\n\\n### Create Views\\n\\nNow that I have created some documents, I can think about the way I want to get the information out of the database. For this application I need:\\n\\n* The list of ideas\\n* The votes by ideas\\n\\nThe list of idea for this first version is very simple, we just need to emit the title:\\n\\n<Gist id=\\"989a5450811dec8f305e\\" />\\n\\nFor the votes by ideas, I choose to create a collated view, this will give me some interesting options when I will expose them into an API/View layer. I am also for this view using `sum()` reduce function to be sure I capture the number of votes.\\n\\n<Gist id=\\"5fe945cbc12cc59e9dbd\\" />\\n\\nI have my documents, I have some views that allow me to retrieve the list of ideas, the number of vote by idea and count the vote... So I am ready to expose all these informations to the application using a simple API layer.\\n\\n### Create Services\\n\\nLately I have been playing a lot with Node.js, just because it is nice to learn new stuff and also because it is really easy to use with Couchbase. Think about it Couchbase loves JSON, and Node.js object format is JSON, this means I do not have any marshaling/unmarshaling to do.\\n\\nMy API layer is quite simple, I just need to create a set of REST endpoint to deal with:\\n\\n* CRUD operation on each type of document\\n* List the different Documents\\n\\nThe code of the services is available in branch [01-simple-services](https://github.com/tgrall/couchbase-node-ideas/tree/01-simple-services):\\n\\nYou can run the application &nbsp;with simple services using the following command:\\n\\n```\\n> git checkout -f 01-simple-services\\n> node app.js\\n```\\n\\nand go to you browser using the http://127.0.0.1:3000\\n\\n*About the project*\\n\\n\\nFor this project I am using only 2 node modules [Express](http://expressjs.com/) and [Couchbase](https://github.com/couchbase/couchnode). The package.json file looks like :\\n\\n``` json\\n{\\n  \\"name\\": \\"couchbase-ideas-management\\",\\n  \\"version\\": \\"0.0.1\\",\\n  \\"private\\": true,\\n  \\"dependencies\\":\\n  {\\n    \\"express\\": \\"3.x\\",\\n    \\"couchbase\\": \\"0.0.11\\"\\n  }\\n}\\n```\\n\\n\\nAfter running the install, let\'s code the new API interface, as said before I am using an iterative approach so for now I am not dealing with the security, I just want to get the basic actions to work.\\n\\nI am starting with the endpoints to get and set the documents. I am creating a generic endpoints that take the type as URI parameter allowing user/application to do a get/post on `/api/vote`, `/api/idea`. The following code captures this:\\n\\n<Gist id=\\"42f0b936a55fd2dcefac\\" />\\n\\nIn each case I start to test if the URI is one of the supported types (idea, vote, user) and if this is the case I call the `get()` or `upsert()` method that will do the call to Couchbase.\\n\\nThe `get()` and `upsert()` methods are using more or less the same approach. I test if the document exists, if the type is correct and do the operation to Couchbase. Let\'s focus on the `upsert()`` method. I call it `upsert()` since the same operation is used to create and update the document.\\n\\n<Gist id=\\"91858dcad51affdf3521\\" />\\n\\nIn this function I start by testing if the document contains a type and if the type is the one expected (line 3).\\n\\nThen I check if the document id is present, to see if I need to create it or not. This is one of the reason why I like to keep the id/key in the document, yes I duplicate it, but it makes the development really easy. So if I have to create a new document I have to generate a new id. I chose to create a counter for each type. this is why I call the incr function (line 7) and then use the returned value to create the document (line 10).\\n\\n_Note:_ as you can see, my documents contain the an ID as part of the attributes. This ID is the same value that the one used to set the document (the \\"key\\"). It is not necessary a good practice to duplicate this information, and in many case the application only use the document key itself. I personally like to put the ID in the document itself too, because it simplifies a lot the development.\\n\\nIf the ID is present, I just call the update operation to save the document. (line 15)\\n\\nThe delete operation is equivalent to the get, using the delete HTTP operation.\\n\\nSo now I can get, insert and update the documents. I still need to do some work to deal with the lists. As you can guess, here I need to call the views. I won\'t go in the detail of the simple list of ideas. Let\'s focus on the view that shows the result of the votes.\\n\\n<Gist id=\\"6468058737ff53553ae1\\" />\\n\\nFor this part of the application I use a small trick to use the collated view. The `/api/results/` call returns the list of ideas with their title and the total number of votes. The result looks like the following:\\n\\n<Gist id=\\"6cfdedf1410ca99744bd\\" />\\n\\n\\nNote that it is also possible to select only one idea , you just need to pass the id to the call for example.\\n\\nIf you look in more detail the function, not only I call the view, but I build an array in which I put the idea id, label, then on the next loop, I add the number of vote. This is possible because the view is a collated view of the ideas and its votes.\\n\\nI have now my REST Services, including advanced query capabilities. It is time now to use these services and build the user interface.\\n\\n### Create the UI\\n\\nFor the view I am using AngularJS, that I am packaging in the same node.js application for simplicity reason\\n\\n\\n#### Simple UI without Login/Security\\n\\n\\nThe code of the application without login is available branch in [02-simple-ui-no-login](https://github.com/tgrall/couchbase-node-ideas/tree/02-simple-ui-no-login)\\n\\nYou can run the application &nbsp;with simple services using the following command:\\n\\n```shell\\n> git checkout -f 02-simple-ui-no-login\\n> node app.js\\n```\\n\\nThe application is based on AngularJS and Twitter Boostrap.\\n\\nI am using basic feature and packaging for Angular :\\n\\n* `/public/js/app.js` contains the module declaration and all the routes to the different views/controllers\\n* `/public/js/controllers.js` contains all the controller. I will show some of them but basically this is where that I call the services that I have created above.\\n* `/views/partials/` contains the different pages/screens used by the application.\\n\\nBecause the application is quite simple I have not done any packaging of directive, or other functions. This is true at for AngularJS and Node.js parts.\\n\\n*Dummy user management*\\n\\nIn this first version of the UI I have not yet integrated any login/security, so I fake the user login using a global scope variable that `$scope.user` that you can see in the controller `AppCtrl()`. Since I have not yet implemented the login/security, I have added at the bottom of the page a textfield where you can enter a \\"dummy\\" username to test the application. This field is inserted in the `/views/index.html` page.\\n\\n*List Views and Number of Votes*\\n\\nThe home page of the application contains the list of ideas and number of votes.\\n\\n![](http://2.bp.blogspot.com/-tniNkr_Pl0Q/USidTLKHw1I/AAAAAAAAAbQ/BWtfTaAWG1w/s320/ideas-home-page.png)\\n\\nLook at the EntriesListCtrl controller and the `view/index.html` file. As you can guess this is based on the Couchbase collated view that return the list of ideas and number of vote.\\n\\n*Create/Edit an idea*\\n\\nWhen the user click on the New link in the navigation, the application call the view `/view/partials/idea-form.html`. &nbsp;This form is called using the \\"/#/idea/new\\" URL.\\n\\nJust look at the `IdeaFormCtrl` controller to see what is happening :\\n\\n```javascript\\nfunction IdeaFormCtrl($rootScope, $scope, $routeParams, $http, $location) {\\n  $scope.idea = null;\\n    if ($routeParams.id ) {\\n        $http({method: \'GET\', url: \'/api/idea/\'+ $routeParams.id }).success(function(data, status, headers, config) {           \\n                $scope.idea = data;\\n            });\\n    }\\n \\n    $scope.save = function() {          \\n        $scope.idea.type = \\"idea\\"; // set the type\\n        $scope.idea.user_id = $scope.user;\\n        $http.post(\'/api/idea\',$scope.idea).success(function(data) {\\n            $location.path(\'/\');\\n        });\\n    }\\n    $scope.cancel = function() {\\n        $location.path(\'/\');\\n    }\\n \\n}\\nIdeaFormCtrl.$inject = [\'$rootScope\', \'$scope\', \'$routeParams\',\'$http\', \'$location\'];\\n```\\n\\nFirst of all I test if the controller is called with a idea identifier in the URL ( `$routeParams.id` - line 3) . If the ID is present, I call the REST API to get the idea and set it into the `$scope.idea` variable.\\n\\nThen on line 9, you can see the `$scope.save()` function that calls the REST API to save/update the idea to Couchbase. I use the line 10 and 11 to set the user and the type of data to the idea.\\n\\n*Note:* It is interesting to look at these lines, by adding the two attributes (user &amp; type) I modify the \\"schema\\" of my data. I am adding new fields to my document that will be stored as it is in Couchbase. Once again, you see here that I drive the data type from my application. I could take another approach and force the type in the service layer. For this example I chose to put that in the application layer, that is supposed to send the proper data types.\\n\\n*Other Interactions*\\n\\nThe same approach is used to create a vote associated to a user/idea as you can see in the `VoteFormCtrl`  controller.\\n\\nI won\'t go in all the details of all operations, I am just inviting you to look at the code of the application, and feel free to add comment to this blog post if I need to clarify other part of the application.\\n\\n#### Iterative Development : adding a value to the vote!\\n\\nThe code of the services is available in branch 01-simple-services:\\n\\nYou can run the application with simple services using the following command:\\n\\n```\\n> git checkout -f 03-vote-with-value\\n> node app.js\\n```\\n*Adding the field in the form*\\n\\nSomething that I really like about working with AngularJS, Node and Couchbase is the fact that the developer uses JSON from the database to the browser.\\n\\nSo let\'s implement a new feature, where instead of having only a comment the user can give a rate to its vote from 1 to 5. Doing that is quite easy, here are the steps:\\n\\n* Modify the UI : adding a new field\\n* Modify the Couchabe View to use the new field\\n\\nThis is it! AngularJS deals with the binding of the new field, so I just need to edit the `/views/partials/idea-form.html` to add this. For this I need to add the list of values in the controller and expose it into a select box in form.\\n\\nThe list of value located in the `$scope.ratings` variable :\\n\\n<Gist id=\\"fe7c8625a6f54dfd2425\\" />\\n\\nOnce this is done you can add a select box into your view using the following code :\\n\\n<Gist id=\\"d4115c3dbdd5a25614d9\\" />\\n\\nTo add the select box into the form, I just use AngularJS features:\\n\\n* the list of value described in my controller using the `ng-options` attribute\\n* the binding to the vote.rating field object using `ng-model` attribute.\\n\\nI am adding the field in my form, I bind this field to my Javascript object; and... nothing else! Since my REST API is just consuming the JSON object as it is, AngularJS will send the vote object with the new attribute.\\n\\n*Update the view to use the rating*\\n\\nNow that my database is dealing with a new attribute in the vote, I need to update my view to use this in the sum function. (I could calculate an average too, but here I want the sum of all the vote/ratings).\\n\\n<Gist id=\\"c2fb3f9a3127df75e454\\" />\\n\\nThe only line that I have changed is the line number 7. The logic is simple, if the rating is present I emit it, if not I emit a 2, that is a medium rating for an idea.\\n\\nThis is a small tip that allow me to have a working view/system without having to update all the existing document if I have some.\\n\\nI\'ll stop here for now, and will add new feature later such as User Authentication and User Management using for example [Passport](http://passportjs.org/).\\n\\n### Version and Upgrade Management\\n\\nIf you looked closely to the code of the application the views are automatically imported from the app.js file when the application is starting.\\n\\nIn fact I have added a small function that check the current version installed and update the views with the correct version when needed.\\n\\nYou can look at the function [`initApplication()`](https://github.com/tgrall/couchbase-node-ideas/blob/03-vote-with-value/app.js#L21) :\\n\\n* Load the version number from Couchbase (document with ID \\"`app.version`\\")\\n* Check the version of if this is different\\n* Update/Create the view (I am doing it in production mode here, in real application it will be better to use dev mode - just prefix the design document ID with `\\"dev_\\"` )\\n* Once the view is created update/create the `\\"app.version\\"` document with the new ID.\\n\\n### Conclusion\\n\\nIn this article we have seen how you can quickly develop your application/prototype and leverage the flexibility of NoSQL for developers. The steps to do this are:\\n\\n1.  Design your document model and API (REST)\\n2.  Create the UI that consumes the API\\n3.  Modify your model by simply adding field into the UI\\n4.  Update the view to adapt your lists to your new model\\n\\nIn addition to this, I have also quickly explain how you can from your code control the version of your application and deploy new views (and other things) automatically.\\n\\nI will post another blog post in few days to explain how you can easily integrate user management, security to your application and database easily"},{"id":"/2013/02/18/how-to-get-the-latest-document-by-date-slash-time-field","metadata":{"permalink":"/blog/2013/02/18/how-to-get-the-latest-document-by-date-slash-time-field","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-02-18-how-to-get-the-latest-document-by-date-slash-time-field.md","source":"@site/blog/2013-02-18-how-to-get-the-latest-document-by-date-slash-time-field.md","title":"How to get the latest document by date/time field?","description":"I read this question on Twitter, let me answer the question in this short article.","date":"2013-02-18T00:00:00.000Z","formattedDate":"February 18, 2013","tags":[],"readingTime":1.295,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to get the latest document by date/time field?","categories":"couchbase json views nosql"},"prevItem":{"title":"Easy application development with Couchbase, Angular and Node","permalink":"/blog/2013/03/06/easy-application-development-with-couchbase-angular-and-node"},"nextItem":{"title":"Introduction to Collated Views with Couchbase 2.0","permalink":"/blog/2013/02/13/introduction-to-collated-views-with-couchbase-2-dot-0"}},"content":"I read this question on Twitter, let me answer the question in this short article.\\n\\nFirst of all you need to be sure your documents have an attribute that contains a date ;), something like :\\n\\n```json\\n{\\n  \\"type\\" : \\"emp\\",\\n  \\"id\\":\\"001\\",\\n  \\"name\\":\\"John Doe\\",\\n  \\"hiredate\\":\\"Jan 1, 2013 8:32:00 AM\\"\\n}\\n```\\n\\n\\nTo get the \\"latest hired employee\\" you need to create a view, and emit the hire date as key. The important part is to check that this date is emitted in a format that is sorted properly, for example an array of value using dateToArray function, or the time as numerical value. In the following view I am using the date as an array like that I will be able to do some grouping but this is another topic. The view looks like the following:\\n\\n```javascript\\nfunction (doc, meta) {\\n  if (doc.hiredate) {\\n    emit( dateToArray(doc.hiredate) );\\n  }\\n}\\n```\\n\\nNow that you have a view. You can now query it using the parameters:\\n\\n*   descending = true\\n*   limit = 1\\n\\nIf you use Java SDK the code will look like the following :\\n\\n```java\\nimport com.couchbase.client.protocol.views.Query;\\nimport com.couchbase.client.protocol.views.View;\\nimport com.couchbase.client.protocol.views.ViewResponse;\\nimport com.couchbase.client.protocol.views.ViewRow;\\n...\\n...\\n...\\n\\n  View view = cb.getView(\\"employees\\", \\"by_hiredate\\");\\n  Query query = new Query();\\n  query.setIncludeDocs(true);\\n  query.setLimit(1);\\n  query.setDescending(true);\\n  ViewResponse viewResponse = cb.query(view, query);\\n  for (ViewRow row : viewResponse) {\\n    String documentJson = row.getDocument();\\n  }\\n```\\n\\nFinally it is important when you work with views to understand how the index are managed by the server so be sure your read the chapter \\"[Index Updates and the stale Parameter](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-writing-stale.html)\\"."},{"id":"/2013/02/13/introduction-to-collated-views-with-couchbase-2-dot-0","metadata":{"permalink":"/blog/2013/02/13/introduction-to-collated-views-with-couchbase-2-dot-0","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-02-13-introduction-to-collated-views-with-couchbase-2-dot-0.md","source":"@site/blog/2013-02-13-introduction-to-collated-views-with-couchbase-2-dot-0.md","title":"Introduction to Collated Views with Couchbase 2.0","description":"Most of the applications have to deal with \\"master/detail\\" type of data:","date":"2013-02-13T00:00:00.000Z","formattedDate":"February 13, 2013","tags":[{"label":"couchbase","permalink":"/blog/tags/couchbase"},{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"view","permalink":"/blog/tags/view"},{"label":"json","permalink":"/blog/tags/json"}],"readingTime":4.755,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Introduction to Collated Views with Couchbase 2.0","tags":["couchbase","nosql","view","json"]},"prevItem":{"title":"How to get the latest document by date/time field?","permalink":"/blog/2013/02/18/how-to-get-the-latest-document-by-date-slash-time-field"},"nextItem":{"title":"Getting started with Couchbase and node.js on Windows","permalink":"/blog/2013/01/04/getting-started-with-couchbase-and-node-dot-js-on-windows"}},"content":"import Gist from \'react-gist\';\\n\\n\\nMost of the applications have to deal with \\"master/detail\\" type of data:\\n\\n* breweries and beer\\n* department and employees\\n* invoices and items\\n* ...\\n\\nThis is necessary for example to create application view like the following:\\n  ![]( http://1.bp.blogspot.com/-vdpPEX_Wfm0/URt23LU3r1I/AAAAAAAAAbA/rDykow-eQY4/s320/Screen+Shot+2013-02-13+at+12.19.11+PM.png )\\n\\nWith Couchbase, and many of the document oriented databases you have different ways to deal with this, you can:\\n\\n* Create a single document for each master and embed all the children in it\\n* Create a master and child documents and link them using an attribute.\\n\\nIn the first case when all the information are stored in a single document it is quite easy to use the entire set of data and for example create a screen that shows all the information, but what about the second case?\\n\\nIn this post I am explaining how it is possible to use Couchbase views to deal with that an make it easy to create master/detail views.\\n\\nAs an ex-Oracle employee, I am using the infamous SCOTT schema with the DEPT and EMP tables, as the first example. Then at the end I will extend this to the beer sample data provided with Couchbase.\\n\\n### The Data\\n\\nCouchbase is a schema-less database, and you can store \u201canything you want\u201d into it, but for this you need to use JSON documents and create 2 types of document : \u201cdepartment\u201d and \u201cemployee\u201d.\\n\\nThe way we usually do that is using a technical attribute to type the document. So the employee and department document will look as follow :\\n\\nDepartment\\n\\n``` json\\n{\\n  \\"type\\": \\"dept\\",\\n  \\"id\\": 10,\\n  \\"name\\": \\"Accounting\\",\\n  \\"city\\": \\"New York\\"\\n}\\n```\\n\\nEmployee\\n\\n``` json\\n{\\n  \\"type\\": \\"emp\\",\\n  \\"id\\": 7782,\\n  \\"name\\": \\"Blake\\",\\n  \\"job\\": \\"Clark\\",\\n  \\"manager\\": 7839,\\n  \\"salary\\": 2450,\\n  \\"dept_id\\": \\"dept__10\\"\\n}\\n```\\nThis shows just the document, in Couchbase you have to associate a document to a key. For this example I am using a simple pattern : `type__id`, for these documents the keys will look like the following:\\n\\n* `dept__10`\\n* `emp__20`\\n\\nYou can use any pattern to create a key, for example for the employee you could chose to put an email.\\n\\nNote the \u201cdept_id\u201d attribute in the employee document. This is the key of the department; you can see that as the \u201cforeign key\u201d. But remember, the relationship between the department and employee documents are managed entirely by the application, Couchbase Server does not enforce it.\\n\\nI have created a Zip file that contains all the data, you can download it from [here](http://db.tt/NsUfweBM); and import the data into Couchbase using the `cbdocloader` utility. To import the data run the following command from a terminal window:\\n\\n```\\n./cbdocloader -n 127.0.0.1:8091 -u Administrator -p password -b default ~/Downloads/emp-dept.zip\\n```\\n\\nYou can learn more about the `cbdocloader` tool in [the documentation](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-admin-cmdline-cbdocloader.html).\\n\\n### The View\\n\\nQueries inside Couchbase are based on [views](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-basics.html); and views build indexes, so we have to create a view, a \\"collated view\\" to be exact.\\n\\nThe idea behing a collated view is to produce an index where the keys are ordered so that a parent id appears first followed by its children. So we are generating an index that will look like:\\n\\n```\\nDEPT_10, Accounting\\nDEPT_10, Blake\\nDEPT_10, Miller\\nDEPT_20, Research\\nDEPT_20, Adams\\nDEPT_20, Ford\\n...\\n```\\n\\nThis is in fact quite easy to do with Couchbase views. The only trick here is to control the order and be sure the master is always the first one, just before its children.\\n\\nSo to control this we can create an compound key that contains the department id, a \\"sorting\\" element and the name (beer or brewery)\\n\\nSo the map function of the view looks like the following:\\n\\n<Gist id=\\"bfb625e0ff175b1778bc\\" \\n/>\\n\\n\\nThe key is composed of:\\n\\n* the department id extracted from the department document itself or from the employee document depending of the type of document\\n* an arbitrary number that is used to control the ordering. I put 0 for the department, 1 for the employee\\n* the name of the department or the employee, this also allows to sort the result by name\\n\\nIn addition to the key, this view is used to emit some information about the salary of the employees. The salary is simply the sum of the salary plus thecommission when exists. The result of the view looks like:\\n\\n![]( http://2.bp.blogspot.com/-wLmoUPfo-f8/URtierY_qBI/AAAAAAAAAaw/i3DMCvilCcU/s400/Screen+Shot+2013-02-13+at+10.52.30+AM.png )\\n\\nWith this view you can now use the result of the view to build report for your application. It is also possible to use parameters in your query to see only a part of the data, for example by departement, using for example `startkey=[\\"dept__20\\",0]&amp;endkey=[\\"dept__20\\",2]` to view only the data -Department and Employees- of the deparment 20-Research.\\n\\n### The Beer Sample Application\\n\\nYou can create an equivalent view for the beer sample application where you print all the breweries and beers in the same report. The view is called \\"`all_with_beers`\\" in the design document \\"`brewery`\\". The view looks like:\\n\\n<Gist id=\\"4944154\\" \\n/>\\n\\n\\nOnce you have publish it in production you can use it in the Beer Sample application, for this example I have modified the Java sample application.\\n\\n**Create a servlet to handle user request and on the /all URI.**\\n\\nThe \\"BreweryAndBeerServlet\\" that calls the view using the following code :\\n\\n<Gist id=\\"4944142\\" \\n/>\\n\\nThe result of the query is set into the HttpRequest and the all.jsp page is executed. The JSP uses JSTL to print the information using the following code:\\n\\n<Gist id=\\"c163f57778d663632e87\\" \\n/>\\nThe JSP gets the items from the HTTP Request and loops on each items, then based on the type of the item the information is printed. The final result looks like :\\n\\n![]( http://1.bp.blogspot.com/-vdpPEX_Wfm0/URt23LU3r1I/AAAAAAAAAbA/rDykow-eQY4/s320/Screen+Shot+2013-02-13+at+12.19.11+PM.png )\\n\\nThis extension to the Beer Sample application is available here :[https://github.com/tgrall/beersample-java/tree/BreweriesAndBeers](https://github.com/tgrall/beersample-java/tree/BreweriesAndBeers)"},{"id":"/2013/01/04/getting-started-with-couchbase-and-node-dot-js-on-windows","metadata":{"permalink":"/blog/2013/01/04/getting-started-with-couchbase-and-node-dot-js-on-windows","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2013-01-04-getting-started-with-couchbase-and-node-dot-js-on-windows.md","source":"@site/blog/2013-01-04-getting-started-with-couchbase-and-node-dot-js-on-windows.md","title":"Getting started with Couchbase and node.js on Windows","description":"In a previous post I have explained how to use Couchbase and Node.js on OS X. Since it is quite different on Windows here another article about it.","date":"2013-01-04T00:00:00.000Z","formattedDate":"January 4, 2013","tags":[],"readingTime":3.495,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Getting started with Couchbase and node.js on Windows","categories":"couchbase nosql nodejs windows"},"prevItem":{"title":"Introduction to Collated Views with Couchbase 2.0","permalink":"/blog/2013/02/13/introduction-to-collated-views-with-couchbase-2-dot-0"},"nextItem":{"title":"Couchbase 101: Create views (MapReduce) from your Java application","permalink":"/blog/2012/12/30/couchbase-101-create-views-mapreduce-from-your-java-application"}},"content":"In a previous post I have explained how to use Couchbase and Node.js on OS X. Since it is quite different on Windows here another article about it.\\n\\n### Install Couchbase Server 2.0\\n\\n\\nIf you have not installed Couchbase Server already, do the following :\\n\\n1.  Download Couchbase Server from [here](http://www.couchbase.com/download)\\n2.  Run the installer\\n3.  Configure the database at [http://localhost:8091](http://localhost:8091/) (if you have issue take a look to [this article](http://tugdualgrall.blogspot.fr/2012/12/what-to-do-if-your-couchbase-server.html))\\n\\nThese steps are documented in the [Couchbase Server Manual](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-getting-started-install-win.html).\\n\\n###  Install Node\\n\\n\\n**Install latest version of node**\\n\\nIt is quite easy to install Node.js using the Windows installer provided at [http://nodejs.org](http://nodejs.org/).\\n\\nOnce you have installed node, you can test is using the command line interface:\\n\\n``` sh\\nnode\\n&gt; console.log(process.version);\\nv0.8.16\\n```\\nNode is installed. So far so good !\\n\\n###  Install Couchnode\\nCouchnode, the Couchbase Client Library for Node.js, is a native module. The tool used to install native modules is `node-gyp`.  So to install couchnode you need to install :\\n\\n* node-gyp\\n* python\\n* Visual Studio to have access to a C/C++ compiler\\n\\n####  Install node-gyp\\n\\nThe node-gyp module is easy to install and you can install it using npm using the following command:\\n\\n``` sh\\nnpm install -g node-gyp\\n```\\n\\n\\nThe *-g* parameter indicates that this module will be installed globally and added to your `%PATH%`.\\n\\n#### Install Python\\n\\nGYP uses Python to generate the project, so you need to install it on your environment. I have installed [Python 2.7.3](http://www.python.org/download/releases/2.7.3/) using the Windows installer.\\n\\n\\n**Install Visual Studio**\\n\\nFinally you need a C/C++ compiler, the best way to get it is to install Visual Studio. As you probably know I am not a Windows expert and I do not know a lot about Microsoft development tools. I have downloaded Visual Studio Express the free development tools from [here](http://www.microsoft.com/visualstudio/eng/downloads); it was sufficient.\\n\\n**Install Libcouchbase for Windows**\\n\\nCouchnode uses libcouchbase the C client library, so before running the npm install for Couchbase, you need to install libcouchbase itself.\\n\\nYou can download it from [Couchbase site](http://www.couchbase.com/develop/c/current). The Windows versions are located in the left menu of the page. Download the zip file, that matches your environment. I have downloaded the \\"Windows, 64-bit MSVC 10\\".\\n\\nNode-gyp will look for all the dependencies (DLL, library headers) into `c:\\\\couchbase` directory, so you need to unzip the file in this folder. This location comes from the [binding.gyp](https://github.com/couchbase/couchnode/blob/master/binding.gyp#L7) file of the couchnode project.\\n\\n**Install and Test Couchnode itself!**\\n\\nLet\'s check what we have done so far; we have installed:\\n\\n*   Node\\n*   node-gyp\\n*   Python\\n*   Visual Studio\\n*   Libcouchbase\\n\\nWe are now ready to install and use couchnode itself. For this we can create a new node project.\\n\\n``` sh\\nmkdir my-app\\ncd my-app\\nnpm install couchbase\\n```\\n\\nThe install command will:\\n\\n*   Create a node_modules folder and put couchbase client library in it\\n*   When installing/building couchnode on Windows I had the following warning :\\n\\nC:\\\\Program Files (x86)\\\\MSBuild\\\\Microsoft.Cpp\\\\v4.0\\\\V110\\\\Microsoft.CppBuild.targets(1138,5): warning MSB8012: TargetExt(.dll) does not match the Linker\'s Output\\nFile property value (.node). This may cause your project to build incorrectly.\\nTo correct this, please make sure that $(OutDir), $(TargetName) and $(TargetExt) property values match the value specified in %(Link.OutputFile).\\n[C:\\\\Users\\\\tgrall\\\\node\\\\node_modules\\\\couchbase\\\\build\\\\couchbase_impl.vcxproj]\\n\\nThis is only a warning and as far as I know, it is not a blocker. At the end of the log you should see:\\n\\n``` sh\\ncouchbase@0.0.10 node_modules\\\\couchbase\\n\u251c\u2500\u2500 bindings@1.0.0\\n\u2514\u2500\u2500 request@2.11.4\\n```\\n\\nYou have successfully installed couchnode.\\n\\nLet\'s now write a small test. Create a `test.js` file with the following content:\\n\\n``` js\\nvar  driver = require(\'couchbase\');\\n\\ndriver.connect({\\n  \\"username\\": \\"\\",\\n  \\"password\\": \\"\\",\\n  \\"hostname\\": \\"localhost:8091\\",\\n  \\"bucket\\": \\"default\\"},\\n  function(err, cb) {\\n    if (err) {\\n      throw (err)\\n    }\\n\\n    var key = \'foo\';\\n    cb.set(key, \'{\\"server\\" : \\"couchbase\\", \\"version\\" : 2 }\' , function (err, meta) {\\n      if (err) { console.log(err); }\\n      cb.get(key, function(err, doc) {\\n        if (err){ console.log(err);}\\n        console.log(doc);\\n      });  \\n    });\\n  });\\n```\\n\\nRun this with the command:\\n\\n``` sh\\nnode test.js\\n```\\n\\nYou should see the following text in your console :\\n\\n```\\n{ server: \'couchbase\', version: 2 }\\n```\\n\\n\\n### Conclusion\\n\\nIn this article you have learned how to:\\n\\n* Install Couchbase\\n* Install Node\\n* Install and configure node-gyp\\n* Install and use Couchbase and Node\\n\\nall this on Windows 7."},{"id":"/2012/12/30/couchbase-101-create-views-mapreduce-from-your-java-application","metadata":{"permalink":"/blog/2012/12/30/couchbase-101-create-views-mapreduce-from-your-java-application","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-12-30-couchbase-101-create-views-mapreduce-from-your-java-application.md","source":"@site/blog/2012-12-30-couchbase-101-create-views-mapreduce-from-your-java-application.md","title":"Couchbase 101: Create views (MapReduce) from your Java application","description":"When you are developing a new applications with Couchbase 2.0, you sometimes need to create view dynamically from your code. For example you may need this when you are installing your application, writing some test, or you can also use that when you are building frameworks, and wants to dynamically create views to query data. This post shows how to do it.","date":"2012-12-30T00:00:00.000Z","formattedDate":"December 30, 2012","tags":[],"readingTime":5.835,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Couchbase 101: Create views (MapReduce) from your Java application","categories":"nosql couchbase java"},"prevItem":{"title":"Getting started with Couchbase and node.js on Windows","permalink":"/blog/2013/01/04/getting-started-with-couchbase-and-node-dot-js-on-windows"},"nextItem":{"title":"What to do if your Couchbase Server does not start?","permalink":"/blog/2012/12/26/what-to-do-if-your-couchbase-server-does-not-start"}},"content":"When you are developing a new applications with Couchbase 2.0, you sometimes need to create view dynamically from your code. For example you may need this when you are installing your application, writing some test, or you can also use that when you are building frameworks, and wants to dynamically create views to query data. This post shows how to do it.\\n\\n### Prerequisites\\n\\n*   [Couchbase Server 2.0](http://www.couchbase.com/download)\\n*   [Couchbase Jave Client Library 1.1.x](http://www.couchbase.com/develop/java/current)\\n*   [Beer Sample dataset](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-sampledata-beer.html)\\n\\nIf you are using Maven you can use the following information in your `pom.xml` to add the Java Client library:\\n\\n``` xml\\n<repositories>\\n  <repository>\\n    <id>couchbase</id>\\n    <name>Couchbase Maven Repository</name>\\n    <layout>default</layout>\\n    <url>http://files.couchbase.com/maven2/</url>\\n    <snapshots>\\n      <enabled>false</enabled>\\n    </snapshots>\\n  </repository>\\n</repositories>\\n\\n<dependencies>\\n  <dependency>\\n    <groupid>couchbase</groupid>\\n    <artifactid>couchbase-client</artifactid>\\n    <version>1.1.0</version>\\n    <type>jar</type>\\n  </dependency>\\n</dependencies>\\n```\\n\\n\\nSee online at [https://gist.github.com/4337172](https://gist.github.com/4337172)\\n\\n### Create and Manage Views From Java\\n\\n\\nThe full Maven project is available on [Github](https://github.com/tgrall/couchbase-java-101/tree/master/java-document-design/).\\n\\n#### Connect to Couchbase Cluster\\n\\nThe first thing to do when you want to create a view from Java is obviously to connect to the cluster.\\n\\n``` java\\nimport com.couchbase.client.CouchbaseClient;\\n...\\n\\nList<uri> uris = new LinkedList<uri>();\\nuris.add(URI.create(\\"http://127.0.0.1:8091/pools\\"));\\nCouchbaseClient client = null;\\ntry {\\n  client = new CouchbaseClient(uris, \\"beer-sample\\", \\"\\");\\n\\n  // put your code here\\n\\n  client.shutdown();\\n\\n  } catch (Exception e) {\\n    System.err.println(\\"Error connecting to Couchbase: \\" + e.getMessage());\\n    System.exit(0);\\n  }\\n...\\n```\\n\\n\\n1.  Create a list of URIs to different nodes of the cluster - lines 5-6. (In this example I am working on a single node)\\n2.  Connect to the bucket, in our case `beer-sample` -line 9. You can include the password if the bucket is protected ( this is not the case here so I am sending an empty string)\\n\\nIf you are looking for more information about Couchbase and Java, you can read this article from DZone : [Hello World with Couchbase and Java](http://architects.dzone.com/articles/hello-world-couchbase-and-java).\\n\\nLet\'s now talk about Couchbase views. You use views/map-reduce functions to index and query data from Couchbase Server based on the content of the JSON document you store inside Couchbase. For more information about views you can look at the [\\"view basics\\" chapter](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-basics.html) of the Couchbase Server Manual.\\n\\n#### Create Views from Java\\n\\nCreating a view from Java is really easy : the Java Client Library contains all the classes and methods to do it. As a concrete use case we will use the Application that is described in the [Couchbase Java Tutorial](http://www.couchbase.com/docs/couchbase-sdk-java-1.1/tutorial.html).\\n\\nWhen you follow this tutorial, you need to manually create some views, as you can see [here](http://www.couchbase.com/docs/couchbase-sdk-java-1.1/preps-views.html). In this example, we will create our map function and directly in our Java code and then store it to Couchbase Server. The tutorial asks you to create the following artifacts:\\n\\n* a view named \\"by_name\\"\\n* in the design document named `\\"dev_beer\\"` (development mode)\\n* and the map function which looks like the following :\\n\\n``` js\\nfunction (doc, meta) {\\n  if(doc.type && doc.type == \\"beer\\") {\\n    emit(doc.name, null);\\n  }\\n}\\n```\\n\\nThe following code allows you to do it from Java:\\n\\n``` java\\nimport com.couchbase.client.protocol.views.DesignDocument;\\nimport com.couchbase.client.protocol.views.ViewDesign;\\n...\\nDesignDocument designDoc = new DesignDocument(\\"dev_beer\\");\\n\\nString viewName = \\"by_name\\";\\nString mapFunction =\\n\\"function (doc, meta) {\\\\n\\" +\\n\\"  if(doc.type && doc.type == \\\\\\"beer\\\\\\") {\\\\n\\" +\\n\\"    emit(doc.name);\\\\n\\" +\\n\\"  }\\\\n\\" +\\n\\"}\\";\\n\\nViewDesign viewDesign = new ViewDesign(viewName,mapFunction);\\ndesignDoc.getViews().add(viewDesign);\\nclient.createDesignDoc( designDoc );\\n...\\n```\\n\\n* Create a design document using the `com.couchbase.client.protocol.views.DesignDocument` class - line 4.\\n* Create a view using `com.couchbase.client.protocol.views.ViewDesign` class with a name and the map function - line 14.\\n* You can add this view to a design document - line 15\\n* Finally save the document into the cluster using the `CouchbaseClient.createDesignDoc` method.\\n\\nIf you need to use a reduce function (built-in or custom) you just need to pass to the ViewDesign constructor as 3rd parameter.\\n\\nWhen developing view, from Java or from any other tool/language be sure you understand what are the best practices, and the life cycle of the index. This is why I am inviting you to take a look to the following chapters in the Couchbase documentation:\\n\\n\\n*   [View Writing Best Practice](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-writing-bestpractice.html) : for example in the map function, I do not emit any value. I only emit a key (the beer name).\\n*   [Views and Stored Data](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-datastore.html)\\n*   [Development and Production Views](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-types.html) : in the view above, I have created the view in the development environment (`dev_` prefix) allowing me to test and use it on a subset of the data (cluster/index)\\n\\n\\n#### Using the view\\n\\nFirst of all, the view that you just created is in \\"development mode\\", and by default the Java client SDK will only access the view when it is in \\"production mode\\". This means that when you are calling a view from your application it will search it into the production environment. So before connecting to Couchbase cluster you need to setup the `viewmode` to development.\\n\\nThis is done using the `viewmode` environment variable from the Java SDK, that could be set using the following methods:\\n\\n* In your code, add this line **before** the client connects to the cluster : `System.setProperty(\\"viewmode\\", \\"development\\");`\\n* At the command line `-Dviewmode=development`\\n* In a properties file `viewmode=development`\\n\\nOnce it is done you can call the view using the following code:\\n\\n``` java\\nimport import com.couchbase.client.protocol.views.*;\\n\\n...\\nSystem.setProperty(\\"viewmode\\", \\"development\\"); // before the connection to Couchbase\\n...\\nView view = client.getView(\\"beer\\", \\"by_name\\");\\nQuery query = new Query();\\nquery.setIncludeDocs(true).setLimit(20);\\nquery.setStale( Stale.FALSE );\\nViewResponse result = client.query(view, query);\\nfor(ViewRow row : result) {\\n  row.getDocument(); // deal with the document/data\\n}\\n...\\n```\\n\\nThis code queries the view you just created. This means Couchbase Server will generate an index based on your map function, will query the server for results. In this case, we specifically want to set a limit of 20 results and also get the most current results by setting Stale.FALSE.\\n\\n* Set the `viewmode` to `development` - line 4\\n* Get the view using the `CouchbaseClient.getView()` method -line 6-. As you can see I just use the name beer for the design document (and not dev_beer, Couchbase will know where to search since I am in development mode)\\n* Create a query and set a limit (20) and ask the SDK to return the document itself\\n`setIncludeDocs(true)` -line 8- The document will be returned from Couchbase server in the most efficient way\\n* Ask the system to update the index before returning the result using query.setStale( Stale.FALSE `); `-line 9-. Once again be careful when you use setStale method. Just to be sure here is the documentation about it : [Index Updates and the stale Parameter](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-writing-stale.html)\\n* Execute the query - line 10\\n* And use the result - lines 11-13\\n\\n### Conclusion\\n\\nIn this article, you have learned:\\n\\n* How to create Couchbase views from Java\\n* Call this view from Java\\n* Configure development/production mode views from Couchbase Java Client Library\\n\\n\\nThis example is limited to the creation of a view, you can take a look to the other methods related to design documents and views if you want to manage your design documents : `getDesignDocument()`, `deleteDesignDocument()`, ..."},{"id":"/2012/12/26/what-to-do-if-your-couchbase-server-does-not-start","metadata":{"permalink":"/blog/2012/12/26/what-to-do-if-your-couchbase-server-does-not-start","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-12-26-what-to-do-if-your-couchbase-server-does-not-start.md","source":"@site/blog/2012-12-26-what-to-do-if-your-couchbase-server-does-not-start.md","title":"What to do if your Couchbase Server does not start?","description":"Working with the Couchbase 2.0.0 release you may have issues when trying to access the Web Admin Console or simply starting the server. This is due to the way Couchbase Server uses the IP address/hostname during the installation process. So when you have one of the following errors :","date":"2012-12-26T00:00:00.000Z","formattedDate":"December 26, 2012","tags":[],"readingTime":5.305,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"What to do if your Couchbase Server does not start?","categories":"nosql couchbase windows"},"prevItem":{"title":"Couchbase 101: Create views (MapReduce) from your Java application","permalink":"/blog/2012/12/30/couchbase-101-create-views-mapreduce-from-your-java-application"},"nextItem":{"title":"New Adventure...","permalink":"/blog/2012/12/01/new-adventure-dot-dot-dot"}},"content":"Working with the Couchbase 2.0.0 release you may have issues when trying to access the Web Admin Console or simply starting the server. This is due to the way Couchbase Server uses the IP address/hostname during the installation process. So when you have one of the following errors :\\n\\n* On Windows, Server is not working at all, even after installation. You can access the sever on port 8092 (Couchbase API port), but cannot on port 8091\\n* You have the following error when accessing the console `\\"[10:02:02] IP address seems to have changed. Unable to listen on \'ns_1@10.0.2.15\'\\"`\\n\\n![]( http://2.bp.blogspot.com/-OXj1bGEZTGg/UNs8QBx6X-I/AAAAAAAAAaI/TproFMOQXcE/s320/cb-20-ip-address-error.png )\\n\\n* When you try to restart the server it does not start and you have the following error message in the error log :\\n`\\"Configured address \'10.0.2.15\' seems to be invalid. Will refuse to start for safety reasons\\"`\\n\\nSome of these issues are related to a known issue on Windows ( see [MB-7417](http://www.couchbase.com/issues/browse/MB-7417) that will be fixed in 2.0.1) or the fact that Couchbase server does not support change of the IP address after installation.  This is documented in the section \u201c[Using Couchbase in the Cloud: Handling Changes in IP Addresses](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-bestpractice-cloud-ip.html)\u201d of the Couchbase Server Manual. This article explains what should be done when configuring Couchbase Server on Windows, but you can do equivalent steps on any platform using the shell scripts available on Linux and/or Mac OS X.\\n\\nOnce you have installed Couchbase, you can see in the console that the IP address of your server is used :\\n  ![]( http://3.bp.blogspot.com/-7wJEnnsZNlA/ULzAYBU09tI/AAAAAAAAAZk/PxZGdUbqo6k/s320/Screen+Shot+2012-12-03+at+4.07.03+PM.png )\\n\\nTypically the address 192.168.0.97 is stored in the configuration of Couchbase. If your server receives a new address from the DHCP server, Couchbase will not work anymore. In this article you will see how you can configure Couchbase to use another IP address or Hostname.\\n\\n**Important:** The steps that follow will completely destroy any data and configuration from the node, so it is best to start with a fresh Couchbase install. If you can not, you should backup your data using the file based backup-restore documented [here](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-backup-restore.html).\\n\\n\x3c!-- truncate --\x3e\\n\\n### Setting up a hostname in hosts file\\n\\nThe best practice is to register Couchbase using a hostname instead of an IP Address. For this you will need to associate this hostname to an IP address in the `hosts` file.\\n\\nSince the `hosts` file is part of the system, *you need to edit it as administrator*. You have different approaches to achieve this:\\n\\nThe following steps explain the easiest way to do it:  \\n\\n1.  Click \u201cStart Menu\u201d\\n2.  Navigate to \u201cAll Programs &gt; Accessories\u201d\\n3.  \u201cRight Click\u201d on \u201cNotepad\u201d (or your favorite text editor)\\n4.  Click \u201cRun as Administrator\u201d\\n\\n![]( http://4.bp.blogspot.com/-gxd-rAWLqd8/ULy-vIxak3I/AAAAAAAAAZc/00pNi3Iu0uc/s320/Screen+Shot+2012-12-03+at+3.57.16+PM.png )\\n\\nYou can now open the file `C:\\\\Windows\\\\System21\\\\drivers\\\\etc\\\\hosts`\\n\\nAdd a new entry with a host name that will be used by Couchbase Server for example something like :\\n\\n`127.0.0.1    couchbase-node1.mycompany.com`\\n\\nHere I am using the local address (127.0.0.1) like that I won\'t have to change it even when my IP address changes. (This is useful when you are working in a single node mode)\\n\\n### Configure Couchbase to use the hostname or new IP address\\n\\nDuring the installation Couchbase has been registered as a Windows Service. To be able to associate Couchbase to the new hostname (or IP address) the service needs to re-configured and reinstalled.\\n\\nThis could be done using the scripts provided with the product. To run the scripts you need to do it as administrators, you can do it with one of the following methods:\\n\\n* Search for the file and right click and select `Run as Administrator` (documented below)\\n* Run the terminal as administrator and run all the command from there (documented below)\\n* Search for the file and run it using `Ctrl+Shift+Enter`\\n\\n#### Option 1 : Using the Start Menu and Search Program\\n\\n**Stop Couchbase Server Windows Service**\\n\\nThe first thing to do, is to stop this service that is automatically started after the installation:\\n\\n1.  Click `Start Menu`\\n2.  Type `Services` in the Search Program form\\n3.  Click on `Services`\\n4.  In the Services Application navigate to `CouchbaseServer`\\n5.  Right Click and Click on `Stop`\\n6.  Couchbase is now stopped.\\n\\n**Edit the Service Register script**\\n\\nNote: Due to a small formatting issue (See [MB-7322](http://www.couchbase.com/issues/browse/MB-7322)), Notepad could not be used, a solution is to take [Notepad++](http://notepad-plus-plus.org/) or any other advanced editing tool.\\n\\n\\n1.  As Administrator, open the `C:\\\\Program Files\\\\Couchbase\\\\Server\\\\bin\\\\service_register.bat` file with your favorite editor. To open the editor as Administrator you can use the approach described in the previous step.\\n2.  Edit the line 9 to replace `%IP_ADDR%` by your hostname, the line should look like: `NS_NAME=ns_1@couchbase-node1.mycompany.com`\\n3.  Save the file\\n\\n**Delete existing configuration and logs**\\n\\n1.  Using the file explorer, go into: `C:\\\\Program Files\\\\Couchbase\\\\Server\\\\var\\\\lib\\\\couchbase\\\\mnesia`\\n2.  Delete its content (Select All and Right Click)\\n\\n\\n**Register the new Configuration as Service**\\n\\n1.  Using the file explorer, go into: `C:\\\\Program Files\\\\Couchbase\\\\Server\\\\bin`\\n2.  Right Click on `service_reregister.bat`\\n3.  Click on `Run as Administrator`\\n\\nThis script recreates the Couchbase Server Windows Service and starts it automatically.\\n\\n\\n**Check the configuration**\\n\\n1.  Launch your Internet Browser\\n2.  Go to http://localhost:8091\\n3.  Follow the Couchbase Installation Steps\\n4.  Once install connect to the console\\n5.  Go to `Server Nodes` tab\\n6.  Check that the server name is now `couchbase-node1.mycompany.com`\\n\\n![]( http://3.bp.blogspot.com/-tAn4jv_cdrk/ULzOA53roDI/AAAAAAAAAZ0/r8gts5uSdbU/s320/Screen+Shot+2012-12-03+at+5.06.07+PM.png )\\n\\nYour Couchbase node is now configured to use the hostname of your server.\\n\\n\\n#### Option 2 : Using the Command Line\\n\\n**Launch Command Prompt as Administrator**\\n\\n1.  Click `Start Menu`\\n2.  Type `Command Prompt` in the Search program form\\n3.  Type `Ctrl+Shift+Enter`\\n4.  Go to `C:\\\\Program Files\\\\Couchbase\\\\Server\\\\bin` (or other location if you have chosen another location during installation)\\n\\nYou are now ready to do the administration tasks.\\n\\n1.  Execute the `service_stop.bat`\\n2.  Edit the Service Register script\\n\\n1.  Open `service_register.bat`\\n2.  Edit the line 9 to replace `%IP_ADDR%`` by your hostname (or your IP address), the line should look like: `set NS_NAME=ns_1@couchbase-node1.mycompany.com`\\n3.  Save the file3.  Delete the content of: `C:\\\\Program Files\\\\Couchbase\\\\Server\\\\var\\\\lib\\\\couchbase\\\\mnesia`\\n4.  Execute the `service_reregister.bat\\n\\nThis script recreates the Couchbase Server Windows Service and starts it automatically.\\n\\n**Check the configuration**\\n\\n1.  Launch your Internet Browser\\n2.  Go to http://localhost:8091\\n3.  Follow the Couchbase Installation Steps\\n4.  Once install connect to the console\\n5.  Go to `Server Nodes` tab\\n6.  Check that the server name is now `couchbase-node1.mycompany.com`\\n\\n![]( http://3.bp.blogspot.com/-tAn4jv_cdrk/ULzOA53roDI/AAAAAAAAAZ0/r8gts5uSdbU/s320/Screen+Shot+2012-12-03+at+5.06.07+PM.png )\\n\\nYour Couchbase node is now configured to use the hostname of your server."},{"id":"/2012/12/01/new-adventure-dot-dot-dot","metadata":{"permalink":"/blog/2012/12/01/new-adventure-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-12-01-new-adventure-dot-dot-dot.md","source":"@site/blog/2012-12-01-new-adventure-dot-dot-dot.md","title":"New Adventure...","description":"Yesterday was my last day at eXo... I have been working at eXo since 2008, and we have achieved many exciting things such as building eXo Platform, the open source social platform, and the Cloud-IDE allowing developers to build, test, and deploy applications online.","date":"2012-12-01T00:00:00.000Z","formattedDate":"December 1, 2012","tags":[],"readingTime":1.09,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New Adventure...","categories":"couchbase nosql"},"prevItem":{"title":"What to do if your Couchbase Server does not start?","permalink":"/blog/2012/12/26/what-to-do-if-your-couchbase-server-does-not-start"},"nextItem":{"title":"Couchbase : Create a large dataset using Twitter and Java","permalink":"/blog/2012/11/05/couchbase-create-a-large-dataset-using-twitter-and-java"}},"content":"Yesterday was my last day at eXo... I have been working at eXo since 2008, and we have achieved many exciting things such as building [eXo Platform](http://www.exoplatform.com/), the open source social platform, and the [Cloud-IDE](http://www.cloud-ide.com/) allowing developers to build, test, and deploy applications online.\\n\\nIt was a great experience for me, but it is time for me to jump into a new adventure...\\n\\nI am joining [Couchbase](http://www.couchbase.com/)&nbsp;as a Technical Evangelist for the EMEA Region. When I have started to work with NoSQL engines (starting with Google BigTable for [Resultri](http://www.resultri.com/)) I really enjoyed the experience and the flexibility that it gives to the developers. Then I choose to work on a documented oriented database because it looks very natural to me, and evaluate the clustering capabilities.&nbsp;This is how I discovered&nbsp;[Couchbase](http://www.couchbase.com/)&nbsp;and its community.\\n\\nThis new job is a great opportunity for me to do the things that I really like about software:\\n\\n*   Coding applications using different languages and frameworks\\n*   Understanding the sysops/devops related challenges when using a new product/technology\\n*   And finally probably the most important part; sharing it with others!\\n\\nI look forward to sharing what I like about NoSQL and Couchbase and discuss with others about their experience or needs around NoSQL.&nbsp;\\n\\n\\nSee you soon online and in real life during conferences and meetups!\\n\\n![Couchbase Logo](http://4.bp.blogspot.com/-jxz50giRSQM/ULnaWxkHkyI/AAAAAAAAAZM/s0y7CFw4jUk/s320/couch-logo-company-page.png)"},{"id":"/2012/11/05/couchbase-create-a-large-dataset-using-twitter-and-java","metadata":{"permalink":"/blog/2012/11/05/couchbase-create-a-large-dataset-using-twitter-and-java","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-11-05-couchbase-create-a-large-dataset-using-twitter-and-java.md","source":"@site/blog/2012-11-05-couchbase-create-a-large-dataset-using-twitter-and-java.md","title":"Couchbase : Create a large dataset using Twitter and Java","description":"An easy way to create large dataset when playing/demonstrating Couchbase -or any other NoSQL engine- is to inject Twitter feed into your database.","date":"2012-11-05T00:00:00.000Z","formattedDate":"November 5, 2012","tags":[{"label":"couchbase","permalink":"/blog/tags/couchbase"},{"label":"nosql","permalink":"/blog/tags/nosql"},{"label":"java","permalink":"/blog/tags/java"},{"label":"twitter","permalink":"/blog/tags/twitter"}],"readingTime":3.025,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Couchbase : Create a large dataset using Twitter and Java","tags":["couchbase","nosql","java","twitter"]},"prevItem":{"title":"New Adventure...","permalink":"/blog/2012/12/01/new-adventure-dot-dot-dot"},"nextItem":{"title":"Create a Simple Node.js and Couchbase application... on OS X","permalink":"/blog/2012/09/24/create-a-simple-node-dot-js-and-couchbase-application-dot-dot-dot-on-os-x"}},"content":"import Gist from \'react-gist\';\\n\\nAn easy way to create large dataset when playing/demonstrating Couchbase -or any other NoSQL engine- is to inject Twitter feed into your database.\\n\\nFor this small application I am using:\\n\\n* [Couchbase Server 2.0 Server](http://www.couchbase.com/downloads-all#couchbase-server-2-0)\\n* [Couchbase Java SDK](http://www.couchbase.com/develop/java/next) (will be installed by Maven)\\n* [Twitter4J](http://twitter4j.org/) (will be installed by Maven)\\n* [Twitter Streaming API](https://dev.twitter.com/docs/streaming-apis) called using Twitter4J\\n\\nIn this example I am using Java to inject Tweets into Couchbase, you can obviously use another langage if you want to.\\n\\nThe sources of this project are available on my Github repository [ Twitter Injector for Couchbase](https://github.com/tgrall/couchbase-twitter-injector) you can also download the Binary version [here](https://github.com/downloads/tgrall/couchbase-twitter-injector/CouchbaseTwitterInjector.jar), and execute the application from the command line, see [Run The Application paragraph](http://www.blogger.com/blogger.g?blogID=785895453418216075#runTheApp). Do not forget to create your Twitter oAuth keys (see next paragraph)\\n\\n#### Create oAuth Keys\\n\\nThe first thing to do to be able to use the Twitter API is to create a set of keys. If you want to learn more about all these keys/tokens take a look to the oAuth protocol : [http://oauth.net/](http://oauth.net/)\\n\\n\\n1- Log in into the Twitter Development Portal : [https://dev.twitter.com/](https://dev.twitter.com/)\\n\\n\\n2- Create a new Application\\n\\nClick on the \\"Create an App\\" link or go into the \\"User Menu &gt; My Applications &gt; Create a new application\\"\\n\\n3- Enter the Application Details information\\n\\n![]( http://4.bp.blogspot.com/-zxOaF_YLaO0/UJhoZOhjoJI/AAAAAAAAAVk/7HnRfFRSKxs/s320/couchbase-twitter-001.png )\\n\\n4- Click \\"Create Your Twitter Application\\" button\\n\\nYour application\'s OAuth settings are now available :\\n\\n![]( http://3.bp.blogspot.com/-ZdaST7c-HdY/UJhqi8WJMGI/AAAAAAAAAVs/ndeXQwc4F0k/s320/couchbase-twitter-002.png )\\n\\n\\n5- Go down on the Application Settings page and click on the \\"Create My Access Token\\" button\\n\\n![]( http://4.bp.blogspot.com/-WIW2AN93n6k/UJhrfZV1AVI/AAAAAAAAAV0/OseQ1EOpg9k/s320/couchbase-twitter-003.png )\\n\\nYou have now all the necessary information to create your application:\\n\\n* Consumer key\\n* Consumer secret\\n* Access token\\n* Access token secret\\n\\nThese keys will be uses in the `twitter4j.properties` file when running the Java application from the command line\\n\\n#### Create the Java Application\\n\\nThe following code is the main code of the application:\\n\\n<Gist id=\\"4022377\\" \\n/>\\n\\nSome basic explanation:\\n\\n* The `setUp()` method simply reads the `twitter4j.properties` file from the classpath to build the Couchbase connection string.\\n* The `injectTweets` opens the Couchbase connection -line 76- and calls the TwitterStream API.\\n* A Listener is created and will receive all the `onStatus(Status status)` from Twitter. The most important method is onStatus() that receive the message and save it into Couchbase.\\n* One interesting thing : since Couchbase is a JSON Document database it allows your to just take the JSON String and save it directly. `cbClient.add(idStr,0 ,twitterMessage);`\\n\\n\\n\\n#### Packaging\\n\\nTo be able to execute the application directly from the Jar file, I am using the assembly plugin with the following informations from the pom.xml:\\n\\n``` xml\\n  ...\\n<archive>\\n<manifest>\\n<mainclass>com.couchbase.demo.TwitterInjector</mainclass>\\n</manifest>\\n<manifestentries>\\n<class-path>.</class-path>\\n</manifestentries>\\n</archive>\\n...\\n```\\n\\nSome information:\\n\\n* The mainClass entry allows you to set which class to execute when running java -jar command.\\n* The Class-Path entry allows you to set the current directory as part of the classpath where the program will search for the twitter4j.properties file.\\n* The assembly file is also configure to include all the dependencies (Twitter4J, Couchbase client  SDK, ...)\\n\\nIf you do want to build it from the sources, simply run :\\n\\n``` sh\\nmvn clean package\\n```\\nThis will create the following Jar file ./target/CouchbaseTwitterInjector.jar\\n\\n\\n#### Run the Java Application\\n\\nBefore running the application you must create a twitter4j.properties file with the following information :\\n\\n``` sh\\ntwitter4j.jsonStoreEnabled=true\\n\\noauth.consumerKey=[YOUR CONSUMER KEY]\\noauth.consumerSecret=[YOUR CONSUMER SECRET KEY]\\noauth.accessToken=[YOUR ACCESS TOKEN]\\noauth.accessTokenSecret=[YOUR ACCESS TOKEN SECRET]\\n\\ncouchbase.uri.list=http://127.0.0.1:8091/pools\\ncouchbase.bucket=default\\ncouchbase.password=\\n```\\n\\nSave the properties file and from the same location run:\\n\\n``` sh\\njar -jar [path-to-jar]/CouchbaseTwitterInjector.jar\\n```\\n\\nThis will inject Tweets into your Couchbase Server. Enjoy !"},{"id":"/2012/09/24/create-a-simple-node-dot-js-and-couchbase-application-dot-dot-dot-on-os-x","metadata":{"permalink":"/blog/2012/09/24/create-a-simple-node-dot-js-and-couchbase-application-dot-dot-dot-on-os-x","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-09-24-create-a-simple-node-dot-js-and-couchbase-application-dot-dot-dot-on-os-x.md","source":"@site/blog/2012-09-24-create-a-simple-node-dot-js-and-couchbase-application-dot-dot-dot-on-os-x.md","title":"Create a Simple Node.js and Couchbase application... on OS X","description":"NOTE: The Couchbase Node.js Client Library is currently changing. I will update this article and source code once the API is stable.","date":"2012-09-24T00:00:00.000Z","formattedDate":"September 24, 2012","tags":[],"readingTime":6.785,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Create a Simple Node.js and Couchbase application... on OS X","categories":"nodejs javascript couchbase nosql"},"prevItem":{"title":"Couchbase : Create a large dataset using Twitter and Java","permalink":"/blog/2012/11/05/couchbase-create-a-large-dataset-using-twitter-and-java"},"nextItem":{"title":"Couchbase 101 : install, store and query data","permalink":"/blog/2012/07/06/couchbase-101-install-store-and-query-data"}},"content":"> **NOTE:** The Couchbase Node.js Client Library is currently changing. I will update this article and source code once the API is stable.\\n\\n\\nI am currently playing a little bit with Node.js . It is quite fun! In this article I won\'t go in a a very complex application but just give you the basic steps to create your first Node.js+Couchbase application... on Mac OS X.\\n\\n### Installation\\n\\n**Couchbase 2.0 Beta:**\\n\\nYou can take a look the first steps of my [previous article](http://tugdualgrall.blogspot.fr/2012/07/couchbase-101-install-store-and-query.html) to install Couchbase. The basics steps are:\\n\\n* Download [Couchbase 2. 0 Beta](http://www.couchbase.com/couchbase-server/beta)\\n* Start the server (Run the \\"Couchbase Server\\" application)\\n* Configure the server\\n\\n**Other Components :**\\n\\n* Node.js\\n* Couchbase Client Library (C version)\\n\\n\x3c!-- truncate --\x3e\\n\\nTo install these two components I am using homebrew (aka brew).\\n\\n*_Hmmm, what is homebrew?*\\n\\n\\nHomebrew is a package manager for OS X that allows you to install, update and uninstall unix tools using very simple commands. You can find more information on the [homebrew site](http://mxcl.github.com/homebrew/). So let\'s start by installing homebrew itself.\\n\\nFrom a terminal window:\\n\\n``` sh\\nruby -e \\"$(curl -fsSkL raw.github.com/mxcl/homebrew/go)\\"\\n\\n```\\nThen let\'s install node\\n\\n``` sh\\nbrew install node\\n```\\n\\nand finally install Couchbase Client Library\\n\\n``` sh\\nbrew install https://github.com/couchbase/homebrew/raw/preview/Library/Formula/libcouchbase.rb\\n```\\n\\n### Coding\\n\\nYou are now ready to start the development of your application.\\n\\n#### Create a very simple application\\n\\n1- Create a folder for your application\\n\\n``` sh\\nmkdir sample-app\\ncd sample-app\\n```\\n\\n2- Create an `app.js` file and copy the following code:\\n\\n``` javascript\\nvar http = require(\\"http\\");\\n\\nfunction onRequest(request, response) {\\n  response.writeHead(200, {\\"Content-Type\\": \\"text/plain\\"});\\n  response.write(\\"Hello World\\");\\n  response.end();\\n}\\n\\nvar server = http.createServer(onRequest);\\nserver.listen(8080);\\n\\nconsole.log(\\"&gt; SERVER STARTED\\");\\n```\\n\\nI won\'t go in all the details about a node application; for this I invite you to read [The Node Beginnger Book](http://www.nodebeginner.org/).\\n\\n3- Start your server\\n\\n``` javascript\\nnode app.js\\n```\\n\\nYou should be able to access your application using http://localhost:8080\\n\\nTo stop your server just use ctrl+c.\\n\\n\\n#### Install and use Couchbase client for node.js\\n\\n\\nnpm is the [node package manager](https://npmjs.org/), that allows you to easily install node.js modules and manage dependencies for your application (I will present the dependency management in another post)\\n\\n1- Install the Couchbase Client for node.js, using the following command\\n\\n``` sh\\nnpm install couchbase\\n```\\n\\nCouldn\'t be simpler! You can find more information about Couchbase Client for node on its [npm page](https://npmjs.org/package/couchbase).\\n\\n\\n2- Lets now insert some data into Couchbase\\n\\nCreate a simple function that inserts some documents, and call it after server startup:\\n\\n``` js\\nvar http = require(\\"http\\");\\n\\n// load the Couchbase driver and connect to the cluster\\nvar driver = require(\'couchbase\');\\nvar cb = new driver.Couchbase(\\"localhost:8091\\", null, null, \\"default\\");\\n\\n...\\n...\\n\\nfunction insertData() {\\n  // insert employees in Couchbase\\n  var emps = [{\\n    \\"type\\": \\"employee\\",\\n    \\"id\\": 100,\\n    \\"name\\": \\"Thomas\\",\\n    \\"dept\\": \\"Sales\\",\\n    \\"salary\\": 5000\\n    }, {\\n      \\"type\\": \\"employee\\",\\n      \\"id\\": 200,\\n      \\"name\\": \\"John\\",\\n      \\"dept\\": \\"Development\\",\\n      \\"salary\\": 4500\\n      }, {\\n        \\"type\\": \\"employee\\",\\n        \\"id\\": 300,\\n        \\"name\\": \\"Jane\\",\\n        \\"dept\\": \\"Marketing\\",\\n        \\"salary\\": 5000\\n        }]\\n\\n        // Insert the data in Couchbase using the add method ()\\n        for (index = 0; index &lt; emps.length; index++) {\\n          cb.add(JSON.stringify(emps[index].id), JSON.stringify(emps[index]), 0, undefined, function(data, err, key, cas) {\\n            if (err &amp;&amp; err != 12) { // 12 : LCB_KEY_EEXISTS  \\n              console.log(\\"Failed to store object:\\\\n\\" + err);\\n            }\\n            });\\n          }\\n        }\\n\\n        server.listen(8080, insertData());\\n```\\n\\n\\n* Lines 4-5 : load the Couchbase driver and connect to the server. I am using the complete list of parameter `connect(\\"server:port\\", \\"username\\", \\"password\\", \\"bucket\\"). But you can use the short version `connect(\\"server:port\\")`\\n* Lines 12-30 : just create JSON object that will be pushed in Couchbase.\\n* Line 33 : the application just read each element of the array and insert them into Couchbase using the `couchbase.add()` function.\\n* Lines 34-38 : the `couchbase.add()` function set the value only if it does not exist. If the value exists the error code LCB_KEY_EEXISTS (12) is return by the callback function\\n\\n3- Start your server - `node app.js` - and check using the Admin Console that employees are inserted into your Couchbase instance. Note that node.js applications do not support hot deployment, so you need to bounce your application when changing the code.\\n\\n\\n#### Create and use Couchbase View\\n\\nLet\'s now create and use a view to return the employee list.\\n\\n1- All Couchbase views are accessible using a simple REST API, but you can also use the node.js plugin : [baseview](https://npmjs.org/package/baseview); so let\'s install this module:\\n\\n``` sh\\nnpm install baseview\\n```\\n\\n2. Create a new view from your application\\n\\nYou can use the Admin Console to create the view, but it is also possible to do it from your node.js code. So let\'s add the view programmatically in the insertData function.\\n\\n``` js\\nvar http = require(\\"http\\");\\n\\n// load the Couchbase driver and connect to the cluster\\nvar driver = require(\'couchbase\');\\nvar cb = new driver.Couchbase(\\"localhost:8091\\", null, null, \\"default\\");\\n// load the baseview module\\nvar baseview = require(\'baseview\')({url: \'http://localhost:8092\', bucket: \'default\'});\\n\\n...\\n...\\n\\nfunction insertData() {\\n\\n  //create a new view\\n  baseview.setDesign(\'design_employees\', {\\n    \'basic_list\': {\\n      \'map\': \\"function (doc, meta) { if(doc.type == \'employee\') {emit(meta.id, doc.name);}}\\"\\n    }\\n  },\\n  function(err, res){\\n    if (err != null) console.log(err);\\n  }\\n);\\n\\n...\\n\\n}\\n...\\n\\n```\\n\\n\\n\\nThis new code, create a new view in Couchbase server:\\n\\n* line 7 : load the module and set the properties to call the view services\\n* line 16-17 : call the `basevie.setDesign()` method, that create a view.\\n* line 18 : set the map function that list all the employees\\n\\n3- Let\'s now call the view in the onRequest function.\\n\\n``` js\\nfunction onRequest(request, response) {\\n  response.writeHead(200, {\\n    \\"Content-Type\\": \\"text/plain\\"\\n  });\\n  response.write(\\"See list of employees in the console\\");\\n  var params =\\n{ \'descending\'  : \'true\'\\n, \'include_docs\' : \'true\'\\n};\\nbaseview.view(\'design_employees\', \'basic_list\', params, function(error, data) {\\n  for( var i = data.rows.length-1,length = data.rows.length ; i >= 0; i-- ) {\\n    var employee = data.rows[i].doc.json;\\n    console.log(employee);\\n  }\\n});\\nresponse.end();\\n}\\n```\\n\\nCalling the view is quite simple :\\n\\n* lines 6-8 : create an object to send view parameters. In this example I am just using descending, and include_docs to get the full document as part of the response. You can find the list of all the parameters you can use in the [Couchbase documentation : Querying Using the REST API](http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-views-querying-rest-api.html) (The [baseview](https://github.com/PatrickHeneise/baseview/blob/master/baseview.js) module is using REST API to call the views).\\n* line 10-14 : just loop on the result content, returned in the data variable, and print the employee information in the console.\\n\\nNote: Because of the asynchronous nature of node.js, and my lack of experience with node, I was not able to send the list of employee to the HTTP response.\\n\\nIn another article I will explain how to integrate Couchbase with an node.js application based on Express and Socket.io, where I list the Employee in my the Web page.\\n\\nBelow, the complete code of this simple node.js application:\\n\\n``` js\\nvar http = require(\\"http\\");\\nvar driver = require(\'couchbase\');\\nvar cb = new driver.Couchbase(\\"localhost:8091\\", null, null, \\"default\\");\\nvar baseview = require(\'baseview\')({url: \'http://127.0.0.1:8092\',bucket: \'default\'});\\n\\nfunction onRequest(request, response) {\\n  response.writeHead(200, {\\n    \\"Content-Type\\": \\"text/plain\\"\\n  });\\n  response.write(\\"See list of employees in the console\\");\\n  var params = {\\n    \'descending\': \'true\',\\n    \'include_docs\': \'true\'\\n  };\\n  baseview.view(\'design_employees\', \'basic_list\', params, function(error, data) {\\n    for (var i = data.rows.length - 1, length = data.rows.length; i >= 0; i--) {\\n      var employee = data.rows[i].doc.json;\\n      console.log(employee);\\n    }\\n  });\\n  response.end();\\n}\\n\\nfunction insertData() {\\n  //create a new view\\n  baseview.setDesign(\'design_employees\', {\\n    \'basic_list\': {\\n      \'map\': \\"function (doc, meta) { if(doc.type == \'employee\') {emit(meta.id, doc.name);}}\\"\\n    }\\n  }, function(err, res) {\\n    if (err != null) console.log(err);\\n  });\\n\\n  // insert employees in Couchbase\\n  var emps = [{\\n    \\"type\\": \\"employee\\",\\n    \\"id\\": 100,\\n    \\"name\\": \\"Thomas\\",\\n    \\"dept\\": \\"Sales\\",\\n    \\"salary\\": 5000\\n  }, {\\n    \\"type\\": \\"employee\\",\\n    \\"id\\": 200,\\n    \\"name\\": \\"John\\",\\n    \\"dept\\": \\"Development\\",\\n    \\"salary\\": 4500\\n  }, {\\n    \\"type\\": \\"employee\\",\\n    \\"id\\": 300,\\n    \\"name\\": \\"Jane\\",\\n    \\"dept\\": \\"Marketing\\",\\n    \\"salary\\": 5000\\n    }]\\n\\n\\n    // Insert the data in Couchbase using the add method ()\\n    for (index = 0; index < emps.length; index++) {\\n      cb.add(JSON.stringify(emps[index].id), JSON.stringify(emps[index]), 0, undefined, function(data, err, key, cas) {\\n        if (err && err != 12) { // 12 : LCB_KEY_EEXISTS  \\n          console.log(\\"Failed to store object:\\\\n\\" + err);\\n        }\\n        });\\n      }\\n    }\\n\\n    var server = http.createServer(onRequest);\\n\\n    server.listen(8080, insertData());\\n\\n    console.log(\\"> SERVER STARTED\\");\\n```"},{"id":"/2012/07/06/couchbase-101-install-store-and-query-data","metadata":{"permalink":"/blog/2012/07/06/couchbase-101-install-store-and-query-data","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-07-06-couchbase-101-install-store-and-query-data.md","source":"@site/blog/2012-07-06-couchbase-101-install-store-and-query-data.md","title":"Couchbase 101 : install, store and query data","description":"Introduction","date":"2012-07-06T00:00:00.000Z","formattedDate":"July 6, 2012","tags":[],"readingTime":7.01,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Couchbase 101 : install, store and query data","categories":"nosql couchbase rest json"},"prevItem":{"title":"Create a Simple Node.js and Couchbase application... on OS X","permalink":"/blog/2012/09/24/create-a-simple-node-dot-js-and-couchbase-application-dot-dot-dot-on-os-x"},"nextItem":{"title":"eXo Platform: Internationalization of content","permalink":"/blog/2012/05/11/exo-platform-internationalization-of-content"}},"content":"## Introduction\\n\\nIn this post I just want to show how easily is to get\\nstarted with Couchbase, and also explain how to \u201cquery\u201d the data. The basic\\nsteps of this tutorial are:\\n\\n1.  Install Couchbase\\n2.  Create Data\\n3.  Query Data\\n\\nI will try to post more articles, if I have time to show how to use Couchbase from your applications (starting with Java).\\n\\nPrerequisites :\\n\\n* Could not be simpler : Couchbase 2.0 available [here](http://www.couchbase.com/downloads-all). (Currently in Developer Preview)\\n\\n## Couchbase 101 : Insert and Query data\\n\\n### Installation\\n\\nI am using Couchbase on Mac OS X, so let me describe the installation in this environment. If you are using other operating system just take a look to the Couchbase documentation.\\n\\nCouchbase installation is very (very!) fast:\\n\\n1.  Download the Mac OS X Zip file.\\n2.  Double-click the downloaded Zip installation file to extract the contents. This will create a single file, the Couchbase.app application.\\n3.  Drag and Drop the Couchbase.app to your chosen installation folder, such as the system Applications folder.\\n\\n### Start and Configure Couchbase Server\\n\\nTo start Couchbase Server, just double click on the Couchbase Server. Once the server is started, a new icon is added in the OS X Menu to indicate that the Server is up and running.\\n\\nYou can now configure your Couchbase instance, for this you just need to access the Admin Console, available at the following location [http://127.0.0.1:8091](http://127.0.0.1:8091/) (change the IP address if needed) or simply by going in the Couchbase menu and click on Open Admin Console entry.\\n\\n![](http://4.bp.blogspot.com/-UokiHs1DlFw/T_VpJHmn8KI/AAAAAAAAAUo/oy7bh5w9nOw/s1600/couchbase-menu.png )\\n\\n\\n1.  Welcome Screen : Click Setup\\n2.  Set the disk and cluster configuration. On my instance I keep the default location for the on disk storage. Just configure the size of the memory usage for your instance, for example 800Mb. So far, we have a single instance, so no need to join a cluster.\\n3.  Choose to generate sample data. This will be interesting to learn more about data and views.\\n4.  Create `the default` bucket (use for testing only). A bucket is used by Couchbase to store data. It could be compared to a \u201cdatabase\u201d in RDBMS world.\\n5.  Configure update notifications to be alerted when new version of Couchbase is released\\n6.  Configure the server with a final step with the administrator username and password\\n7.  When this is done you are automatically redirected to the Admin Console.\\n\\n![](http://2.bp.blogspot.com/-_a1iynqdk8w/T_VpTwa5qEI/AAAAAAAAAUw/ZGBHZsC6x_8/s320/install-step8.png )\\n\\nThis is it! You are ready to use your Couchbase server.\\n\\nCouchbase has many interesting features, especially around scalability and elasticity but for not in this article let\'s focus on the basics :\\n\\n* Insert some data and query them\\n\\n### Insert Data\\n\\nCouchbase has many ways to manipulate data from you favorite programming language using the different client libraries : Java, Python, PHP, Ruby, .Net, C. For now let\'s use the Admin Console to create and query data.\\n\\nCouchbase can store any type of data, but when you need to manipulate some data with a structure the best way is to use JSON Documents. So let\'s use the console and create documents.\\n\\nTo create new documents in your database, click on the \\"Data Buckets\\" tab. If you have installed the sample you see 2 buckets: `default` and `gamesim-sample`.\\n\\nLet\'s create a new documents in the `default` bucket:\\n\\n1.  Click on Documents button\\n2.  Click on Create Document\\n3.  Since each document must have an id for example 100.\\n4.  Couchbase save the document and add some metadata such as &#95;rev, $flags, expiration\\n5.  Add new attributes to the document that describe an employee : Name, Departement and Salary, then save it. You just need to update the JSON object with values\\n\\n``` json\\n{\\n      \\"_id\\": \\"100\\",\\n      \\"name\\": \\"Thomas\\",\\n      \\"dept\\": \\"Sales\\",\\n      \\"salary\\": 5000\\n  }\\n```\\n\\nRepeat the operation with some other employees :\\n\\n``` java\\n  200,Jason,Technology,5500\\n  300,Mayla,Technology,7000\\n  400,Nisha,Marketing,9500\\n  500,Randy,Technology,6000\\n  501,Ritu,Accounting,5400\\n```\\n\\nYou have now a list of employees in your database. That was easy isn\'t? Let\'s now query them.\\n\\n### Query Data\\n\\nAccess document directly from its ID\\n\\nFirst of all you can quickly access a document using a simple HTTP request using its id. For example to access the Mayla with the id 300 just enter the following URL:\\n\\n*   `http://127.0.0.1:8092/default/300`  \\n\\nIn this URL you have :\\n\\n* `8092` is the Couch API REST port used to access data (where 8091 is the port for the Admin console)\\n* `default` is the bucket in which the document is stored\\n* `300` is the id of the document\\n\\n#### Search your data with queries\\n\\nSo we have seen how you can access one document. But what if my need is :\\n\\n* \\"Give me all the employee of the Technology department\\"\\n\\n\\nTo achieve such query it is necessary to create views. The views are used by Couchbase server to index and search your data. A view is a Map function written in JavaScript, that will generate a key value list that is compliant with logic you put in the Map function. Note that this key,value is now indexed and sorted by key. This is important when you query your data.\\n\\nSo let\'s create a new view from the Admin Console:\\n\\n1.  Click on the Views tab (be sure you are on the default bucket)\\n2.  Click on the \\"Create Development View\\"\\n3.  Enter the Document and View name:\\n  *   Document Name : _design/dev_dept\\n  *   View Name : dept\\n4.  Cick Save\\n5.  Click on your View to edit it\\n\\nSince we need to provide the list of employees that are part of a the Technology department, we need to create a view that use the <u>department as key</u>, so the map function looks like :\\n\\n``` javascript\\nfunction (doc) {\\n    emit(doc.dept, null);\\n}\\n```\\n\\nSave the view\\n\\nThis function takes the document and create a list that contains the \\"dept\\" as key and null as value. The value itself is not that important in our case. A simple rule will be : do not put too much data in the value since at the end Couchbase server creates an index with this map. Will see that Couchbase allows developer to easily get the document information when accessing a view.\\n\\nClick on the \\"Show Results\\" button, the result will look like:\\n\\n``` json\\n{\\"total_rows\\":6,\\"rows\\":[\\n  {\\"id\\":\\"501\\",\\"key\\":\\"Accounting\\",\\"value\\":null},\\n  {\\"id\\":\\"400\\",\\"key\\":\\"Marketing\\",\\"value\\":null},\\n  {\\"id\\":\\"100\\",\\"key\\":\\"Sales\\",\\"value\\":null},\\n  {\\"id\\":\\"200\\",\\"key\\":\\"Technology\\",\\"value\\":null},\\n  {\\"id\\":\\"300\\",\\"key\\":\\"Technology\\",\\"value\\":null},\\n  {\\"id\\":\\"500\\",\\"key\\":\\"Technology\\",\\"value\\":null}\\n  ]\\n}\\n```\\n\\nAs we have seen in earlier it is possible to access the document using a single URL, it is the same for views. You can for example access the view we have just created using the following URL:\\n\\n* [http://127.0.0.1:8092/default/_design/dev_dept/_view/dept\\n](http://127.0.0.1:8092/default/_design/dev_dept/_view/dept)\\n\\nNow it is possible to use query parameter to filter the results using the key parameter with the value entered using a JSON String :\\n\\n* [http://127.0.0.1:8092/default/_design/dev_dept/_view/dept?key=\\"Technology\\"](http://127.0.0.1:8092/default/_design/dev_dept/_view/dept?key=%22Technology%22)\\n\\nThe result of this query is now :\\n\\n``` json\\n{\\"total_rows\\":6,\\"rows\\":[\\n  {\\"id\\":\\"200\\",\\"key\\":\\"Technology\\",\\"value\\":null},\\n  {\\"id\\":\\"300\\",\\"key\\":\\"Technology\\",\\"value\\":null},\\n  {\\"id\\":\\"500\\",\\"key\\":\\"Technology\\",\\"value\\":null}\\n  ]\\n}\\n```\\n\\nYou have many other parameters you can use when accessing a view to control the size, the time out, .... One of them is quite interesting is include_docs that ask Couchbase to include the full content of the document in the result. So if you call :\\n\\n* [http://127.0.0.1:8092/default/_design/dev_dept/_view/dept?key=\\"Technology\\"&amp;include_docs=true](http://127.0.0.1:8092/default/_design/dev_dept/_view/dept?key=%22Technology%22&amp;include_docs=true)\\n\\nThe result is :\\n\\n``` json\\n{\\"total_rows\\":6,\\"rows\\":[\\n  {\\"id\\":\\"200\\",\\"key\\":\\"Technology\\",\\"value\\":null,\\"doc\\":  {\\"_id\\":\\"200\\",\\"_rev\\":\\"1-1de6e6751608eada0000003200000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Jason\\",\\"dept\\":\\"Technology\\",\\"salary\\":5500}},\\n  {\\"id\\":\\"300\\",\\"key\\":\\"Technology\\",\\"value\\":null,\\"doc\\":{\\"_id\\":\\"300\\",\\"_rev\\":\\"1-f3e44cee742bfae10000003200000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Mayla\\",\\"dept\\":\\"Technology\\",\\"salary\\":7000}},\\n  {\\"id\\":\\"500\\",\\"key\\":\\"Technology\\",\\"value\\":null,\\"doc\\":  {\\"_id\\":\\"500\\",\\"_rev\\":\\"1-05780359aac8f3790000003200000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Randy\\",\\"dept\\":\\"Technology\\",\\"salary\\":6000}}\\n]\\n}\\n```\\n\\nLet\'s now create a little more complicated view to answer the following business requirement:\\n\\n* \\"Give me all the employee with a salary between 5000 and 6000\\"\\n\\nSo now you know that you need to create a new view with the salary as key let\'s with the following Map function:\\n\\n``` javascript\\nfunction (doc) {\\n  emit(doc.salary, null);\\n}\\n```\\n\\nCouchbase is automatically sorting the key when creating/updating the index so, let\'s use the ` startkey`  and  `endkey` parameter when calling the view. So let\'s call the view with from the following URL:\\n\\n* [http://127.0.0.1:8092/default/_design/dev_salary/_view/salary?startkey=5000&amp;endkey=6000&amp;include_docs=true](http://127.0.0.1:8092/default/_design/dev_salary/_view/salary?startkey=5000&amp;endkey=6000&amp;include_docs=true)\\n\\nThe result is :\\n\\n``` json\\n{\\"total_rows\\":6,\\"rows\\":[\\n {\\"id\\":\\"100\\",\\"key\\":5000,\\"value\\":null,\\"doc\\":{\\"_id\\":\\"100\\",\\"_rev\\":\\"1-0f33580d780014060000002e00000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Thomas\\",\\"dept\\":\\"Sales\\",\\"salary\\":5000}},\\n {\\"id\\":\\"501\\",\\"key\\":5400,\\"value\\":null,\\"doc\\":{\\"_id\\":\\"501\\",\\"_rev\\":\\"1-b1fe5bc79637720e0000003100000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Ritu\\",\\"dept\\":\\"Accounting\\",\\"salary\\":5400}},\\n {\\"id\\":\\"200\\",\\"key\\":5500,\\"value\\":null,\\"doc\\":{\\"_id\\":\\"200\\",\\"_rev\\":\\"1-1de6e6751608eada0000003200000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Jason\\",\\"dept\\":\\"Technology\\",\\"salary\\":5500}},\\n {\\"id\\":\\"500\\",\\"key\\":6000,\\"value\\":null,\\"doc\\":{\\"_id\\":\\"500\\",\\"_rev\\":\\"1-05780359aac8f3790000003200000000\\",\\"$flags\\":0,\\"$expiration\\":0,\\"name\\":\\"Randy\\",\\"dept\\":\\"Technology\\",\\"salary\\":6000}}\\n]\\n}\\n```\\n\\n\\n### Conclusion\\n\\nIn this short article you have learn how to:\\n\\n* Install Couchbase*   Create data using the Admin Console\\n* Query data with views\\n\\nWhen I get more time I will write another article that do the same from Java, and other languages.\\n\\n  * * *\\nNote from @ingenthr\\n\\n> Nice blog! Note that while querying the REST interface directly is okay, we\'ve really tried to make it easy by having high-level language support for queries in each of the official client libraries. They\'re all listed over at http://www.couchbase.com/develop"},{"id":"/2012/05/11/exo-platform-internationalization-of-content","metadata":{"permalink":"/blog/2012/05/11/exo-platform-internationalization-of-content","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-05-11-exo-platform-internationalization-of-content.md","source":"@site/blog/2012-05-11-exo-platform-internationalization-of-content.md","title":"eXo Platform: Internationalization of content","description":"In this screencast I explain the support of multi-lingual content of eXo Platform 3.5. The different features that you can see in this video are:","date":"2012-05-11T00:00:00.000Z","formattedDate":"May 11, 2012","tags":[],"readingTime":0.335,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"eXo Platform: Internationalization of content","categories":"exo portal"},"prevItem":{"title":"Couchbase 101 : install, store and query data","permalink":"/blog/2012/07/06/couchbase-101-install-store-and-query-data"},"nextItem":{"title":"Twitter Boostrap 2 and Google Maps","permalink":"/blog/2012/04/11/twitter-boostrap-2-and-google-maps"}},"content":"In this screencast I explain the support of multi-lingual content of eXo Platform 3.5. The different features that you can see in this video are:\\n\\n*   Support of multiple languages from URL\\n*   Configure the eXo File Explorer to add support for multi-lingual content (new button)\\n*   Content translation\\n*   Add new language to the platform\\n\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/huzPX3UPvrY\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2012/04/11/twitter-boostrap-2-and-google-maps","metadata":{"permalink":"/blog/2012/04/11/twitter-boostrap-2-and-google-maps","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-04-11-twitter-boostrap-2-and-google-maps.md","source":"@site/blog/2012-04-11-twitter-boostrap-2-and-google-maps.md","title":"Twitter Boostrap 2 and Google Maps","description":"Like many developers I am using Twitter Boostrap for my Web applications. Using this framework has been very helpful for me, since I am really not a good HTML/CSS developer. For now, on my site Resultri I am using the default look and feel, will customize it later.","date":"2012-04-11T00:00:00.000Z","formattedDate":"April 11, 2012","tags":[],"readingTime":1,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Twitter Boostrap 2 and Google Maps","categories":null},"prevItem":{"title":"eXo Platform: Internationalization of content","permalink":"/blog/2012/05/11/exo-platform-internationalization-of-content"},"nextItem":{"title":"SAP Cloud Inside : Build and Run Applications in the Cloud","permalink":"/blog/2012/02/19/sap-cloud-inside-build-and-run-applications-in-the-cloud"}},"content":"Like many developers I am using [Twitter Boostrap](http://twitter.github.com/bootstrap/) for my Web applications. Using this framework has been very helpful for me, since I am really not a good HTML/CSS developer. For now, on my site [Resultri](http://www.resultri.com/) I am using the default look and feel, will customize it later.\\n\\nLately, I wanted to integrate Google Map to my application, and when testing it, I had the bad surprise to see that the Controls and WindowInfo are not printed correctly as you can see in the screen shot below:\\n\\n![]( http://2.bp.blogspot.com/-J9_mgk-eCmw/T4VCyQ86jFI/AAAAAAAAAT0/rc6uKm5qyQM/s400/Screen+Shot+2012-04-11+at+10.29.11+AM.png )\\n\\nThis is not a big issue at all, just a conflict on the **`img`** tag and its style (max-width) coming from Twitter Bootstrap. The quick fix :\\n\\n* override the style of the **`img`** tag for the div that contains your map.\\n\\nFor example in my case the div for my map is define as:\\n\\n``` html\\n<div id=\\"resultriMap\\" />\\n```\\n\\nYou just need to add a new style to your page with the following definition:\\n\\n``` css\\n<style>\\n\\n#resultriMap img {  \\n  max-width: none;\\n}\\n\\n</style>\\n```\\n\\nAfter adding this to my page the map is correctly printed as you can see in the following screenshot :\\n\\n![]( http://3.bp.blogspot.com/-BZje60RpKWo/T4VESYjt7gI/AAAAAAAAAT8/Az1xV8Xv2Dg/s320/Screen+Shot+2012-04-11+at+10.29.42+AM.png )"},{"id":"/2012/02/19/sap-cloud-inside-build-and-run-applications-in-the-cloud","metadata":{"permalink":"/blog/2012/02/19/sap-cloud-inside-build-and-run-applications-in-the-cloud","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-02-19-sap-cloud-inside-build-and-run-applications-in-the-cloud.md","source":"@site/blog/2012-02-19-sap-cloud-inside-build-and-run-applications-in-the-cloud.md","title":"SAP Cloud Inside : Build and Run Applications in the Cloud","description":"Last week I was invited to present eXo Cloud IDE during the SAP Cloud Inside. This SAP Community event was a great opportunity to discuss about the cloud with an interesting point of view: the impact of the cloud for SAP customers (especially administrators and developers).","date":"2012-02-19T00:00:00.000Z","formattedDate":"February 19, 2012","tags":[],"readingTime":0.69,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"SAP Cloud Inside : Build and Run Applications in the Cloud","categories":"cloud conference exo"},"prevItem":{"title":"Twitter Boostrap 2 and Google Maps","permalink":"/blog/2012/04/11/twitter-boostrap-2-and-google-maps"},"nextItem":{"title":"eXo Platform : Integrate Twitter and eXo Activity Stream","permalink":"/blog/2012/01/19/exo-platform-integrate-twitter-and-exo-activity-stream"}},"content":"Last week I was invited to present eXo Cloud IDE during the SAP Cloud Inside. This SAP Community event was a great opportunity to discuss about the cloud with an interesting point of view: the impact of the cloud for SAP customers (especially administrators and developers).\\n\\nDuring this presentation I have introduced the [eXo Cloud IDE](http://www.cloud-ide.com/), and I did a demonstration in which O have built and deployed applications : Open Social Gadgets, Ruby on Rails and Java/Spring, and explain how it could be extended to SAP business services.\\n\\nHere the slides that I have used during this presentation:\\n\\n**[SAP Cloud Inside : Develop and Run on the Cloud](http://www.slideshare.net/tgrall/sap-cloud-inside-develop-and-deploy-on-the-cloud \\"SAP Cloud Inside : Develop and Run on the Cloud\\")**\\n\\n\\nRemember that you can register yourself to the [eXo Cloud IDE](http://www.cloud-ide.com/) Service and start develop application from your browser."},{"id":"/2012/01/19/exo-platform-integrate-twitter-and-exo-activity-stream","metadata":{"permalink":"/blog/2012/01/19/exo-platform-integrate-twitter-and-exo-activity-stream","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2012-01-19-exo-platform-integrate-twitter-and-exo-activity-stream.md","source":"@site/blog/2012-01-19-exo-platform-integrate-twitter-and-exo-activity-stream.md","title":"eXo Platform : Integrate Twitter and eXo Activity Stream","description":"eXo Platform 3.5 provides many extension points and API for developers, allowing them to create very cool stuff.","date":"2012-01-19T00:00:00.000Z","formattedDate":"January 19, 2012","tags":[],"readingTime":0.725,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"eXo Platform : Integrate Twitter and eXo Activity Stream","categories":null},"prevItem":{"title":"SAP Cloud Inside : Build and Run Applications in the Cloud","permalink":"/blog/2012/02/19/sap-cloud-inside-build-and-run-applications-in-the-cloud"},"nextItem":{"title":"How to watch YouTube videos offline (on OS X)?","permalink":"/blog/2011/12/05/how-to-watch-youtube-videos-offline-on-os-x"}},"content":"eXo Platform 3.5 provides many extension points and API for developers, allowing them to create very cool stuff.\\n\\nI have developed a small extension that allows any user to associate his Twitter account to his eXo Platform account. This extension simply post on your Twitter account when you write a message with a special hashtag (#tw).\\n\\nThis is a very quick development that I have done while waiting for my kids, so I still have things to integrate to provide complete feature, but this is a good example to show how you can extend the platform.\\n\\nYou can view it in action in this video:\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/KN0jsOdauPU\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\nYou can download the source code and the binaries from this [GitHub project](https://github.com/tgrall/exo-twitter-integration). I hope to find some time to complete the feature, and document this use case.\\n\\nEnjoy!"},{"id":"/2011/12/05/how-to-watch-youtube-videos-offline-on-os-x","metadata":{"permalink":"/blog/2011/12/05/how-to-watch-youtube-videos-offline-on-os-x","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-12-05-how-to-watch-youtube-videos-offline-on-os-x.md","source":"@site/blog/2011-12-05-how-to-watch-youtube-videos-offline-on-os-x.md","title":"How to watch YouTube videos offline (on OS X)?","description":"Lately I have been traveling a lot, and I was not able to access the internet all the time; but I still want to look at some YouTube video, for example the greate Google I/O Sessions...","date":"2011-12-05T00:00:00.000Z","formattedDate":"December 5, 2011","tags":[],"readingTime":0.61,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to watch YouTube videos offline (on OS X)?","categories":null},"prevItem":{"title":"eXo Platform : Integrate Twitter and eXo Activity Stream","permalink":"/blog/2012/01/19/exo-platform-integrate-twitter-and-exo-activity-stream"},"nextItem":{"title":"Installing Memcached on Mac OS X and using it in Java","permalink":"/blog/2011/11/20/installing-memcached-on-mac-os-x-and-using-it-in-java"}},"content":"Lately I have been traveling a lot, and I was not able to access the internet all the time; but I still want to look at some YouTube video, for example the greate Google I/O Sessions...\\n\\nMy needs are simple :\\n\\n*   running on OS X (Lion)\\n*   download the video easily\\n*   no need to convert the file (to be able to read the file as soon as possible)\\n*   free\\n\\nAfter some basic research, I found an easy way to achieve this using the following softwares:\\n\\n*   [Enolsoft Free YouTube Downloader HD](http://www.enolsoft.com/free-youtube-downloader-hd-for-mac.html) : this software allows you to download easily the YouTube FLV on your hard drive.\\n*   [VLC Media Player](http://www.videolan.org/vlc/download-macosx.html) : to play the FLV file locally.\\n\\n![](http://2.bp.blogspot.com/-8j7u3NgOGkQ/TtyWZDO6-_I/AAAAAAAAATQ/rNkRF1JNUYE/s400/capture-youtube-video.png)\\n\\n\\nEnjoy!"},{"id":"/2011/11/20/installing-memcached-on-mac-os-x-and-using-it-in-java","metadata":{"permalink":"/blog/2011/11/20/installing-memcached-on-mac-os-x-and-using-it-in-java","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-11-20-installing-memcached-on-mac-os-x-and-using-it-in-java.md","source":"@site/blog/2011-11-20-installing-memcached-on-mac-os-x-and-using-it-in-java.md","title":"Installing Memcached on Mac OS X and using it in Java","description":"Introduction","date":"2011-11-20T00:00:00.000Z","formattedDate":"November 20, 2011","tags":[],"readingTime":5.68,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Installing Memcached on Mac OS X and using it in Java","categories":"java memcached osx"},"prevItem":{"title":"How to watch YouTube videos offline (on OS X)?","permalink":"/blog/2011/12/05/how-to-watch-youtube-videos-offline-on-os-x"},"nextItem":{"title":"JAX-RS: Jersey and JSON single element arrays","permalink":"/blog/2011/09/02/jax-rs-jersey-and-json-single-element-arrays"}},"content":"### Introduction\\n\\nIn this article I will explain how you can:\\n\\n1.  Install and Configure Memcached on Mac OS X\\n2.  Use Memcached in your Java Application\\n\\nI won\'t go in too much detail about the benefits of using a distributed cache in your applications, but let\'s at least provide some use cases for applications that are running in the context of an enterprise portal, eXo Platform in my case - *surprising isn\'t?* And I will show this in another post.\\n\\nWe have many reasons to use a cache (distributed or not), in the context of enterprise portal, let\'s take a look to some of these reasons:\\n\\n* A portal is used to aggregate data in a single page. These data could come from different sources : Web Services, Database, ERP, ..... and accessing the data in real time could be costly. So it will be quite interesting to cache the result of the call when possible.\\n* If the portal is used to aggregate many data from many sources, it is sometime necessary to jump into another application to continue some operation. A distributed and shared cache could be used to manage some context between different applications running in different processes (JVM or even technologies)\\nThese are two example where a shared cache could be interesting for your portal based applications, we can find many other reason.\\n\\nNote that the Portlet API (JSR-286) contains already a cache mechanism that cache the HTML fragment, and that eXo Platform also provide a [low level cache](http://docs.jboss.org/exojcr/1.14.0-CR4/developer/en-US/html/ch-cache.html), based on [JBoss Cache](http://www.jboss.org/jbosscache).\\n\\n\x3c!-- truncate --\x3e\\n\\n### Installation and Configuration\\n\\n\\n**Installing Memcached from sources**\\n\\nYou can find some information about Memcached installation on the Memcached [Wiki](http://code.google.com/p/memcached/wiki/NewStart). The following steps are the steps that I have used on my environment.\\n\\nAs far as I know, Memached is not available as package for Mac OS X. I am still on Snow Leopard (10.6.8), and I have installed XCode and all development tools. I have use the article \\"Installing memcached 1.4.1 on Mac OS X 10.6 Snow Leopard\\" from [wincent.com](https://wincent.com/wiki/Installing_memcached_1.4.1_on_Mac_OS_X_10.6_Snow_Leopard). For simplicity reason I have duplicate the content and updated to the latest releases.\\n\\n1- Create a working directory :\\n\\n``` sh\\n$ mkdir memcachedbuild\\n$ cd memcachebuild\\n```\\n\\n2- Install [libevent](http://libevent.org/) that is mandatory for memcached\\n\\n``` sh\\n$ curl -O http://www.monkey.org/~provos/libevent-1.4.14-stable.tar.gz\\n$ tar xzvf libevent-1.4.14-stable.tar.gz\\n$ cd libevent-1.4.14-stable\\n$ ./configure\\n$ make\\n$ make verify\\n$ sudo make install&nbsp;\\n```\\n\\n3- Install memcached\\n\\nGo back to your install directory (_memcachedbuild_)\\n\\n``` sh\\n$ curl -O http://memcached.googlecode.com/files/memcached-1.4.10.tar.gz\\n$ tar xzvf memcached-1.4.10.tar.gz\\n$ cd memcached-1.4.10\\n$ ./configure\\n$ make\\n$ make test\\n$ sudo make install\\n```\\n\\nYou are now ready to use memcached that is available at `/usr/local/bin/memcached`.\\n\\nThis allows you to avoid changing to the pre-installed memcached located in /usr/bin, if you want to replace it instead of having you own install, just run the configure command with the following parameter: `./configure --prefix=/usr`\\n\\n**Starting and testing Memcached**\\n\\nStart the memcached server, using the following command line:\\n\\n``` sh\\n$ /usr/local/bin/memcached -d -p 11211\\n```\\n\\nThis command starts the memcached server as demon (-d parameter), on the TCP port 11211 (this is the default value). You can find more about the memcached command using `man memcached`.\\n\\nIt is possible to connect and test your server using a telnet connection. Once connected you can set and get object in the cache, take a look to the following paragraph.\\n\\n``` sh\\n$ telnet 127.0.0.1 11211\\nTrying 127.0.0.1...\\nConnected to tgrall-server.\\nEscape character is \'^]\'.\\nset KEY 0 600 16\\nThis is my value\\nSTORED\\nget KEY\\nVALUE KEY 0 16\\nThis is my value\\nEND\\n```\\n\\n\\nThe `set` command allows you to put a new value in the cache using the following syntax:\\n\\n``` sh\\nset <key>  <flags> <expiration_time>  <number_of_bytes> [noreply] \\\\n\\\\n\\n\\n<value>\\n```\\n\\n*   key : the key used to store the data in the cache\\n*   flags : a 32 bits unsigned integer that memcached stored with the data\\n*   expiration_time : expiration time in seconds, if you put 0 this means no delay\\n*   number_if_bytes : number of bytes in the data block\\n*   noreply : option to tell the server to not return any value\\n*   value : the value to store and associate to the key.\\n\\nThis is a short view of the documentation located in your source directory `/memcachedbuild/memcached-1.4.10/doc/protocol.txt`.\\n\\nThe `get` command allows you to access the value that is associated with the key.\\n\\nYou can check the version of memcahed you are running by calling the `stats` command in your telnet session.\\n\\nYour memcached server is up and running, you can now start to use it inside your applications.\\n\\n### Simple Java Application with Memcached\\n\\nThe easiest way to use memcached from your Java applications is to use a client library. You can find many [client libraries](http://code.google.com/p/memcached/wiki/Clients#Java). In this example I am using [spymemcached](http://code.google.com/p/spymemcached/) developped by the people from [Couchbase](http://www.couchbase.com/).\\n\\n1- Adding SpyMemcached to your Maven project\\n\\nAdd the repository to you pom.xml (or you setting.xml)\\n\\n``` xml\\n<repository>\\n  <id>spy</id>\\n  <name>Spy Repository</name>\\n  <layout>default</layout>\\n  <url>http://files.couchbase.com/maven2/</url>\\n</repository>\\n```\\n\\nthen the dependency to your pom.xml\\n\\n``` xml\\n<dependency>\\n  <groupid>spy</groupid>\\n  <artifactid>spymemcached</artifactid>\\n  <version>2.7.3</version>\\n</dependency>\\n```\\n\\n2- Use SpyMemcache client in your application\\n\\nThe following code is a simple Java class that allows you to enter the key and the value and set it in the cache.\\n\\n``` java\\npackage com.grallandco.blog;\\n\\nimport java.io.BufferedReader;\\nimport java.io.IOException;\\nimport java.io.Console;\\nimport java.io.InputStreamReader;\\nimport java.util.Date;\\nimport java.util.logging.Level;\\nimport java.util.logging.Logger;\\nimport net.spy.memcached.AddrUtil;\\nimport net.spy.memcached.MemcachedClient;\\n\\npublic class Test {\\n\\n  public static void main(String[] args) {\\n    try {\\n\\n      System.out.print(\\"Enter the new key : \\");\\n      BufferedReader reader = new BufferedReader( new InputStreamReader(System.in));\\n      String key = null;\\n      key = reader.readLine();\\n\\n      System.out.print(\\"Enter the new value : \\");\\n      String value = null;\\n      value = reader.readLine();\\n\\n      MemcachedClient cache = new MemcachedClient(AddrUtil.getAddresses(\\"127.0.0.1:11211\\"));\\n\\n      // read the object from memory\\n      System.out.println(\\"Get Object before set :\\"+ cache.get(key)  );\\n\\n      // set a new object\\n      cache.set(key, 0, value );\\n\\n      System.out.println(\\"Get Object after set :\\"+ cache.get(key)  );\\n\\n\\n    } catch (IOException ex) {\\n      Logger.getLogger(Test.class.getName()).log(Level.SEVERE, null, ex);\\n      System.exit(0);\\n    }\\n\\n\\n    System.exit(0);\\n\\n  }\\n}\\n\\n```\\n\\nSo when executing the application you will see something like :\\n\\n``` sh\\nEnter the new key : CITY\\nEnter the new value : Paris, France\\n2011-11-16 15:22:09.928 INFO net.spy.memcached.MemcachedConnection:  Added {QA sa=/127.0.0.1:11211, #Rops=0, #Wops=0, #iq=0, topRop=null, topWop=null, toWrite=0, interested=0} to connect queue\\n2011-11-16 15:22:09.932 INFO net.spy.memcached.MemcachedConnection:  Connection state changed for sun.nio.ch.SelectionKeyImpl@5b40c281\\nGet Object before set :null\\nGet Object after set :Paris, France\\n\\n```\\n\\nYou can also access the object from a Telnet session:\\n\\n``` sh\\nget CITY\\nVALUE CITY 0 13\\nParis, France\\nEND\\n```\\n\\n\\nYou can use any Java class in your application, the only thing to do is to make this class serializable.\\n\\nThis is it for the first post about memcached and Java,&nbsp; I am currently working on a small example integrating Web Services call, Portlets and memcached."},{"id":"/2011/09/02/jax-rs-jersey-and-json-single-element-arrays","metadata":{"permalink":"/blog/2011/09/02/jax-rs-jersey-and-json-single-element-arrays","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-09-02-jax-rs-jersey-and-json-single-element-arrays.md","source":"@site/blog/2011-09-02-jax-rs-jersey-and-json-single-element-arrays.md","title":"JAX-RS: Jersey and JSON single element arrays","description":"Last week I have been struggling with a small issue while developing a service using Jersey.","date":"2011-09-02T00:00:00.000Z","formattedDate":"September 2, 2011","tags":[],"readingTime":1.675,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"JAX-RS: Jersey and JSON single element arrays","categories":"java rest json"},"prevItem":{"title":"Installing Memcached on Mac OS X and using it in Java","permalink":"/blog/2011/11/20/installing-memcached-on-mac-os-x-and-using-it-in-java"},"nextItem":{"title":"How to create a new Content List Template for eXo Platform 3","permalink":"/blog/2011/08/31/how-to-create-a-new-content-list-template-for-exo-platform-3"}},"content":"Last week I have been struggling with a small issue while developing a service using Jersey.\\n\\nThe goal of this service is to provide JSON object to my Web application, so called directly from the browser. This service returns in a JSON array a list of Employees, something like:\\n\\n``` javascript\\n{\\"employee\\":[\\n{\\"email\\":\\"jdoe@example.com\\",\\"firstName\\":\\"John\\",\\"lastName\\":\\"Doe\\"},\\n{\\"email\\":\\"mmajor@example.com\\",\\"firstName\\":\\"Mary\\",\\"lastName\\":\\"Major\\"}\\n]}\\n```\\n\\nSo an \\"employee\\" array, this is perfect and expected, but when my service returns a single element the returned object looks like:\\n\\n``` javascript\\n{\\"employee\\":{\\"email\\":\\"jdoe@example.com\\",\\"firstName\\":\\"John\\",\\"lastName\\":\\"Doe\\"}}\\n```\\n\\nAs you can see brackets **[...]** are missing around the employee item. This is an issue since your client code is expecting an array.\\n\\n\x3c!-- truncate --\x3e\\n\\n#### A solution...\\n\\nMy application is using Jersey, the JAX-RS Reference Implementation, and JAXB for the serialization of Java Objects to JSON, as I have explained in a [previous blog post](http://tugdualgrall.blogspot.com/2010/02/create-and-deploy-jax-rs-rest-service.html). I found a solution to this by creating a new [JAXB Context Resolver](http://jersey.java.net/nonav/documentation/latest/json.html#d4e919).\\n\\nIn this resolver I can control how the JSON object should be generated, here is my implementation :\\n\\n``` java\\nimport com.grallandco.employee.service.converter.EmployeeConverter;\\nimport javax.ws.rs.ext.ContextResolver;\\nimport javax.ws.rs.ext.Provider;\\nimport javax.xml.bind.JAXBContext;\\n\\nimport com.sun.jersey.api.json.JSONConfiguration;\\nimport com.sun.jersey.api.json.JSONJAXBContext;\\n\\n@Provider\\npublic class JAXBContextResolver implements ContextResolver &lt; JAXBContext &gt; {\\n\\n  private JAXBContext context;\\n  private Class[] types = {EmployeeConverter.class};\\n\\n  public JAXBContextResolver() throws Exception {\\n    this.context = new JSONJAXBContext(JSONConfiguration.mapped().arrays(\\"employee\\").build(),\\n    types);\\n\\n  }\\n\\n  public JAXBContext getContext(Class objectType) {\\n    for (Class type : types) {\\n      if (type == objectType) {\\n        return context;\\n      }\\n    }\\n    return null;\\n  }\\n}\\n\\n```\\n\\nFirst of all I declare this new class as a [`@Provider`](http://jersey.java.net/nonav/apidocs/1.7/jersey/index.html?javax/ws/rs/ext/Provider.html) to say that it this class is of interest to the JAX-RS runtime.\\n\\nI put in the `types` array the list of the Java classes that are concerned by the serialization (line#13). Then I create the ContextResolved with the different options that fulfill my requirements. You can take a look to the [`JAXBContextResolver`](http://jersey.java.net/nonav/apidocs/1.7/jersey/index.html?com/sun/jersey/api/json/JSONJAXBContext.html) Javadoc to see all the possible options available.\\n\\nWith this class, the service now returned the following JSON String:\\n\\n``` javascript\\n{\\"employee\\":[{\\"email\\":\\"jdoe@example.com\\",\\"firstName\\":\\"John\\",\\"lastName\\":\\"Doe\\"}]}\\n```\\n\\nYou can find a complete example (NetBeans project) [here](https://github.com/tgrall/Sample-REST-Service-using-JSON)."},{"id":"/2011/08/31/how-to-create-a-new-content-list-template-for-exo-platform-3","metadata":{"permalink":"/blog/2011/08/31/how-to-create-a-new-content-list-template-for-exo-platform-3","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-08-31-how-to-create-a-new-content-list-template-for-exo-platform-3.md","source":"@site/blog/2011-08-31-how-to-create-a-new-content-list-template-for-exo-platform-3.md","title":"How to create a new Content List Template for eXo Platform 3","description":"Introduction","date":"2011-08-31T00:00:00.000Z","formattedDate":"August 31, 2011","tags":[],"readingTime":8.755,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to create a new Content List Template for eXo Platform 3","categories":"exo cms portal groovy"},"prevItem":{"title":"JAX-RS: Jersey and JSON single element arrays","permalink":"/blog/2011/09/02/jax-rs-jersey-and-json-single-element-arrays"},"nextItem":{"title":"How to protect your REST service and Gadget in eXo Platform","permalink":"/blog/2011/04/11/how-to-protect-your-rest-service-and-gadget-in-exo-platform"}},"content":"### Introduction\\n\\neXo Platform provide many powerful features to manipulate and expose any type of content on a page. This is due to the fact that eXo stores the all the content in its Java Content Repository (JCR) and render the content on a page using Groovy Templates.\\n\\nIn this how to you will learn how you can create and use template that are used in the \\"Content List\\" portlet. For example in the ACME sample site you can show the content in 1 or 2 columns just by selecting different templates.\\n\\n![]( http://2.bp.blogspot.com/-aEn1Gp75nD4/TbbALBRftVI/AAAAAAAAAPc/3JLHePfFkuM/s200/one-col-view.png )\\nOne Column View\\n\\n\\n![]( http://4.bp.blogspot.com/-xSn44HNlBZ8/TbbAMSRDq7I/AAAAAAAAAPg/YEu715pX3WE/s200/two-col-view.png )\\nTwo Columns View\\n\\n### Creating a new Content List Template\\n\\nIn this section you will learn how to create a new template using the eXo IDE. Before writing the new template it is important to learn where the templates are stored.\\n\\n#### eXo Content Service: Template Storage\\n\\nLike many things inside eXo Platform, the eXo JCR is used to stored the templates. Templates are just a special type of content. This allows developers to easily write and test code without having complex deployment process, but also it make the life easy to export a running configuration to another one. For this you just need to use the standard JCR export/import features.\\n\\nAll the template and eXo Content Service configurations are stored inside a specific JCR workspace named : `dms-system`.\\n\\nEach template type (Document Type, Content List, ....) is stored in a specific location. In our case we want to work on the \\"Content List\\" portlet so the template are stored inside the following folder:\\n\\n* `/exo:ecm/views/templates/content-list-viewer/list/`\\n\\nJust for the fun of it, let\'s inspect this folder using the eXo CRaSH utility. If you are not interested you can jump to the next section. CRaSH is a shell for Java Content Repositories, the source of CRaSH is available on [Google Code](http://code.google.com/p/crsh/). So in a terminal window:\\n\\n1-  Connect to CRaSH using telnet client:\\n\\n``` sh\\n telnet 127.0.0.1 5000\\n```\\n\\n2-  Connect to the JCR workspace using the following command:\\n\\n``` sh\\n connect -u root -p gtn -c portal dms-system\\n```\\n\\nWhere: -u is the user, -p the password, -c the Portal Container, and dms-system the workspace to use\\n\\n3-  Move the the folder that contains all the templates for the Content List portlet:\\n\\n``` sh\\n cd /exo:ecm/views/templates/content-list-viewer/list/\\n```\\n\\n4-  List all the templates using the `ls` command\\nYou can see the list of all the templates available for the Content List portlet.\\n\\n#### Create a new template using the eXo IDE\\n\\nLet\'s now create a new template using the IDE. For this be sure you are connected with a user that is part of the `/Developer` group. For simplicity reason I am using the `root` user.\\n\\nThe first important step is to go the the template location using the following steps:\\n\\n1-  Access the IDE: click on `My Spaces &gt; IDE`.\\n\\n2-  Switch `dms-system` workspace: In the IDE menu click `My Spaces &gt;on Window &gt; Select Workspace`. And select the dms-system location in the dialog box and click OK.\\n\\n3-  In the file structure on the left navigate to the template location :\\n\\n``` sh\\n/exo:ecm/views/templates/content-list-viewer/list/\\n```\\n\\n4-  Create a new template : In the IDE Menu click on `File &gt; New &gt; Groovy Template`\\n\\n5-  Save the file as \\"`MyNewTemplate.gtmpl`\\"\\n\\n6-  Enter some basic code:\\n\\n``` html\\n\\n<h1>This is my template</h1>\\nThe date is <= new Date()>\\n\\n```\\n\\n7-  Save the template\\n\\n8-  Go back to the Home page of the Acme Site\\n\\n9-  Switch to Edit more by selecting Edit in the top right drop down list.\\n\\n![]( http://3.bp.blogspot.com/-EsSUDUdUo_8/TbbRJZlZlHI/AAAAAAAAAPk/5iTn-NoC-JE/s200/edit-mode.png )\\n\\n10-  Move you mouse at the top of the list of news and click on the preference button: ![](http://1.bp.blogspot.com/-ksljDe3llf4/TbbSIOl0DII/AAAAAAAAAPs/C0eBv-SJ25I/s200/PreferencesIcon.png )\\n\\n11-  In the list of templates, select the \\"MyNewTemplate\\", and click save.\\n\\n![]( http://2.bp.blogspot.com/-7hT5osGXK1w/TbbTTJ7ZKSI/AAAAAAAAAP0/--VCTefhrSI/s320/select-template.png )\\n\\nWe have created our new template, and use it on a page. We should now add some more interesting code to the template to really loop over the content based on the portlet configuration. But before this it is important to talk about caching and code modification.\\n\\n#### eXo Template and Cache\\n\\nTo improve performances and a running system, the compiled version of the template is by default cached. This is why when you are modifying a template you do not see any change. We have two ways to work around this:\\n\\n*   Run eXo Platform in Debug mode, in this case nothing is cached\\n*   Invalidate the cache manually using JMX\\n\\nSince working with no cache at all is not an option, here is the MBean you have to use to invalidate the Template Service cache:\\n\\n* `exo:portal=\\"portal\\",service=cache,name=\\"TemplateService\\"`, then call the `clearCache` operation on it\\n\\nI do use JConsole for this, but you can use any method to call your MBeans operation.\\n\\n![]( http://1.bp.blogspot.com/-ZuMQz-VHBYY/TbldhSRY2NI/AAAAAAAAAP8/pb09UoA9_b4/s200/TemplateServiceCacheMbean.png )\\n\\nDo not forget to call this operation each time you modify your template to be sure eXo recompile the template.\\n\\n#### Accessing Content in the template\\n\\nThe current code of the template is really simple, you need now to add code to print the content in the page. For this we will be using some eXo Content programming, once again in the IDE.\\n\\nIf you are not interested to have detailed explanation of the code you can go to the complete source code [here](javascript: alert().\\n\\nThe template used by the Content List portlet is based on the following Java class `org.exoplatform.wcm.webui.clv.UICLVPresentation`, this class is responsible to set the complete context that you can use in the template such as:\\n\\n* The folder or category that contains the content to show. The \\"Folder Path\\" field in the preference screen\\n* The display settings: title, number of documents, elements to show, ...\\n\\nHere is the code to access these preferences:\\n\\n``` groovy\\n// import all the classes need in the template\\nimport javax.jcr.Node;\\nimport org.exoplatform.wcm.webui.paginator.UICustomizeablePaginator;\\nimport org.exoplatform.wcm.webui.clv.UICLVPortlet;\\nimport org.exoplatform.wcm.webui.Utils;\\nimport org.exoplatform.services.wcm.core.NodeLocation;\\n\\n// get the portlet preferences\\ndef header = uicomponent.getHeader();\\ndef isShowRssLink = uicomponent.isShowRssLink();\\ndef isShowHeader = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_HEADER);\\ndef isShowRefresh = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_REFRESH_BUTTON);\\n\\ndef isShowTitle = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_TITLE);\\ndef isShowDate = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_DATE_CREATED);\\ndef isShowLink = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_LINK);\\ndef isShowReadmore = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_READMORE);\\ndef isShowImage = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_ILLUSTRATION) ;\\ndef isShowSummary = uicomponent.isShowField(UICLVPortlet.PREFERENCE_SHOW_SUMMARY);\\n\\n```\\n\\nThe `uicomponent` object is defined by the container class of the portlet that calls the template. This class contains many utility methods. In the code above I retrieve all the preferences of the portlet, since the name are self-explanatory it is not necessary to detail them, especially when you look at the preferences screen below:\\n\\n![]( http://3.bp.blogspot.com/-bDIJtSVaD9A/TblkjTr2wMI/AAAAAAAAAQE/6mRLmEYqpsE/s320/preference-screen.png )\\n\\nNow that the template has all the preferences, it is time to loop on the content on print the information.\\n\\nThe eXo Content Service provides API to manipulate the content, including pagination of content. The idea behind this is to let the Content Service manage the JCR query, sorting, caching and pagination of data. So in your template you will mainly manage two classes to loop through the content to show:\\n\\n* `uicomponent.getUIPageIterator()` a paginator object that is configured based on the portlet preferences\\n* `uicomponent.getCurrentPageData()`contains a list of the content (JCR Nodes) that should print on the current page\\n\\nSo let\'s print all the content of the page as a simple HTML list:\\n\\n``` html\\n<ul style=\\"margin: 20px\\">\\n<%\\nfor (viewNode in uicomponent.getCurrentPageData()) {\\n  def title = viewNode.getProperty(\\"exo:title\\").getString();\\n  print(\\"<li>$title</li>\\");\\n}\\n%>\\n</ul>\\n```\\n\\nJust  copy this code in your template, save it, refresh the cache... and go to your page. You should see the list of the content in a simple HTML list.\\n\\nOn each content (Node), eXo Content API provides some helper method to easily manipulate the content and avoid using the JCR API directly. In the following code you can see the most important methods accessing content properties:\\n\\n``` java\\ndef itemName = viewNode.getName();\\ndef itemLink = uicomponent.getURL(viewNode);  \\ndef webdDavLink = uicomponent.getWebdavURL(viewNode);\\ndef itemDateCreated = uicomponent.getCreatedDate(viewNode);\\ndef itemModifiedDate = uicomponent.getModifiedDate(viewNode);\\ndef itemAuthor = uicomponent.getAuthor(viewNode);\\ndef imgSrc = uicomponent.getIllustrativeImage(viewNode);\\ndef itemTitle = uicomponent.getTitle(viewNode);\\ndef itemSummary = uicomponent.getSummary(viewNode);\\n```\\n\\nOne important point is the fact that these methods are responsible of many things, for example: formatting dates, returning complete URLs depending of the context of the portlet.\\n\\nBased on these methods you can now work on the presentation of the information on the page. Let\'s for example print the image, the title and allow user to click on the title to go in the detail view of the article. This is done simply using the following code:\\n\\n``` java\\n<%\\nfor (viewNode in uicomponent.getCurrentPageData()) {\\n  def itemName = viewNode.getName();\\n  def itemLink = uicomponent.getURL(viewNode);  \\n  def webdDavLink = uicomponent.getWebdavURL(viewNode);\\n  def itemDateCreated = uicomponent.getCreatedDate(viewNode);\\n  def itemModifiedDate = uicomponent.getModifiedDate(viewNode);\\n  def itemAuthor = uicomponent.getAuthor(viewNode);\\n  def imgSrc = uicomponent.getIllustrativeImage(viewNode);\\n  def itemTitle = uicomponent.getTitle(viewNode);\\n  def itemSummary = uicomponent.getSummary(viewNode);\\n\\n  %>\\n  <div style=\\"overflow: auto;\\">\\n  <img src=\\"$imgSrc\\" align=\\"left\\">\\n  <h3><a href=\\"$itemLink\\">$itemTitle</a></h3>\\n  $itemSummary\\n  </div>\\n\\n  <%\\n}\\n%>\\n```\\n\\nFor simplicity reason, this code does not manage any null value. Also the template do not deal with the portlet preferences such as the \\"Header\\", \\"RSS\\" links and so on, do not hesitate to do it if you want.\\n\\nThe Web site should look like the following image:\\n\\n![]( http://3.bp.blogspot.com/-JvoTBERVTH0/Tbmf9tYRcUI/AAAAAAAAAQM/sdUy2gs39ks/s320/new-site-001.png )\\n\\n\\nThe last important point is to add the support for the in context editing that allows the user to edit the content directly from the template. Once again this is done with a method of the `uicomponent` object, that create a DIV around the content. Let\'s add this to the template:\\n\\n``` java\\n<%\\nfor (viewNode in uicomponent.getCurrentPageData()) {\\n  def itemName = viewNode.getName();\\n  def itemLink = uicomponent.getURL(viewNode);  \\n  def webdDavLink = uicomponent.getWebdavURL(viewNode);\\n  def itemDateCreated = uicomponent.getCreatedDate(viewNode);\\n  def itemModifiedDate = uicomponent.getModifiedDate(viewNode);\\n  def itemAuthor = uicomponent.getAuthor(viewNode);\\n  def imgSrc = uicomponent.getIllustrativeImage(viewNode);\\n  def itemTitle = uicomponent.getTitle(viewNode);\\n  def itemSummary = uicomponent.getSummary(viewNode);\\n\\n  %>\\n  <div style=\\"overflow: auto;\\">\\n  <%=uicomponent.addQuickEditDiv(\\"MyTemplateContentEditor\\", viewNode)%>\\n  <img src=\\"$imgSrc\\" align=\\"left\\">\\n  <h3><a href=\\"$itemLink\\">$itemTitle</a></h3>\\n  $itemSummary\\n  < /div>\\n  </div>\\n\\n  <%\\n}\\n%>\\n```\\n\\nThe lines 15 and 19 are new in this template and provide support for Quick Edit feature.\\n\\nDone! We have created a new template for eXo Platform Content Service using embedded IDE.\\n\\n#### Conclusion\\n\\nIn this article you have learned how to create a new template for eXo Content Service, with some basic steps:\\n\\n* Create a new Groovy Template using the eXo IDE\\n* Edit this template using the eXo Java Content API\\n* Configure your Content List portlet instance on a page to select the new template\\n\\nYou can now create your own templates and use your imagination to add cool features to your site (for example the carrousels you see on the [eXo site](http://www.exoplatform.com/) are using custom CLV template.)"},{"id":"/2011/04/11/how-to-protect-your-rest-service-and-gadget-in-exo-platform","metadata":{"permalink":"/blog/2011/04/11/how-to-protect-your-rest-service-and-gadget-in-exo-platform","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-04-11-how-to-protect-your-rest-service-and-gadget-in-exo-platform.md","source":"@site/blog/2011-04-11-how-to-protect-your-rest-service-and-gadget-in-exo-platform.md","title":"How to protect your REST service and Gadget in eXo Platform","description":"During a partner workshop I was showing to the developers how the eXo IDE can help them to develop new features quickly and push them to the users in few minutes. A person asked me if it is possible to put some restriction in services and gadgets based on user profile.","date":"2011-04-11T00:00:00.000Z","formattedDate":"April 11, 2011","tags":[],"readingTime":6.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to protect your REST service and Gadget in eXo Platform","categories":"java exo rest"},"prevItem":{"title":"How to create a new Content List Template for eXo Platform 3","permalink":"/blog/2011/08/31/how-to-create-a-new-content-list-template-for-exo-platform-3"},"nextItem":{"title":"eXo Cloud IDE : develop for the Cloud on the Cloud","permalink":"/blog/2011/03/16/exo-cloud-ide-develop-for-the-cloud-on-the-cloud"}},"content":"During a partner workshop I was showing to the developers how the eXo IDE can help them to develop new features quickly and push them to the users in few minutes. A person asked me if it is possible to put some restriction in services and gadgets based on user profile.\\n\\nAs you can guess the answer is YES WE CAN!\\n\\n* How to access the security context in a REST service\\n* How to check is a user is member of a  group and manage permission from this information\\n* How to consume this service in a gadget and leverage the security to protect resources\\n\\n![](http://1.bp.blogspot.com/-eR15bpCaiXo/TZ8yp1kkLYI/AAAAAAAAAPM/8Az5EVfVrzU/s200/rest-no-access.png )\\nNot-authorized\\n\\n\\n![](http://2.bp.blogspot.com/-wcfWsRgV8Xc/TZ8ypzrBsLI/AAAAAAAAAPE/U9VnHpc3q9M/s200/rest-access.png )\\nAuthorized\\n\\nIf you are not interested to follow steps by step the explanations you can directly jump to the complete [REST Service code](#completeService) or download the full eXo IDE Project from [GitHub](https://github.com/tgrall/sample-gadget-with-security)\\n\\n### Access the User Profile from your REST Service\\n\\nAs you probably know eXo Platform uses [JAX-RS](http://jcp.org/en/jsr/detail?id=311) as API to develop and deploy REST Services. eXo developers can create REST services using their favorite Java IDE, but here I am using the eXo IDE package with [eXo Platform](http://www.exoplatform.org/company/public/website/platform).\\n\\nTo access the security and user information in your service method it is possible to use the [SecurityContext](http://jersey.java.net/nonav/apidocs/1.5/jersey/javax/ws/rs/core/SecurityContext.html) class of the JAX-RS API.  Your method signature will look like:\\n\\n``` java\\nimport javax.ws.rs.Path\\nimport javax.ws.rs.GET\\nimport javax.ws.rs.PathParam\\nimport javax.ws.rs.core.Response\\nimport javax.ws.rs.core.MediaType\\nimport javax.ws.rs.Produces\\nimport javax.ws.rs.core.SecurityContext\\nimport javax.ws.rs.core.Context\\n\\n@Path(\\"/system\\")\\n@Produces(\\"application/json\\")\\npublic class SystemInformationService {\\n\\n  @GET\\n  @Path(\\"information\\")\\n  public Response getSystemInfo(@Context SecurityContext sc) {\\n    sc.getUserPrincipal();\\n    return Response.ok(\\"foo\\", MediaType.APPLICATION_JSON).build();\\n  }\\n\\n}\\n\\n```\\n\\nIn lines 7 and 8, I import the classes needed to inject the security context in the method `getSystemInfo()` in line 16. For now let\'s forget about the other part of the code.\\n\\nWith the Security Context object you can now access many things in your code. Two methods are quite interesting for this example: `getUserPrincipal()` and `isUserInRole()`, since our goal is to check if a user is allowed to execute or not a part of the business logic.\\n\\nIt is important here to remember that we cannot directly use the `isUserInRole()` method since this method uses the logical JavaEE roles that are defined at the Java application level. In our case we are interested to know if a user is present in a \\"eXo User Identity\\" Group, for example member of the `/platform/administrators group`. This information is populated during the login process and comes from the user provider that could be LDAP, the eXo Database or JCR, or any other source since developers can extend this API to plug their own provider.\\n\\nLet\'s create an helper method that check, using the eXo Identity Service, if the user that executes the method is present in a group.\\n\\n``` java\\n...\\nimport org.exoplatform.container.ExoContainer;\\nimport org.exoplatform.container.ExoContainerContext;\\nimport org.exoplatform.container.component.ComponentPlugin;\\nimport org.exoplatform.services.security.Identity;\\nimport org.exoplatform.services.security.IdentityRegistry;\\n...\\n...\\n\\nprivate boolean isMemberOf(String username,String group) {\\n  ExoContainer container = ExoContainerContext.getCurrentContainer();\\n  IdentityRegistry identityRegistry = (IdentityRegistry) container.getComponentInstanceOfType(IdentityRegistry.class);\\n  Identity identity = identityRegistry.getIdentity(username);\\n  return identity.isMemberOf( group );\\n}\\n```\\n\\nSo this method is quite simple, it takes as parameter:\\n\\n* the name of the user, that you can get from the `UserPrincipal.getName()` method\\n* the eXo Group you want to check, for example `/platform/administrator`\\n\\nYou can now call this method from your resource to check the user, and code the \\"permission business logic\\". The method could now looks like:\\n\\n``` java\\n@GET\\n@Path(\\"information\\")\\npublic Response getSystemInfo(@Context SecurityContext sc) {\\n  String groupToCheck = \\"/platform/administrators\\";\\n  String response = \\"\\";\\n  if (sc.getUserPrincipal() == null || !this.isMemberOf(sc.getUserPrincipal().getName(), groupToCheck) ) {\\n    response = \\"NOT-ALLOWED\\";\\n    } else {\\n      response = \\"ALLOWED\\";\\n    }\\n    return Response.ok(  response   , MediaType.APPLICATION_JSON).build();\\n  }\\n...\\n```\\n\\nIn this example for simplicity reason I have hard coded the group to check, you can obviously use smarter code to user external configuration to inject a list of group to check for example. I manage the security logic of my method using simple if statement and return a string. You can also depending of your needs, manage the status of your response and use HTTP Code for example return an [HTTP 403](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.4). For this you just need to return a different response using following code:\\n\\n``` java\\n    return Response.status(Response.Status.FORBIDDEN).build();\\n```\\n\\nFor simplicity reason I will stay with a single Response status (OK) and manage the permission in my client code.\\n\\n\\n### Complete REST Service\\n\\nLet\'s take a look to the full service now, this service allows administrators to get the list of the System Properties, other users get an status string \\"NOT-ALLOWED\\":\\n\\n\\n``` java\\nimport javax.ws.rs.Path\\nimport javax.ws.rs.GET\\nimport javax.ws.rs.PathParam\\nimport javax.ws.rs.core.Response\\nimport javax.ws.rs.core.MediaType\\nimport javax.ws.rs.core.CacheControl\\nimport javax.ws.rs.Produces\\nimport javax.ws.rs.core.SecurityContext\\nimport javax.ws.rs.core.Context\\n\\nimport org.exoplatform.container.ExoContainer;\\nimport org.exoplatform.container.ExoContainerContext;\\nimport org.exoplatform.container.component.ComponentPlugin;\\nimport org.exoplatform.services.security.Identity;\\nimport org.exoplatform.services.security.IdentityRegistry;\\n\\n@Path(\\"/system\\")\\n@Produces(\\"application/json\\")\\npublic class SystemInformationService {\\n\\n\\n  @GET\\n  @Path(\\"information\\")\\n  public Response getSystemInfo(@Context SecurityContext sc) {\\n    String groupToCheck = \\"/platform/administrators\\";\\n    SimpleResponseWrapper response = new SimpleResponseWrapper();\\n    String status = \\"\\";\\n    if (sc.getUserPrincipal() == null || !this.isMemberOf(sc.getUserPrincipal().getName(), groupToCheck) ) {\\n      response.status = \\"NOT-ALLOWED\\";\\n    } else {\\n      response.status = \\"OK\\";\\n      response.data = System.getProperties();\\n\\n    }  \\n\\n    CacheControl cacheControl = new CacheControl();\\n    cacheControl.setNoCache(true);\\n    cacheControl.setNoStore(true);\\n    return Response.ok(  response   , MediaType.APPLICATION_JSON).cacheControl(cacheControl).build();\\n  }\\n\\n  private boolean isMemberOf(String username,String role) {\\n    ExoContainer container = ExoContainerContext.getCurrentContainer();\\n    IdentityRegistry identityRegistry = (IdentityRegistry) container.getComponentInstanceOfType(IdentityRegistry.class);\\n    Identity identity = identityRegistry.getIdentity(username);\\n    return identity.isMemberOf( role );\\n  }\\n\\n}\\n\\npublic class SimpleResponseWrapper {\\n  String status;\\n  Object data;\\n}\\n\\n```\\n\\n\\nTo summarize:\\n\\n* Line 24 : the `SecurityContext` is injected to the method\\n* Line 26 : Initialization of a simple `ResponseWrapper` defined on line 51, that contains a status and data. That will be serialized in JSON by the eXo REST engine.* Line 28 : the method check if a user is connected and member of `/platform/administratorc. If not it send response with the status NO-ALLOWED.\\n* Line 31/32 : The response object is sent. This response contains an OK status and the data (system properties list)\\n* Line 42 : Using the eXo Identity Service, the method check if the connected user is member of a specific group.\\n\\n### Consume the service into a Gadget\\n\\nI can now take this service and consume it into an Gadget. I also develop this Gadget using the eXo IDE.\\n\\nThe following code shows the Javascript part of the Gadget that calls the service, check the security and push the response content in Gadget body. For productivity I use JQuery framework.\\n\\n``` javascript\\n<script>\\nfunction printInfo(result) {\\n  var htmlResponse= [];\\n  if (result.status == \\"OK\\") {\\n    var data = result.data;\\n    htmlResponse.push(\\"<tr>\\");\\n    $.each(data, function(index, value) {\\n      htmlResponse.push(\'<tr><td>\'+ index + \'</td><td>\' + value + \'</td></tr>\');\\n    });\\n    htmlResponse.push(\\"</tr>\\");\\n    $(\'#systemInfo\').height(200);\\n  }\\n  else {\\n    htmlResponse.push(\\"Not permitted\\");\\n    $(\'#systemInfo\').height(50);\\n  }\\n  $(\'#systemInfo\').html(htmlResponse.join(\'\'));\\n  gadgets.window.adjustHeight( $(\'#sysInfoContainer\').outerHeight()  );\\n\\n}\\n\\nfunction loadInformationFromServer() {\\n  $.getJSON(\'/rest/private/system/information\', function(result){ printInfo(result);   } );\\n}\\n\\nfunction init() {\\n  loadInformationFromServer();\\n}\\n\\ngadgets.util.registerOnLoadHandler(init);\\n\\n<\/script>\\n```\\n\\nHere some quick explanation about this code:\\n\\n* Line 23: To call the REST service, I use the `$.getJSON()` method. This method is really easy to use when you are executing the Gadget is in the same container than the portal that consumes it. When you are using the gadget.io.MakeRequest is interesting to proxy a request and you need to re-authenticate, for example using oAuth.\\n* Line 3 : This is the call back method, as you can see in this method I use the `ResponseWrapper` to check the code in the status attribute. Depending of the status OK or not I do print the value.\\n\\n### Conclusion\\n\\nIn this how-to you have learned how to:\\n\\n* Get the security context in your REST Service\\n* Check the membership of a user using the eXo Identity Service\\n* Create a gadget that consume this service and expose only data to user with correct profile\\n* Download the full project from [GitHub](https://github.com/tgrall/sample-gadget-with-security)"},{"id":"/2011/03/16/exo-cloud-ide-develop-for-the-cloud-on-the-cloud","metadata":{"permalink":"/blog/2011/03/16/exo-cloud-ide-develop-for-the-cloud-on-the-cloud","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-03-16-exo-cloud-ide-develop-for-the-cloud-on-the-cloud.md","source":"@site/blog/2011-03-16-exo-cloud-ide-develop-for-the-cloud-on-the-cloud.md","title":"eXo Cloud IDE : develop for the Cloud on the Cloud","description":"Yesterday, eXo has launched a new cloud based service: the eXo Cloud IDE. This IDE is an online service that facilitates the development of gadgets and mashups that could be deployed directly to a PaaS.","date":"2011-03-16T00:00:00.000Z","formattedDate":"March 16, 2011","tags":[],"readingTime":1.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"eXo Cloud IDE : develop for the Cloud on the Cloud","categories":"exo cloud"},"prevItem":{"title":"How to protect your REST service and Gadget in eXo Platform","permalink":"/blog/2011/04/11/how-to-protect-your-rest-service-and-gadget-in-exo-platform"},"nextItem":{"title":"How to calculate the size of a folder in JCR (Java Content Repository)?","permalink":"/blog/2011/03/01/how-to-calculate-the-size-of-a-folder-in-jcr-java-content-repository"}},"content":"![]( http://1.bp.blogspot.com/-IgPlseainto/TYBq0-Sr6nI/AAAAAAAAAOw/YL3TECcXJts/s400/logoexoplatform.png )\\n\\nYesterday, eXo has launched a new cloud based service: the [eXo Cloud IDE](http://www.cloud-ide.com/). This IDE is an online service that facilitates the development of gadgets and mashups that could be deployed directly to a PaaS.\\n\\nBefore launching this service on the Cloud we, eXo team and customers, have used the IDE embedded in the [eXo Platform](http://www.exoplatform.com/company/en/platform/exo-platform-35) to extend our intranet and customer deployments (some of the modules that we have developed live on our intranet are available as plugins on the [eXo Resource Center](http://www.exoplatform.com/company/en/Content-types/Plugins)).\\n\\nThis IDE is the last mile between the users and the developers. It provides a way to add new services asked by business users at a lower cost with a good time to market. And all this based on standards that corporate and Web developers like : REST Services using [JAX-RS](http://jcp.org/en/jsr/detail?id=311) and UI based on [OpenSocial](http://www.opensocial.org/) gadgets in which you can leverage all the cool and powerful features of HTML5.\\n\\nWhat eXo has launched yesterday is a big step for developers since you can now develop, test and deploy your gadgets and services online, and this in your \\"personal environment\\" using eXo Cloud IDE supports multi-tenancy. I won\'t go in all the features of the IDE since you can test it yourself by joining the [beta program](http://cloud-ide.com/) and look at this video:\\n\\n\\neXo Cloud IDE resources:\\n\\n* Overview of [eXo Platform 3.5](http://www.exoplatform.com/company/en/platform/exo-platform-35)\\n* Overview of [eXo Cloud IDE](http://cloud-ide.com/), including how-to videos and sample applications\\n* Join the [eXo Cloud IDE private beta](http://cloud-ide.com/)\\n* Benjamin Mestrallet\u2019s [blog post](http://blog.exoplatform.org/2011/03/15/history-of-exo-cloud-ide/) about eXo Cloud IDE\\n* [Evaluation download](http://www.exoplatform.com/exo-platform-3-trial/eXoPlatform-3.zip) of eXo Platform 3.0"},{"id":"/2011/03/01/how-to-calculate-the-size-of-a-folder-in-jcr-java-content-repository","metadata":{"permalink":"/blog/2011/03/01/how-to-calculate-the-size-of-a-folder-in-jcr-java-content-repository","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-03-01-how-to-calculate-the-size-of-a-folder-in-jcr-java-content-repository.md","source":"@site/blog/2011-03-01-how-to-calculate-the-size-of-a-folder-in-jcr-java-content-repository.md","title":"How to calculate the size of a folder in JCR (Java Content Repository)?","description":"Today I was working with a partner in Paris and he wanted to know how to calculate the size of a specific folder in the eXo Java Content Repository (JCR).","date":"2011-03-01T00:00:00.000Z","formattedDate":"March 1, 2011","tags":[],"readingTime":2.715,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to calculate the size of a folder in JCR (Java Content Repository)?","categories":"java exo"},"prevItem":{"title":"eXo Cloud IDE : develop for the Cloud on the Cloud","permalink":"/blog/2011/03/16/exo-cloud-ide-develop-for-the-cloud-on-the-cloud"},"nextItem":{"title":"iOS 101: How to convert a String to a NSDate","permalink":"/blog/2011/01/02/ios-101-how-to-convert-a-string-to-a-nsdate"}},"content":"Today I was working with a partner in Paris and he wanted to know how to calculate the size of a specific folder in the [eXo Java Content Repository (JCR)](http://www.exoplatform.com/company/en/resource-viewer/Features-and-Benefits/eXo-JCR-Features-and-Benefits).\\n\\nFor this specific need the goal is to calculate the size of all the documents stored inside a specific location in the content repository. This could be used for example to manage quotas, estimate the size of a shared or personal storage, ... For this specific sample I will only take in consideration the size of the binary part of the document stored in the repository; this means I will not pay attention to the various attributes and meta-data that are also stored, neither the full text index created by [Lucene](http://lucene.apache.org/) that is embedded in eXo JCR.\\n\\n### How the files are stored in the JCR?\\n\\nFiles are stored in eXo JCR in the standard node type `nt:file` (and `nt:resource`). So for this example I will simply list all the `nt:file` of a folder and aggregate the size of the file itself. It is important to understand how JCR is storing the binary content. The best way to understand it, is to view it. For that I am using the print information given by  [CRaSH](http://www.exoplatform.com/company/en/resource-viewer/Tutorial/Introduction-to-Crash), a shell for content repository developed by eXo team and lead by [Julien Viet](http://julienviet.com/).\\n\\nHere the structure of a PDF document :\\n\\n``` sh\\n/Documents/jsr170-1.0.pdf\\n+-properties\\n| +-jcr:primaryType: nt:file\\n| +-jcr:mixinTypes: [mix:referenceable,mix:versionable]\\n| +-jcr:uuid: \'6b89b6f0c0a8006530a8617df51bb0d7\'\\n| +-jcr:created: 2011-02-28T10:11:50.770+01:00\\n+-children\\n| +-/Groups/spaces/intranet_v2/Documents/Technical/jsr170-1.0.pdf/jcr:content\\n```\\n\\nAs you can see in this node the \'binary\' is not visible, nothing bad here. As written in the specification in the section `6.7.22.6 nt:file`, the binary content is an attribute of the child node `jcr:content` that is exposed below:\\n\\n``` sh\\n/Groups/spaces/intranet_v2/Documents/Technical/jsr170-1.0.pdf/jcr:content\\n+-properties\\n| +-jcr:primaryType: nt:resource\\n| +-jcr:mixinTypes: [exo:owneable,exo:datetime,dc:elementSet]\\n| +-jcr:uuid: \'6b89b6f7c0a80065735b1a8853d389d0\'\\n| +-jcr:data: <binary>\\n| +-jcr:encoding: \'UTF-8\'\\n| +-jcr:lastModified: 2011-02-28T10:11:50.767+01:00\\n| +-jcr:mimeType: \'application/pdf\'\\n+-children\\n</binary>\\n```\\n\\nYou can see now that the `jcr:content` contains some interesting attributes:\\n\\n* `jcr:mimeType`\\n* `jcr:data` this is where the binary content, the PDF itself , is.\\nSo using the JCR API you just need to get the content length using the following java code:\\n\\n``` java\\nnode.getProperty(\\"jcr:content/jcr:data\\").getLength()\\n```\\n\\nThis returns the number of bits of the binary data.\\n\\nSo to calculate the size of a folder you just need to navigate in all the documents (nt:file or jcr:content) and cumulate the size of all the files. In this following code, I am calculating the size of the folder \\"/Documents\\" by navigating into all the files contains in this folder and subfolders. (I could have chose to query all the `jcr:content` type instead of `nt:file`)\\n\\n``` java\\nSession session = getSession(); // use RepositoryService or context to get a session\\nQueryManager manager = session.getWorkspace().getQueryManager();\\nString queryStatement = \\"select * from nt:file where (jcr:path LIKE \'/Documents/%\')\\";\\nQuery query = manager.createQuery(queryStatement, Query.SQL);\\nNodeIterator nodeIterator = query.execute().getNodes();\\nNode node = null;\\nlong totalSizeInMb = 0;\\n\\nwhile (nodeIterator.hasNext()) {\\n  node = nodeIterator.next();\\n  totalSizeInMb = totalSizeInMb + node.getProperty(\\"jcr:content/jcr:data\\").getLength() / (1024*1024);\\n}\\n```\\n\\nAs you can guess since we are navigating in the hierarchy you have to be very careful when using such query. This example is just a simple code sample to show you some of the cool features provided by the JCR API."},{"id":"/2011/01/02/ios-101-how-to-convert-a-string-to-a-nsdate","metadata":{"permalink":"/blog/2011/01/02/ios-101-how-to-convert-a-string-to-a-nsdate","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2011-01-02-ios-101-how-to-convert-a-string-to-a-nsdate.md","source":"@site/blog/2011-01-02-ios-101-how-to-convert-a-string-to-a-nsdate.md","title":"iOS 101: How to convert a String to a NSDate","description":"During my vacations, I took some time to play with iOS development. I have been struggling with many small issues... This is the price to pay when learning a new technology, and this is part of the fun of doing it. I will try to document some of these issues in articles...  Let\'s start with a very common story : working with date.","date":"2011-01-02T00:00:00.000Z","formattedDate":"January 2, 2011","tags":[],"readingTime":1.36,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"iOS 101: How to convert a String to a NSDate","categories":"ios"},"prevItem":{"title":"How to calculate the size of a folder in JCR (Java Content Repository)?","permalink":"/blog/2011/03/01/how-to-calculate-the-size-of-a-folder-in-jcr-java-content-repository"},"nextItem":{"title":"What Apple\u2019s Announcement Really Means to Java Developers","permalink":"/blog/2010/10/29/what-apples-announcement-really-means-to-java-developers"}},"content":"During my vacations, I took some time to play with iOS development. I have been struggling with many small issues... This is the price to pay when learning a new technology, and this is part of the fun of doing it. I will try to document some of these issues in articles...  Let\'s start with a very common story : working with date.\\n\\nObjective-C and iOS SDK provide a class to help formatting date (marshaling and unmarshaling), this class is [NSDateFormatter](http://developer.apple.com/library/mac/#documentation/Cocoa/Reference/Foundation/Classes/NSDateFormatter_Class/Reference/Reference.html). No surprise, the NSDateFormatter uses the [Unicode Date Format Patterns](http://unicode.org/reports/tr35/#Date_Format_Patterns).\\n\\nA small example of date creating from a string:\\n\\n``` objectivec\\n\\n    NSDateFormatter *dateFormatter = [[NSDateFormatter alloc]init];\\n    [dateFormatter setDateFormat:@\\"yyyy-MM-dd\\"];\\n    NSDate *date = [dateFormatter dateFromString:publicationDate ];\\n    [dateFormatter release];\\n     // use your date object\\n```\\n\\nThe date that I have to create from a sting looks like \\"`2010-11-12`\\". So I do not have any time information. When I do convert this string with the code above, the result is  \\"`2010-11-11 23:00:00 +0000`\\". As you can see the date is calculated from my current time zone, small reminder I am in France. So the \\"date\\" object itself is perfectly fine, but in my example I want to have the date independently of the time.\\n\\nTo be able to manage the date without any time/timezone information, I can force the timezone I want to use when using the  `NSDateFormatter` class. I just need to use the `setTimeZone` instance method.\\n\\nThe code looks like that now (see line#3):\\n\\n``` objectivec\\n    NSDateFormatter *dateFormatter = [[NSDateFormatter alloc]init];\\n    [dateFormatter setDateFormat:@\\"yyyy-MM-dd\\"];\\n    [dateFormatter setTimeZone:[NSTimeZone timeZoneForSecondsFromGMT:0]];\\n    NSDate *date = [dateFormatter dateFromString:publicationDate ];\\n    [dateFormatter release];\\n     // use your date object\\n```\\n\\nHope that helps!"},{"id":"/2010/10/29/what-apples-announcement-really-means-to-java-developers","metadata":{"permalink":"/blog/2010/10/29/what-apples-announcement-really-means-to-java-developers","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2010-10-29-what-apples-announcement-really-means-to-java-developers.md","source":"@site/blog/2010-10-29-what-apples-announcement-really-means-to-java-developers.md","title":"What Apple\u2019s Announcement Really Means to Java Developers","description":"Hey Steve, keep the bean in the Apple!","date":"2010-10-29T00:00:00.000Z","formattedDate":"October 29, 2010","tags":[],"readingTime":2.85,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What Apple\u2019s Announcement Really Means to Java Developers","categories":"java osx apple"},"prevItem":{"title":"iOS 101: How to convert a String to a NSDate","permalink":"/blog/2011/01/02/ios-101-how-to-convert-a-string-to-a-nsdate"},"nextItem":{"title":"Create and Deploy a JAX-RS REST service on Google App Engine","permalink":"/blog/2010/02/28/create-and-deploy-a-jax-rs-rest-service-on-google-app-engine"}},"content":"Hey Steve, keep the bean in the Apple!\\n\\nThe [news from last week](http://www.infoq.com/news/2010/10/apple-deprecates-java) that grabbed the attention of many Java developers was Apple\u2019s announcement of [its intentions to deprecate Java in the latest OS X 10.6 update](http://developer.apple.com/library/mac/#releasenotes/Java/JavaSnowLeopardUpdate3LeopardUpdate8RN/Introduction/Introduction.html). One sentence stood out in particular, \u201cDevelopers should not rely on the Apple-supplied Java runtime being present in future versions of Mac OS X,\u201d and raised the question: should Java developers (many of whom, like me, develop on Macs) freak out?\\n\\nI don\u2019t think so. (Though it prompted additional speculation and [follow-on news stories](http://gigaom.com/2010/10/26/java-is-under-siege-will-oracle-let-it-burn/).)\\n\\nLet\u2019s be realistic. Most applications run on the server side, on Unix/Linux and/or Windows Server \u2013 which has nothing to do with Apple or Mac OS X. And more and more applications are running on the cloud, where the language isn\u2019t necessarily irrelevant, but certainly less important than the services that the application exposes. And I\u2019m sure Java will have a big role in \u2018development in the cloud,\u2019 as we can already see with Google AppEngine and the VMWare/SpringSource effort.\\n\\nI think the more interesting question to ask is \u201cWhy did Apple do this?\u201d  \\n\\nI believe this is related Apple\u2019s other big news last week: the new \u201cMac App Store,\u201d which looks like an effort to have one single technology and language to develop \u201cofficial\u201d applications for Mac. In fact, for all Apple platforms running OS X and iOS, developers should use X Code and Objective C. That\u2019s fine with me, as I enjoy developing small apps for my iPhone and iPad in my spare time, using these tools. But at eXo, many of our developers are using Java, often on Macs, to build our software.\\n\\nWe\u2019re not talking about the same kind of applications. If, in the future, Java does not exist on Macs, it will not cause enterprise developers to abandon Java, but simply force them to move away from their Macs. Personally, I don\u2019t want that to happen. I switched to Mac in 2001, and I\u2019ve been a big fan of all Apple products ever since (most of my extended family are now also on Macs, and they couldn\u2019t care less about Java).\\n\\nAs a Java developer, do I switch back to PC now? Unlikely. I am very confident (overconfident?) that Java will still be present on OS X. The difference is that Apple will simply stop caring about it -- the same way that Microsoft doesn\u2019t care now. I cannot believe that Apple will stop/block Java on their platform. So the future of Java in general, and now on Mac, is fully under the control of the Java community, driven by Oracle and OpenJDK.  I am sure we will find many skilled \u201cMacAddicts\u201d to maintain and improve Java on OS X, to at least allow Java developers to run their favorite IDE and test their applications before deploying them on the servers -- keeping the \u201cWrite Once, Run Anywhere\u201d a reality (almost...). The only \u201cbad\u201d part is the fact that \u201cJava Desktop\u201d will not borrow any of the cool features of Apple Mac OS X. Not a big deal, since Java Desktop has never been that successful anyway.\\n\\nSo my advice to fellow Java developers is this: if you care, be vocal.  Let\u2019s make sure Apple lets the community drive the future of Java on Mac, since the future of the Java platform is still very exciting for many of us.\\n\\nOriginal Post on&nbsp;[eXo Blog](http://blog.exoplatform.org/2010/10/29/what-apples-announcement-really-means-to-java-developers/)."},{"id":"/2010/02/28/create-and-deploy-a-jax-rs-rest-service-on-google-app-engine","metadata":{"permalink":"/blog/2010/02/28/create-and-deploy-a-jax-rs-rest-service-on-google-app-engine","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2010-02-28-create-and-deploy-a-jax-rs-rest-service-on-google-app-engine.md","source":"@site/blog/2010-02-28-create-and-deploy-a-jax-rs-rest-service-on-google-app-engine.md","title":"Create and Deploy a JAX-RS REST service on Google App Engine","description":"In this article you will learn how to create a REST service using JAX-RS reference implementation (Jersey) and deploy it on Google AppEngine.","date":"2010-02-28T00:00:00.000Z","formattedDate":"February 28, 2010","tags":[],"readingTime":8.96,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Create and Deploy a JAX-RS REST service on Google App Engine","categories":"java gae google cloud rest"},"prevItem":{"title":"What Apple\u2019s Announcement Really Means to Java Developers","permalink":"/blog/2010/10/29/what-apples-announcement-really-means-to-java-developers"},"nextItem":{"title":"VirtualBox: How to clone a virtual machine?","permalink":"/blog/2010/02/24/virtualbox-how-to-clone-a-virtual-machine"}},"content":"In this article you will learn how to create a REST service using JAX-RS reference implementation (Jersey) and deploy it on Google AppEngine.\\n\\n###  Prerequisites\\n\\nFor this tutorial you will need:\\n\\n* a Google AppEngine account : [http://code.google.com/appengine/](http://code.google.com/appengine/)\\n* Eclipse Galileo (3.5.x)\\n* Google App Engine SDK for Java\\n\\n\x3c!-- truncate --\x3e\\n\\n* Install the Google Plugin for Eclipse as documented [here](http://code.google.com/eclipse/docs/install-eclipse-3.5.html) (Check that you are using the release 1.3.1 of the GAE Java SDK, if not download it and configure the plugin to use it)\\n  * it is also useful to have the AppEngine documentation locally, you can download it from [here](http://code.google.com/appengine/downloads.html#Download_the_Google_App_Engine_Documentation).\\n  * JAX-RS Reference Implementation, be sure you take the\\n  Jersey 1.1.5 release. You can download it from [here](https://jersey.dev.java.net/).\\n  * Unzip the file in a directory that we will call `$JERSEY_HOME`\\n  * JAXB 2.2 Implementation to simplify the marshalling/unmarshalling of the XML, and also facilitate the JSON support. Download it from [here](https://jaxb.dev.java.net/)\\n\\n### Creating new application\\n\\nTo create a new App Engine project in Eclipse:\\n\\n1-  Click on the \\"**New Web Application Project**\\" button ![](http://2.bp.blogspot.com/_aoQgQ1obiyE/S4pv-VRkifI/AAAAAAAAAKc/cNoURwz9n8M/s320/new_app_button.png) in the toolbar . It is also possible to do it using the menu **File &gt; Web Application Project**\\n\\n2-  The \\"Create a Web Application Project\\" wizard opens:\\n\\n* Project Name: `EmployeeService`\\n* Package : `com.grallandco.employee.service`\\n* Uncheck \\"`Use Google Web Toolkit`\\"\\n* Check that the SDK version your are using is \\"`App Engine 1.3.0`\\"; if not configure the project to use it.\\n* The screen should look like the following screen :\\n\\n![](http://3.bp.blogspot.com/_aoQgQ1obiyE/S4pu9erB81I/AAAAAAAAAKE/ER3aDZKd520/s320/app-engine-wizard.png )\\n\\n* Click Finish\\n* The project should look like the following screen :\\n\\n![](http://4.bp.blogspot.com/_aoQgQ1obiyE/S4pvDuZVWQI/AAAAAAAAAKM/8I2aUTtx7Os/s320/project-structure.png )\\n\\n\\n#### Running the application\\n\\nThe App Egine SDK, installed with the Eclipse plugin contains a Web server (based on Jetty), that could be used for testing and debugging. To test that your application has been created correctly select the menu **Run &gt; Run As &gt; Web Application**. I personally most of the time run my server using the debug command **Run &gt; DebugAs &gt; Web Application**. In debug mode you can change source code and test is without restarting the server.\\n\\nThe web server is starting automatically, you should see the following messagein the Eclipse console\\n\\nThe server is runningat `http://localhost:8080/`\\n\\nYou can access the application, and the sample servlet that has been created using the URL: `http://localhost:8080/employeeservice`.\\n\\nTo stop the server, click on the terminate button ![]( http://1.bp.blogspot.com/_aoQgQ1obiyE/S4pvYft_VPI/AAAAAAAAAKU/UCNDdfNlEnM/s320/terminate_button.png ) in the Eclipse console.\\n\\n### Configuring the REST support in the application\\n\\nTo be able to create and run REST services in your application you need to:\\n\\n* Add the JAX-RS, JAXB Jars in your project and application\\n* Configure the web application (web.xml) to handle REST requests **Add JAX-RS, JAXB to your project**\\n\\n1.  Right click on the project and select menu entry **Build Path &gt; Configure Build Path...**\\n2.  Click on the Add External JARs button\\n3.  Select all the JARs located in $JERSEY_HOME/lib and $JAXB_HOME/lib folders. You can for better visibility and reuse create a user library with all these JARs\\n4.  You also need to copy the JARs in the web-inf/lib directory of your application, this step is mandatory to be sure that the JARs are included in the application when deployed to App Engine.\\n *Note: I do not like this step. I would prefer to do that by configuration of the build path, to automatically add the JARs to the WEB-INF/lib directory when executing/deploying the application. Unfortunately I did not find the way to do it, so if you know it, feel free to post a comment and I will\\n  update the article.*\\n\\n**Configure the web application**\\n\\nIn this step you will register a new URI to handle REST requests. To do that you need to register a new servlet that is using the Jersey API and configure it to a specific URI (eg: /ressources  and/or /rest) and configure what are the Java packages that contain the REST implementation classes. So you need to modify the web.xml of your application with the following entries:\\n\\n``` xml\\n<servlet>\\n  <servlet-name>Jersey Web Application</servlet-name>\\n  <servlet-class>com.sun.jersey.spi.container.servlet.ServletContainer</servlet-class>\\n  <init-param>\\n    <param-name>com.sun.jersey.config.property.packages</param-name>\\n    <param-value>com.grallandco.employee.service.rest.impl</param-value>\\n  </init-param>\\n  <load-on-startup>1</load-on-startup>\\n</servlet>\\n<servlet-mapping>\\n  <servlet-name>Jersey Web Application</servlet-name>\\n  <url-pattern>/resources/*</url-pattern>\\n</servlet-mapping>\\n<servlet-mapping>\\n  <servlet-name>Jersey Web Application</servlet-name>\\n  <url-pattern>/rest/*</url-pattern>\\n</servlet-mapping>\\n```\\n\\nThis servlet that will answer to the /resources/ and /rest/ URL. The configuration parameter `com.sun.jersey.config.property.packages` is used by Jersey to list the packages where REST services implementation are located.Note that you can put as many package as you need to, you just need to separate the package names by a ; .\\n\\n### Creating a simple REST Service to test the environment\\n\\nThe project is now ready to contain REST service. It is time to create  one.Create for example the class `com.grallandco.employee.service.rest.impl.HelloWorldResource`, be sure to use the package name that you have configured in the web.xml for the Jersey servlet, based on the configuration we have made in previous step the package is `com.grallandco.employee.service.rest.impl`\\n\\nHere a sample class with the JAX-RS annotations:\\n\\n``` java\\npackage com.grallandco.employee.service.rest.impl;\\nimport javax.ws.rs.Path;\\nimport javax.ws.rs.GET;\\nimport javax.ws.rs.Produces;\\n@Path(\\"/hr/\\")\\npublic class EmployeeResource {\\n\\n  @GET\\n  @Produces(\\"text/plain\\")\\n  @Path(\\"/employee\\")\\n  public String getEmployee() {\\n    return \\"Hello World!\\";\\n  }\\n}\\n```\\n\\nYou should be able to test it, stop the server and run it again, enter the following URL in your browser:\\n\\n`http://localhost:8080/resources/hr/employee`\\n\\nor\\n\\n`http://localhost:8080/rest/hr/employee`\\n\\n#### Deploying the application to Google App Engine\\n\\nBefore deploying the application you need to register a new application in Google App Engine using the Administartion Console, see the documentation [here](http://code.google.com/appengine/docs/theadminconsole.html). In my example I have used \\"`tugdual`\\" as Application ID.\\n\\nYou can easily now deploy the application to Google App Engine by clicking on the  \\"Deploy App Engine Project\\" button \\"Deploy App Engine Project Button\\") available in the Eclipse toolbar.\\n\\nTo be able to deploy your application to Google App Engine, you need to check that your application can be registered, the application ID is stored  in the `WEB-INF/lib/appengine-web.xml`.\\n\\n``` xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<appengine-web-app xmlns=\\"http://appengine.google.com/ns/1.0\\">\\n  <application>[your-application-id]</application>\\n  <version>1</version>\\n  \x3c!-- Configure java.util.logging --\x3e\\n  <system-properties>\\n    <property name=\\"java.util.logging.config.file\\" value=\\"WEB-INF/logging.properties\\"/>\\n  </system-properties>\\n</appengine-web-app>\\n```\\n\\n\\nThe App Engine deploy button prompts you for multiple informations: username (your Google account) and password.\\n\\nWhen the deployment is complete you can access your application using the following URL:\\n\\n`http://[your-application-id].appspot.com/resources/hr/employee`\\n\\nor\\n\\n`http://[your-application-id].appspot.com/rest/hr/employee`\\n\\n### Ading XML and JSON support to the service\\n\\nLet\'s now add new method to manipulate an \\"Employee\\" object using the service, and the data format should be based on JSON and XML. This is where JAXB is useful, since it allows easily to transform marshall/unmarshall Java objects in XML -obviously- and JSON (cool isn\'t!)\\n\\n#### Creating an Employee Class\\n\\nStart with the creation of a new class to manipulate Employee data, this is a very simple Java class that could look like the following code:\\n\\n``` java\\npackage com.grallandco.employee.service.model;\\nimport java.util.Date;\\n\\npublic class Employee {\\n  private String firstName;\\n  private String lastName;\\n  private Date hireDate;\\n  private String email;\\n  public Employee(String firstName, String lastName, Date hireDate, String email) {\\n    this.firstName = firstName;\\n    this.lastName = lastName;\\n    this.hireDate = hireDate;\\n    this.email = email;\\n  }\\n  public Employee() {}\\n    public String getFirstName() {\\n      return firstName;\\n    }\\n    public void setFirstName(String firstName) {\\n      this.firstName = firstName;\\n    }\\n    public String getLastName() {\\n      return lastName;\\n    }\\n    public void setLastName(String lastName) {\\n      this.lastName = lastName;\\n    }\\n    public Date getHireDate() {\\n      return hireDate;\\n    }\\n    public void setHireDate(Date hireDate) {\\n      this.hireDate = hireDate;\\n    }\\n    public String getEmail() {\\n      return email;\\n    }\\n    public void setEmail(String email) {\\n      this.email = email;\\n    }\\n    public String toString() {\\n      StringBuffer sb = new StringBuffer();\\n      sb.append(\\"First: \\").append(getFirstName());\\n      sb.append(\\" - Last: \\").append(getLastName());\\n      sb.append(\\" - Date: \\").append(getHireDate());\\n      sb.append(\\" - Email: \\").append(getEmail());\\n      return sb.toString();\\n    }\\n  }\\n```\\n\\n\\nWhen implementing your \\"real\\" application with some persistence layer\\nthis POJO is the one as JDO/JPA entity.\\n\\n#### Create a Converter class for your entity\\n\\nI usually encapsulate all the transformation in some converter class, like that I do not directly couple my business class to the serialisation mechanism. (So I do that for classes and lists of classes). So instead of adding the JAXB annotations to the Employee class itself, let\'s create an EmployeeConverter class that will be responsible of the transformation and used by your REST service.\\n\\n``` java\\npackage com.grallandco.employee.service.converter;\\n\\nimport java.util.Date;\\nimport javax.xml.bind.annotation.XmlElement;\\nimport javax.xml.bind.annotation.XmlRootElement;\\nimport com.grallandco.employee.service.model.Employee;\\n\\n@XmlRootElement(name = \\"employee\\")\\npublic class EmployeeConverter {\\n  private Employee entity = null;\\n  public EmployeeConverter() {\\n    entity = new Employee();\\n  }\\n\\n  public EmployeeConverter(Employee entity) {\\n    this.entity = entity;\\n  }\\n\\n  @XmlElement\\n  public String getFirstName() {\\n    return entity.getFirstName();\\n  }\\n\\n  @XmlElement\\n  public String getLastName() {\\n    return entity.getLastName();\\n  }\\n\\n  @XmlElement\\n  public Date getHireDate() {\\n    return entity.getHireDate();\\n  }\\n\\n  @XmlElement\\n  public String getEmail() {\\n    return entity.getEmail();\\n  }\\n\\n  public Employee getEmployee() {\\n    return entity;\\n  }\\n\\n  public void setFirstName(String firstName) {\\n    entity.setFirstName(firstName);\\n  }\\n\\n  public void setHireDate(Date hireDate) {\\n    entity.setHireDate(hireDate);\\n  }\\n\\n  public void setLastName(String email) {\\n    entity.setEmail(email);\\n  }\\n\\n  public void setEmail(String lastName) {\\n    entity.setLastName(lastName);\\n  }\\n}\\n\\n```\\n\\nYou can now update your service to use this utility/converter class to return XML or JSON ojbect based on the content type of the request.\\n\\n#### Add support to JSON and XML to your REST service\\n\\nYou need to change the `EmployeeRessource` class, to change the signature and add new annotations of the `getEmployee()` method.\\n\\nThe annotation you are adding:\\n\\n* `@Produces({\\"application/xml\\", \\"application/json\\"})` : indicates which type of content will be produced by the service. Based on the type of the request.\\n* `@Path(\\"/employee/{employeeEmail}/\\")` : change the Path to indicate a Path parameter, here for example the URL can accept an email in the URI - not the best example, but you get the point...\\n* `public EmployeeConverter getEmployee( @PathParam (\\"employeeEmail\\") String email)` : change the type returned by the method and take a parameter as String that match the Path param defined in the @Path annotation.\\n\\nHere the complete class code:\\n\\n``` java\\npackage com.grallandco.employee.service.rest.impl;\\n\\nimport javax.ws.rs.Path;\\nimport javax.ws.rs.GET;\\nimport javax.ws.rs.PathParam;\\nimport javax.ws.rs.Produces;\\nimport com.grallandco.employee.service.converter.EmployeeConverter;\\nimport com.grallandco.employee.service.model.Employee;\\n\\n@Path(\\"/hr/\\")\\npublic class EmployeeRessource {\\n\\n\\n  @GET\\n  @Produces({\\"application/xml\\", \\"application/json\\"})\\n  @Path(\\"/employee/{employeeEmail}/\\")\\n  public EmployeeConverter getEmployee( @PathParam (\\"employeeEmail\\") String email) {\\n    //dummy code\\n    Employee emp = new Employee();\\n    emp.setEmail(email);\\n    emp.setFirstName(\\"John\\");\\n    emp.setLastName(\\"Doe\\");\\n    EmployeeConverter converter = new EmployeeConverter(emp);\\n    return converter;\\n  }\\n}\\n\\n```\\n\\n### Test the service\\n\\nYou can now run the server locally and test the service\\n\\n`http://localhost:8080/resources/hr/employee/tug@grallandco.com`\\n\\nThis will return an XML document.\\n\\nIf you want to test the JSON call you have multiple choice:\\n\\n* Using following command\\n\\n``` bash\\ntgrall$ curl -H \\"Accept: application/json\\" http://localhost:8080/resources/hr/employee/tug@grallandco.com\\n    {\\"email\\":\\"tug@grallandco.com\\",\\"firstName\\":\\"John\\",\\"lastName\\":\\"Doe\\"}\\n```\\n\\n* Using an HTTP client that allows your to configure/set the HTTP request completely, I am using the [Poster Firefox Plugin](https://addons.mozilla.org/en-US/firefox/addon/2691)\\n* Using some Javascript code in an application\\n\\nYou can repeat the test on your deployed application on Google AppEngine.\\n\\n### Conclusion\\n\\nIn this article you have learned how to create and deploy a new REST Service on Google App Engine. This service has been created with the JAX-RS Reference Implementation the Jersey project. In the next article you will learn how to add persistence and create a CRUD Rest service on Google App Engine."},{"id":"/2010/02/24/virtualbox-how-to-clone-a-virtual-machine","metadata":{"permalink":"/blog/2010/02/24/virtualbox-how-to-clone-a-virtual-machine","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2010-02-24-virtualbox-how-to-clone-a-virtual-machine.md","source":"@site/blog/2010-02-24-virtualbox-how-to-clone-a-virtual-machine.md","title":"VirtualBox: How to clone a virtual machine?","description":"During some testing I had to put in place a cluster on my network. So I create a first virtual machine. It is not possible to directly copy the Virtual Disk Image (.vdi). VirtualBox saved in each disk image a UUID that is also store inside the virtual machine image. VirtualBox does not support two images with the same number. So to clone the an image you need to use the VBoxManage clonehd command line.","date":"2010-02-24T00:00:00.000Z","formattedDate":"February 24, 2010","tags":[],"readingTime":0.935,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"VirtualBox: How to clone a virtual machine?","categories":null},"prevItem":{"title":"Create and Deploy a JAX-RS REST service on Google App Engine","permalink":"/blog/2010/02/28/create-and-deploy-a-jax-rs-rest-service-on-google-app-engine"},"nextItem":{"title":"NantesJUG: Next Meeting Friday November 13th about \'Google Technologies\'","permalink":"/blog/2009/11/08/nantesjug-next-meeting-friday-november-13th-about-google-technologies"}},"content":"During some testing I had to put in place a cluster on my network. So I create a first virtual machine. It is not possible to directly copy the Virtual Disk Image (.vdi). VirtualBox saved in each disk image a UUID that is also store inside the virtual machine image. VirtualBox does not support two images with the same number. So to clone the an image you need to use the `VBoxManage clonehd` command line.\\n\\nThe clonehd command copy the VDI file and assigns a new UUID into it.\\n\\n``` Bash\\nVBoxManage  clonehd /opt/tools/vm/vm1-rhel.vdi  /opt/tools/vm/vm2-rhel.vdi\\n```\\n\\nOnce the copy is done, you can now register this new VDI in your VirtualBox environment and create a new virtual machine.\\n\\nNote: I am running VirtualBox on MacOS X, and I needed to put complete path to VDI files, if not the command id not working\\n\\n_Alternative approach_\\n\\nInitially I had issue with the clonehd command since I was not using full path. So what you can do is:\\n\\n``` bash\\ncp vm1-rhel.vdi vm2-rhel.vdi\\nVBoxManage internalcommands sethduuid vm2-rhel.vdi\\n```\\n\\nYou can now add the new VDI to your VirtualBox environment."},{"id":"/2009/11/08/nantesjug-next-meeting-friday-november-13th-about-google-technologies","metadata":{"permalink":"/blog/2009/11/08/nantesjug-next-meeting-friday-november-13th-about-google-technologies","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-11-08-nantesjug-next-meeting-friday-november-13th-about-google-technologies.md","source":"@site/blog/2009-11-08-nantesjug-next-meeting-friday-november-13th-about-google-technologies.md","title":"NantesJUG: Next Meeting Friday November 13th about \'Google Technologies\'","description":"This Friday , the Nantes JUG is pleased to organize a conference about \\"Google Technologies\\". Didier Girard, http","date":"2009-11-08T00:00:00.000Z","formattedDate":"November 8, 2009","tags":[],"readingTime":0.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"NantesJUG: Next Meeting Friday November 13th about \'Google Technologies\'","categories":"conference jug"},"prevItem":{"title":"VirtualBox: How to clone a virtual machine?","permalink":"/blog/2010/02/24/virtualbox-how-to-clone-a-virtual-machine"},"nextItem":{"title":"USI2009: The Geek and Boss French Conference","permalink":"/blog/2009/07/08/usi2009-the-geek-and-boss-french-conference"}},"content":"![]( http://sites.google.com/site/nantesjug/_/rsrc/1222637453551/config/app/images/customLogo/customLogo.gif?revision=2 )\\n\\nThis Friday , the Nantes JUG is pleased to organize a conference about \\"Google Technologies\\". Didier Girard, http://twitter.com/dgirard , will be presenting and demoing cool stuff from Google:\\n\\n* GWT 2\\n* Android\\n* Wave\\n* App Engine\\n\\nWant to come? Register for free [here](http://jugevents.jugpadova.it/jugevents/event/registration.form?event.id=20744)\\n\\nThe event will occur at the \\"[Ecole des Mines de Nantes](http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=ecole+des+mines+de+nantes&amp;sll=47.176415,-1.340556&amp;sspn=0.078063,0.181103&amp;ie=UTF8&amp;hq=%C3%89cole+des+mines+de+Nantes&amp;hnear=%C3%89cole+des+mines+de+Nantes,+France&amp;ll=47.283363,-1.520834&amp;spn=0.019476,0.045276&amp;t=h&amp;z=15&amp;iwloc=near)\\" one of our sponsor from 7 to 9 pm."},{"id":"/2009/07/08/usi2009-the-geek-and-boss-french-conference","metadata":{"permalink":"/blog/2009/07/08/usi2009-the-geek-and-boss-french-conference","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-07-08-usi2009-the-geek-and-boss-french-conference.md","source":"@site/blog/2009-07-08-usi2009-the-geek-and-boss-french-conference.md","title":"USI2009: The Geek and Boss French Conference","description":"This year I was lucky enough to have a presentation at the second edition of the \\"Universit\xe9 du SI\\", organized by Octo Technologies. I have to say that this conference is one of the best that I have attended, for sure it is the best in France. Unfortunately I was only able to attend the first day of the conference, but even in one day, I was very happy with the content of the presentations, keynotes, and networking opportunities.","date":"2009-07-08T00:00:00.000Z","formattedDate":"July 8, 2009","tags":[],"readingTime":1.14,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"USI2009: The Geek and Boss French Conference","categories":"conference presentation exo portal"},"prevItem":{"title":"NantesJUG: Next Meeting Friday November 13th about \'Google Technologies\'","permalink":"/blog/2009/11/08/nantesjug-next-meeting-friday-november-13th-about-google-technologies"},"nextItem":{"title":"Next Generation Portals : How OpenSocial Standard Adds Social to the Mix?","permalink":"/blog/2009/04/07/next-generation-portals-how-opensocial-standard-adds-social-to-the-mix"}},"content":"This year I was lucky enough to have a presentation at the second edition of the \\"[Universit\xe9 du SI](http://www.universite-du-si.com/)\\", organized by [Octo Technologies](http://octo.com/). I have to say that this conference is one of the best that I have attended, for sure it is the best in France. Unfortunately I was only able to attend the first day of the conference, but even in one day, I was very happy with the content of the presentations, keynotes, and networking opportunities.\\n\\nI won\'t go in details in all the presentations that I have seen, Google for the Enterprise, Application Server Future, Usability concerns, and keynotes. If you want to have a good feedback about this conference I invite you to read, in French, the reports from [Le Touilleur Express](http://www.touilleur-express.fr/tag/usi/).\\n\\nLet me just share the presentation that I gave with [Vincent Massol](http://massol.net/) from [XWiki](http://www.xwiki.com/), about CMS vs Wiki.\\n\\n**Wiki vs CMS duel**\\n\\nFirst of all, the room was packed, so it looks like it is an interesting subject for many of you, so do not hesitate to post comments or question on this entry. Vincent and I will be pleased to update our presentation for a new event.\\n\\nThe main message of the talk was:\\n\\n* **For collaboration on content** the wiki is king\\n* **For publication of content** the CMS is king\\n\\n[Wiki vs CMS ](http://www.slideshare.net/tgrall/wiki-vs-cms-1678352 \\"Wiki vs CMS \\")"},{"id":"/2009/04/07/next-generation-portals-how-opensocial-standard-adds-social-to-the-mix","metadata":{"permalink":"/blog/2009/04/07/next-generation-portals-how-opensocial-standard-adds-social-to-the-mix","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-04-07-next-generation-portals-how-opensocial-standard-adds-social-to-the-mix.md","source":"@site/blog/2009-04-07-next-generation-portals-how-opensocial-standard-adds-social-to-the-mix.md","title":"Next Generation Portals : How OpenSocial Standard Adds Social to the Mix?","description":"I have published on SlideShare a new presentation about OpenSocial and eXo Platform.","date":"2009-04-07T00:00:00.000Z","formattedDate":"April 7, 2009","tags":[],"readingTime":0.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Next Generation Portals : How OpenSocial Standard Adds Social to the Mix?","categories":"exo portal presentation"},"prevItem":{"title":"USI2009: The Geek and Boss French Conference","permalink":"/blog/2009/07/08/usi2009-the-geek-and-boss-french-conference"},"nextItem":{"title":"Conference season for eXo Platform in Paris","permalink":"/blog/2009/03/30/conference-season-for-exo-platform-in-paris"}},"content":"I have published on SlideShare a new presentation about OpenSocial and eXo Platform.\\n\\nFeel free to test OpenSocial and eXo Platform integration by downloading latest release of eXo products.\\n\\n[Open Social Introduction - JUG SummerCamp 2010](https://www.slideshare.net/tgrall/open-social-introduction-jug-summercamp-2010)"},{"id":"/2009/03/30/conference-season-for-exo-platform-in-paris","metadata":{"permalink":"/blog/2009/03/30/conference-season-for-exo-platform-in-paris","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-03-30-conference-season-for-exo-platform-in-paris.md","source":"@site/blog/2009-03-30-conference-season-for-exo-platform-in-paris.md","title":"Conference season for eXo Platform in Paris","description":"eXo Platform, and I, will be present in conferences in the upcoming weeks:","date":"2009-03-30T00:00:00.000Z","formattedDate":"March 30, 2009","tags":[],"readingTime":0.635,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Conference season for eXo Platform in Paris","categories":"conference exo"},"prevItem":{"title":"Next Generation Portals : How OpenSocial Standard Adds Social to the Mix?","permalink":"/blog/2009/04/07/next-generation-portals-how-opensocial-standard-adds-social-to-the-mix"},"nextItem":{"title":"JAX-WS: How to configure the service end point at runtime?","permalink":"/blog/2009/02/17/jax-ws-how-to-configure-the-service-end-point-at-runtime"}},"content":"eXo Platform, and I, will be present in conferences in the upcoming weeks:\\n\\n* [Linux Solutions](http://www.solutionslinux.fr/), March 31st - April 2nd : In addition to the demonstration pod where you can meet eXo people, I am inviting you to joing us during the [OW2 Annual Conference](http://www.ow2.org/view/Events2009AnnualConference/) presentations:\\n   * Next generation Portals: how OpenSocial standard adds social to the mix (April 2, 01:30 - 02:00)\\n   * Which Portlet Bridge is made for you? (April 2, 02:00 - 02:30)\\n\\nYou can find the [full program here](http://www.ow2.org/view/Events2009AnnualConference/Sessions).\\n\\n* [Salon Intranet](http://www.salon-intranet.com/), May 12th,13th : Once again, eXo will be present with a demonstration pod, but also come to meet eXo CEO, Benjamin Mestrallet and myself during the \\"eXo Platform, the Open Source solution for your Intranet\\" on May 12th from 3pm-4pm."},{"id":"/2009/02/17/jax-ws-how-to-configure-the-service-end-point-at-runtime","metadata":{"permalink":"/blog/2009/02/17/jax-ws-how-to-configure-the-service-end-point-at-runtime","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-02-17-jax-ws-how-to-configure-the-service-end-point-at-runtime.md","source":"@site/blog/2009-02-17-jax-ws-how-to-configure-the-service-end-point-at-runtime.md","title":"JAX-WS: How to configure the service end point at runtime?","description":"When deploying your Web Service client you often need to change the endpoint of the service that &nbsp;has been set during the code generation. This short post explains how you can set change it at runtime in the client code.","date":"2009-02-17T00:00:00.000Z","formattedDate":"February 17, 2009","tags":[],"readingTime":1.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JAX-WS: How to configure the service end point at runtime?","categories":null},"prevItem":{"title":"Conference season for eXo Platform in Paris","permalink":"/blog/2009/03/30/conference-season-for-exo-platform-in-paris"},"nextItem":{"title":"Interest of Enterprise Portals","permalink":"/blog/2009/02/10/interest-of-enterprise-portals"}},"content":"When deploying your Web Service client you often need to change the endpoint of the service that &nbsp;has been set during the code generation. This short post explains how you can set change it at runtime in the client code.\\n\\nYou have two approaches to do that:\\n\\n* set the endpoint in the `Port` using the `BindingProvider`\\n* get the endpoint `URL` from the WSDL itself at runtime\\n\\n### Use the Binding Provider to set the endpoint URL\\n\\nThe first approach is to change the\\nBindingProvider.ENDPOINT_ADDRESS_PROPERTY property value of the\\nBindingProvider (Port) using the following code:\\n\\n``` java\\ntry {\\n  EmployeeServiceService service = new EmployeeServiceService();\\n  EmployeeService port = service.getEmployeeServicePort();\\n\\n  BindingProvider bp = (BindingProvider)port;\\n  bp.getRequestContext().put(BindingProvider.ENDPOINT_ADDRESS_PROPERTY, \\"http://server1.grallandco.com:8282/HumanRessources/EmployeeServiceService\\");\\n\\n  Employee emp = port.getEmployee(123);\\n\\n  System.out.println(\\"Result = \\"+ emp);\\n} catch (Exception ex) {...}\\n```\\n\\n### Use the WSDL to get the endpoint URL\\n\\nAnother part is to set the WSDL when you are creating the Service. The\\nservice will be using the value that is located in the WSDL port -SOAP\\nEndpoint-. This is simply done using the following code:\\n\\n``` java\\ntry {\\n  EmployeeServiceService service =\\n  new org.demo.service.EmployeeServiceService\\n  (new URL(\\"http://server1.grallandco.com:8282/HumanRessources/EmployeeServiceService?wsdl\\"),\\n  new QName(\\"http://service.demo.org/\\",\\"EmployeeServiceService\\"));\\n\\n  EmployeeService port = service.getEmployeeServicePort();\\n  Employee emp = port.getEmployee(123);\\n\\n  System.out.println(\\"Result = \\"+ emp);\\n} catch (Exception ex) { ... }\\n```\\n\\nNote that, in Glassfish, like lot of Web Service environments the WSDL can generate dynamically the Endpoint URL based on the URL used\\n    &nbsp;to get the WSDL. With this approach you can also dynamically\\n    change the Soap endpoint. (If compatible with the network configuration\\n      of the production environment.)"},{"id":"/2009/02/10/interest-of-enterprise-portals","metadata":{"permalink":"/blog/2009/02/10/interest-of-enterprise-portals","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-02-10-interest-of-enterprise-portals.md","source":"@site/blog/2009-02-10-interest-of-enterprise-portals.md","title":"Interest of Enterprise Portals","description":"I am writing this post as an answer to Christian Faure\'s blog post, (in French), about interest of enterprise portals. Let me take each point, one by one and comment them. I won\'t go in the all the details of many other points that why new enterprise portals are interesting for many of us, I just want to focus on Christian\'s remarks.","date":"2009-02-10T00:00:00.000Z","formattedDate":"February 10, 2009","tags":[],"readingTime":9.1,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Interest of Enterprise Portals","categories":"portal"},"prevItem":{"title":"JAX-WS: How to configure the service end point at runtime?","permalink":"/blog/2009/02/17/jax-ws-how-to-configure-the-service-end-point-at-runtime"},"nextItem":{"title":"Screen Cast: eXo WebOS: How to use a Google Gadget as an eXo WebOS Application","permalink":"/blog/2009/01/26/screen-cast-exo-webos-how-to-use-a-google-gadget-as-an-exo-webos-application"}},"content":"I am writing this post as an answer to [Christian Faure\'s blog post](http://www.christian-faure.net/2009/01/31/de-linteret-dun-portail-dentreprise/), (in French), about interest of enterprise portals. Let me take each point, one by one and comment them. I won\'t go in the all the details of many other points that why new enterprise portals are interesting for many of us, I just want to focus on Christian\'s remarks.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Single Access Point\\n\\n> Christian\'s quote: I do not need it, I have bookmarks, that I can put in any of my internet browser, allowing me to access any Web applications that I am using.\\n\\nI think that here it is a little broader that simply the bookmarks, but let starts with this.\\n\\nSome other important part of the \\"single access point\\" is the fact that it becomes your home page in your working environment where you not only have access to many \\"links\\"  but also you have some information that are pushed to you in a personalized and automatic fashion without having to go to each applications, some example:\\n\\n* a manager see that he has some holiday or expenses to approve\\n* a content editor see which content has been modified and should be validated to be published.\\n\\nAnother interesting point, is the fact that the IT &amp; Business people define together what is the important view for this \\"portal home page\\" providing automatically the needed information to the users depending of its profile.\\n\\nFinally you can access your portal from everywhere, from any browser, so your \u201cworking environment\u201d is always with you - from any browser, or computer/device. If you are using your browser bookmark you are \u201climited\u201d to your computer only (I know you can use internet bookmak service \xe0 la Delicious)\\n\\n##### Many Application screens in the same time\\n\\n> Christian\'s quote: I do not need that since I can use many windows, or tabs in my Web browser\\n\\nFirst of all, deploying a portal does not mean that you \\"must\\" execute all the applications in it. But I have to admit that it is quite useful to be able to have a quick overview of all the applications, content that you need to be aware of. When designing your portal you first design the way you want to aggregate the content based on the user profiles/roles. This is a key point in a portal project, you need to take time to choose the information you will make available to your user.\\n\\nI can reuse the example of the business process/workflow validation from the portal page. Since you are authenticated, information can be pushed to your page to inform you that you have some tasks to manage and this without to have to go to the application itself. It could make you more efficient.\\n\\nAll these do not mean that you have many application screens in the same time but an overview of your most important application. Then based on the design of your application or portal you will jump to the application in another window/tab, or use the application in the portal.\\n\\n### Single Authentication Point\\n\\n> Christian\'s quote:  Any Active Directory or LDAP will do that the same way\\n\\nPortal is not necessary a \\"Single Authentication Point\\" it depends how you look at it.\\n\\nIf you have multiple Web applications, including the portal, Portal is not necessary the single entry point, any application that will share the same SSO will be the entry point. You will access one of the application (the portal or another), the SSO will ask you for your crendetials, that will then be shared by all the applications in the same \u201cdomain\u201d.  A good example is the Google applications, you are not forced to enter the system by iGoogle (the portal) to access in a secured way all the applications. This is a pure WebSSO concern that is not directly related to Enterprise Portals.\\n\\nThat said, Portals provide also a mechanism for SSO. The portal is here to propagate the identity to all the application that are integrated with it.\\n\\nLet\u2019s reuse the example of the expenses validation. The manager is authenticated to the portal. The workflow/BPM application is integrated to the portal as portlet (most BPM product provides such portlets, in eXo Platform we have dedicated portlet allowing you to integrate easily JBPM and OW2 Bonita). The cool thing here is the fact that the user identity is transmitted by the portal to the portlet so the \u201cBPM\u201d portlet knows who the user is and can execute the process in this context.  (You do not have to configure a WebSSO here since the credentials are managed by the backend)\\n\\n### Portal provides \'personnalized views\'\\n\\n> Christian\'s quote:  Who needs that?\\n\\nHere we need to see two points:\\n\\n1. the personalization: the end user that really has it \\"own and Personalized\\" page\\n2. the profiling: a view of the portal that is based on the profile (group, business role) of the connected user.\\n\\n*Personalization*\\n\\nFor the first point, personalization, we need to see what are the various personalization features that a portal can offer to the users.\\n\\nIt is true that many portals, including eXo Portal with User Pages, allow any user to create, rearrange pages. This features has probably most of the time be oversold by the vendors. Standard users do not \\"adapt\\" their tools, they most of the time use the portal as it is defined by the managers.\\n\\nAnother very important part of the personalization is the fact that portlets can be personalized to the user (by him or for him), for example a WebMail portlet that allows you to see \\"your\\" mails and select some properties (number of mails per pages, order, ...) This part is quite useful when you put a portal in place.\\n\\nFinally, with the arrivals of new generation \u201cconsumer portals\u201d such as iGoogle, Netvibes, ... users have started to create their own dashboard based on a set of simple gadgets. Enterprise Portal, starting with eXo Portal, provide now these features that are simpler to use than standard portal page creation.\\n\\n*Profiling*\\n\\nSo personalization is quite important but the real benefits of most of the enterprise portals, so not come directly from the \u201cpersonalization\u201d but more from the profiling. The portal, and applications are managed based on the role of the user in the enterprise.\\n\\nThe pages you can see, the content of the applications available on the pages are not managed directly by you, but by your role. This is very useful to help people to be more productive. The flip side of that: when designing/deploying a portal it is important to take some time to understand the way people are working to give them \u201ctheir portal\u201d. The use of the profiling will help the IT to provide a business view of the information system.\\n\\n### An page with integrated applications\\n\\n> Christian\'s quote:  Who is \\"working in a portal? A simple personalizes page with widgets will offer me the perfect dashboard to manage my daily work and launch a full page with the application if needed.\\n\\nFor sure today\'s portal are not built to be used directly to \\"work\\" in them, but this is more a design issue, than an user experience one. When the application is already built and working perfectly used in a stand alone mode, no point to entirely \\"portletize\\" it to run it in the portal. When it is the case just develop Web services and publish them in the portal using Portlets or Gadgets.\\n\\nAt the opposite when you build a new application from scratch, it may be useful to develop it to be executed in a Portal, to leverage interesting services of a portal: profiling, integrated SSO, centralized management, ... The integration of applications in Portal are made even simpler today with the concept of Portlet Bridges allowing the developer to use a rich framework (JSF, Seam, ...) and publish the application in the portal.\\n\\nYes I do agree with that, you can see the portal as simple dashboard, but in reality if you take some time to design it properly you can also use the application in it. Portlets have been made for that. You can for example design your applications/portlets to leverage the state of the portlet: when normal you use it as a \u201csimple view\u201d on your dashboard, when maximize you can use the full application.\\n\\nAnother important point, it is the fact that with the portal the user has a virtualize working environnement that is not dependant of the client machine but work in a simple browser. With eXo WebOS, we have pushed the concept further to provide a Web based operating system giving access in a multi window environment to the various application and content from anywhere with a rich user experience. (So we are pushing all the benefits of Enterprise Portal into an virtual operating system)\\n\\n### Conclusion\\n\\nIn my opinion enteprise portals are still very interesting. We need to keep in mind that here we are talking about \\"enterprise\\", just to say that deploying a portal in the enteprise is also here to provide consistent user experience to the user, facilitate the management of the applications and users for the IT, etc etc...\\n\\nLike any other IT project, the Portal project should be managed and validated by user and management (Business and IT), if not it will be a failure. So when starting such project it is important to scope it correctly and deploy it step by step. (it is a content oriented portal or an intergration one?; which type of users will be the first audience? Which content &amp; data will be the most important for them, ...) Another important point is, on the technology side, how the portal that I will chose can be integrated with my current IS?  Finally when you start the project always start with the \\"most bang for the buck\\" approach, choose the community of users and applications that will help you to sell the project.\\n\\nIn my arguments, I have been focused on a very centralized view of the enterprise (the IT and Business people choose for the rest of us), this is because it is one important point in current portal project. But do not get me wrong, I am also a big fan of the community driven portals and tools. The next step in the portal deployment is to give more power to the user letting them create not \\"their own portal\\" but use the existing infrastructure to create portals/sub-portals based on their center of interest/role in the enterprise. This is where all the concepts coming from the Social Networks will be very useful in the enterprise, At eXo Platform, we have integrated in our portal an Open Social container, that is fully working in the context of the Portal and Web OS to facilitate the created of Enterprise Social Network Portals."},{"id":"/2009/01/26/screen-cast-exo-webos-how-to-use-a-google-gadget-as-an-exo-webos-application","metadata":{"permalink":"/blog/2009/01/26/screen-cast-exo-webos-how-to-use-a-google-gadget-as-an-exo-webos-application","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2009-01-26-screen-cast-exo-webos-how-to-use-a-google-gadget-as-an-exo-webos-application.md","source":"@site/blog/2009-01-26-screen-cast-exo-webos-how-to-use-a-google-gadget-as-an-exo-webos-application.md","title":"Screen Cast: eXo WebOS: How to use a Google Gadget as an eXo WebOS Application","description":"In this short screen cast I show you, how you can easily take a Google Gadget and use it inside WebOS.","date":"2009-01-26T00:00:00.000Z","formattedDate":"January 26, 2009","tags":[],"readingTime":0.16,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Screen Cast: eXo WebOS: How to use a Google Gadget as an eXo WebOS Application","categories":"exo portal"},"prevItem":{"title":"Interest of Enterprise Portals","permalink":"/blog/2009/02/10/interest-of-enterprise-portals"},"nextItem":{"title":"eXo Portal and Google Gadgets: Great post from Laurent Bois","permalink":"/blog/2008/11/24/exo-portal-and-google-gadgets-great-post-from-laurent-bois"}},"content":"In this short screen cast I show you, how you can easily take a Google Gadget and use it inside WebOS.\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/ijD0ZPnTcKg\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>"},{"id":"/2008/11/24/exo-portal-and-google-gadgets-great-post-from-laurent-bois","metadata":{"permalink":"/blog/2008/11/24/exo-portal-and-google-gadgets-great-post-from-laurent-bois","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-11-24-exo-portal-and-google-gadgets-great-post-from-laurent-bois.md","source":"@site/blog/2008-11-24-exo-portal-and-google-gadgets-great-post-from-laurent-bois.md","title":"eXo Portal and Google Gadgets: Great post from Laurent Bois","description":"I cannot find time to post any interesting things lately, so I can only share great stuff with readers.","date":"2008-11-24T00:00:00.000Z","formattedDate":"November 24, 2008","tags":[],"readingTime":0.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"eXo Portal and Google Gadgets: Great post from Laurent Bois","categories":"exo portal"},"prevItem":{"title":"Screen Cast: eXo WebOS: How to use a Google Gadget as an eXo WebOS Application","permalink":"/blog/2009/01/26/screen-cast-exo-webos-how-to-use-a-google-gadget-as-an-exo-webos-application"},"nextItem":{"title":"(French) eXo Platform Nominee for 01.Net awards.","permalink":"/blog/2008/10/06/french-exo-platform-nominee-for-01-dot-net-awards"}},"content":"I cannot find time to post any interesting things lately, so I can only share great stuff with readers.\\n\\nThis week end, [Laurent Bois](http://www.laurentbois.com/) has published a great post about:\\n\\n* [Deploying a remote (OpenSocial) Google Gadget made with GWT in eXo](http://laurentbois.com/2008/11/23/deploy-a-remote-opensocial-google-gadget-made-with-gwt-in-exo/)\\n\\nTake a look to this tutorial, that is based on eXo Portal 2.5 and eXo WebOS 1.5.\\n\\n![]( http://1.bp.blogspot.com/_aoQgQ1obiyE/SSqarvVSitI/AAAAAAAAAF0/GnCQ-Y-17xo/s320/image-29.png )\\n\\n\\nThanks Laurent for this great post!"},{"id":"/2008/10/06/french-exo-platform-nominee-for-01-dot-net-awards","metadata":{"permalink":"/blog/2008/10/06/french-exo-platform-nominee-for-01-dot-net-awards","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-10-06-french-exo-platform-nominee-for-01-dot-net-awards.md","source":"@site/blog/2008-10-06-french-exo-platform-nominee-for-01-dot-net-awards.md","title":"(French) eXo Platform Nominee for 01.Net awards.","description":"eXo Platform is a nominee in the 01.net Pro 2008 awards. If you like our product I am inviting you to vote for our solution:","date":"2008-10-06T00:00:00.000Z","formattedDate":"October 6, 2008","tags":[],"readingTime":0.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"(French) eXo Platform Nominee for 01.Net awards.","categories":"exo portal"},"prevItem":{"title":"eXo Portal and Google Gadgets: Great post from Laurent Bois","permalink":"/blog/2008/11/24/exo-portal-and-google-gadgets-great-post-from-laurent-bois"},"nextItem":{"title":"New cool tools that  I am using: DabbleBoard &amp; Balsamiq","permalink":"/blog/2008/08/07/new-cool-tools-that-i-am-using-dabbleboard-and-balsamiq"}},"content":"![](http://2.bp.blogspot.com/_aoQgQ1obiyE/SOnBLoos9wI/AAAAAAAAAFU/H8_WYRtid5U/s400/01trophy.png )\\n\\n[eXo Platform](http://www.exoplatform.org/) is a nominee in the [01.net](http://www.01net.com/) Pro 2008 awards. If you like our product I am inviting you to vote for our solution:\\n\\n*   [01Net Pro Awards](http://www.01net.com/contenu/3322/trophees2008/trophees-pro-1)In addition to the professional awards you can also vote for your favorites products, technologies, entrepreneurs, ... :\\n\\n*   [01Net Technologies Awards](http://www.01net.com/contenu/4681/trophees2008/trophees-1)And if you do not know eXo Platform yet... this is a good time to test it!"},{"id":"/2008/08/07/new-cool-tools-that-i-am-using-dabbleboard-and-balsamiq","metadata":{"permalink":"/blog/2008/08/07/new-cool-tools-that-i-am-using-dabbleboard-and-balsamiq","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-08-07-new-cool-tools-that-i-am-using-dabbleboard-and-balsamiq.md","source":"@site/blog/2008-08-07-new-cool-tools-that-i-am-using-dabbleboard-and-balsamiq.md","title":"New cool tools that  I am using: DabbleBoard &amp; Balsamiq","description":"When working on software design, UI Mockups are quite important and Patrice, colleague of mine at eXo has pointed me to a very cool tool to use when you have to quickly do a mockup, and work with the dev team in an iterative fashion","date":"2008-08-07T00:00:00.000Z","formattedDate":"August 7, 2008","tags":[],"readingTime":0.96,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New cool tools that  I am using: DabbleBoard &amp; Balsamiq","categories":null},"prevItem":{"title":"(French) eXo Platform Nominee for 01.Net awards.","permalink":"/blog/2008/10/06/french-exo-platform-nominee-for-01-dot-net-awards"},"nextItem":{"title":"A month in eXo... busy busy busy...","permalink":"/blog/2008/07/29/a-month-in-exo-dot-dot-dot-busy-busy-busy-dot-dot-dot"}},"content":"When working on software design, UI Mockups are quite important and Patrice, colleague of mine at eXo has pointed me to a very cool tool to use when you have to quickly do a mockup, and work with the dev team in an iterative fashion: This tool is \\"[Balsamiq Mockup](http://www.balsamiq.com/products/mockups)\\". Here an example of mockup realized with Balsamiq:\\n\\n[![](http://www.balsamiq.com/images/myTunez.gif)](http://www.balsamiq.com/images/myTunez.gif)\\n\\nAlso if you read more about the tool, you can see that it has native integration with others tools such as JIRA, so cool to be able to integrate a mockup easily when defining a new item...\\n\\nTake a look to this screencast explaining the basic features and use of the product:\\n\\n<iframe width=\\"675\\" height=\\"380\\" src=\\"https://www.youtube.com/embed/aJTuFRaIi_g\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n\\nAnother interesting tool is the online white board \\"[DabbleBoard](http://www.dabbleboard.com/main)\\" that allows user to quickly create and share graphs, ideas, ... A very cool feature of this tool is the automatic recognition of shapes. The best way to understand, is simply to see it in action:\\n\\n<object classid=\\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\\" id=\\"viddler\\" height=\\"342\\" width=\\"437\\">\\n<param name=\\"movie\\" value=\\"http://www.viddler.com//simple_on_site/a95e1956\\" />\\n<param name=\\"allowScriptAccess\\" value=\\"always\\" />\\n<param name=\\"allowFullScreen\\" value=\\"true\\"/>\\n<embed src=\\"http://www.viddler.com//simple_on_site/a95e1956\\" type=\\"application/x-shockwave-flash\\" allowscriptaccess=\\"always\\" allowfullscreen=\\"true\\" name=\\"viddler\\" height=\\"342\\" width=\\"437\\"></embed>\\n</object>"},{"id":"/2008/07/29/a-month-in-exo-dot-dot-dot-busy-busy-busy-dot-dot-dot","metadata":{"permalink":"/blog/2008/07/29/a-month-in-exo-dot-dot-dot-busy-busy-busy-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-07-29-a-month-in-exo-dot-dot-dot-busy-busy-busy-dot-dot-dot.md","source":"@site/blog/2008-07-29-a-month-in-exo-dot-dot-dot-busy-busy-busy-dot-dot-dot.md","title":"A month in eXo... busy busy busy...","description":"I have been within eXo Platform for almost 2 months now.. As expected, it is crazy, and exciting. This is why I did not have lot of time to blog... Ideas are here, just time is not... So here a very quick post on my current work...","date":"2008-07-29T00:00:00.000Z","formattedDate":"July 29, 2008","tags":[],"readingTime":1.225,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"A month in eXo... busy busy busy...","categories":"exo portal"},"prevItem":{"title":"New cool tools that  I am using: DabbleBoard &amp; Balsamiq","permalink":"/blog/2008/08/07/new-cool-tools-that-i-am-using-dabbleboard-and-balsamiq"},"nextItem":{"title":"Joining eXo Platform","permalink":"/blog/2008/06/17/joining-exo-platform"}},"content":"I have been within eXo Platform for almost 2 months now.. As expected, it is crazy, and exciting. This is why I did not have lot of time to blog... Ideas are here, just time is not... So here a very quick post on my current work...\\n\\nWe have been releasing new releases of products (Portal/Portlet Container, Collaboration Suite), deliver a training on eXo Portal,  writing docs, articles, and many other exciting things... One of them is this nice example of integration of [GWT Google Gadget in eXo WebOS](http://blog.exoplatform.org/2008/07/29/create-gwt-portlet-to-deploy-in-exo/).\\n\\nDuring this period, when I was not working with customers, prospects and partners, I have learned a lot about one of the most exciting and powerful features: eXo ECM coupled with eXo Portal/WebOS. eXo exposed using a powerful UI, and set of API, its JavaContent Repository on which you can build powerful sites/portal using eXo templating engine... and one other cool thing, at least for me, is the fact that most of the programming that you do, including rich Ajax based application is done using Groovy. (using GroovyServer Pages)... More to come about eXo features on this blog, but the best place to be is to follow our company/project blog at [http://blog.exoplatform.org](http://blog.exoplatform.org), where all the team is presenting new features, and events of the eXo community...\\n\\nAlso, I have created,  with Sebastien Roul a JUG in Nantes, here some information for French people:\\n\\n* [Nantes JUG\'s Web Site](http://www.nantesjug.org/)\\n* [Press Release](http://www.nantes-developpement.com/1217255362373/0/fiche___actualite/&amp;RH=ACCUEIL) in the \\"Nantes Development\\" site."},{"id":"/2008/06/17/joining-exo-platform","metadata":{"permalink":"/blog/2008/06/17/joining-exo-platform","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-06-17-joining-exo-platform.md","source":"@site/blog/2008-06-17-joining-exo-platform.md","title":"Joining eXo Platform","description":"I am very excited to say that this week is my first week as an employee","date":"2008-06-17T00:00:00.000Z","formattedDate":"June 17, 2008","tags":[],"readingTime":4.03,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Joining eXo Platform","categories":"portal exo"},"prevItem":{"title":"A month in eXo... busy busy busy...","permalink":"/blog/2008/07/29/a-month-in-exo-dot-dot-dot-busy-busy-busy-dot-dot-dot"},"nextItem":{"title":"The good side of expensive oil price...","permalink":"/blog/2008/06/02/the-good-side-of-expensive-oil-price-dot-dot-dot"}},"content":"I am very excited to say that this week is my first week as an employee\\nof eXo Platorm. Excited for many reasons.... I\'ll pass on the\\nexcitement of a new job, we are all excited about that... So what are\\nthe others reasons:\\n\\n* working for open source\\n* be part of a visionary team/product.\\n\\n#### Working for open source\\n\\nSome of you know that I have been working for Oracle from 1999 to 2007,\\nand lately for Sogeti (Cap Gemini Group) that is one of the biggest\\nIBM\'s partner... So moving to eXo Platform an open source company is\\ndefinitively a big shift for me. Anyway, I was looking for more \u201cagile\u201d\\nworking environment....\\n\\nI am pleased to work for a product where users can follow on a daily\\nbasis what is the exact status of the different features, not from\\nmarketing slides but directly from the source trunk... But also where\\nany user can influence the product either by providing direct feedback\\nto the developers using mailing lists, tracking tools and wikis; even\\nmore if they want to participate to the development itself.\\n\\nSo being open source, will avoid some kind of lock-in, but this is not\\nthe only point: standards is another important point. eXo Platform,\\nsince its first release in 2003, has been great about defining,\\nsupporting and implementing standards. So eXo Portal is a great\\nimplementation of a JSR-168 portlet container, and it is today also one\\nof the first portal and portlet container supporting the new JSR-286\\nJava Portlet API. In addition to the Java Portlet API support, it is\\nalso possible to consume remote portlet using WSRP 1&amp;2. As you\\ncan see eXo Portal is a great solution to implement and integrate an\\nenterprise portal based on standards. In addition to portal standard,\\neXo is also providing a powerful ECM that is based on a Java Content\\nRepostory (JCR/JSR-170) implementation.\\n\\nBased on the fact that eXo is open source and support industry\\nstandard\\nit is a great tool to use; and it has been chosen by some other\\nprojects for example:\\n\\n* NovaForge: an innovative Forge platform that aggregates and\\nglue\\nseveral Open Source tools used for Software Developments such as SVN\\nrepositories, Bug trackers, Continuous build\u2026 It used eXo WebOS and ECM\\nto provide a single access points to all the resources as you can see\\nin the following screenshot.\\n\\n![](http://farm4.static.flickr.com/3087/2586150929_85d1f80644_m.jpg Novaforge and eXo WeboS )\\n\\n\\n* Bonita: is an open source workflow and BPM that is using\\neXo WebOS to\\nexpose its console, as you can see in the following screenshot\\n\\nYou also have other softwares that are using various part of the product from te JCR to the Portlet Container... it depends a lot of your needs...\\n\\n#### Working with visionary team and product\\n\\neXo Platform is a lot more than en enterprise portal. The core architecture of eXo is based on an internal SOA built at the top of an IoC container (Pico Container). This architecture has been leveraged in many point to expose new services and assemble all the components of the eXo Platform suite.\\n\\nLet\'s take a quick look to the components of eXo Platform:\\n\\n* eXo Portal 2.0: create and publish your portal\\nusing a simple\\nand powerful Ajax based user interface\\n* eXo ECM 2.0: manage all your digital assets that are stored\\nin the JCR,\\nand expose them using many protocols (WebDac, FTP, CIFS, ...)* eXo CS 1.0: collaborate inside and outside your enterprise\\nusing shared\\ncalendars, WebMail, Forums and Contact Management\\n* eXo WebOS 1.0: virtualize your desktop in this revolutionary user\\ninterface, where you can expose portlets and widgets and run any Web\\napplications.\\n\\n![](http://farm4.static.flickr.com/3262/2586521963_56fece571a_m.jpg )\\n\\nIn addition to these component that are available today in the eXo Ultimate distribution or as stand alone solution, you can see in the source trunk:\\n\\n* LiveRoom: a Web based real time collaborative tools allowing shared whiteboard, visioconference/VoIP using once again your browser, (in fact we have been implementing  it using Flash/Flex)\\n*  WCM: Web CMS System that is extending the ECM to facilitate Web site publishing.\\n* and many other features...\\n\\nOne last example of the vision of the eXo Platform is the fact that all services could be exposed either as REST Services using the JSR-311(JAX-RS) or as SOAP Services using the JSR-181 (Annotations based services).\\n\\nOne of the feature that I am currently watching closely is the implementation and support of Open Social standard, that allows easy integration of \\"social-applications\\" with other very hot subjects for\\n    the \u201cEnterprise 2.0\u201d such as social networks, mashups and so on...\\n\\nStay tuned, and take the time to download and install eXo Platform...\\n\\nAs a closure, I just invite you to take a look to eXo and contact me if you have any question about the product or if you have."},{"id":"/2008/06/02/the-good-side-of-expensive-oil-price-dot-dot-dot","metadata":{"permalink":"/blog/2008/06/02/the-good-side-of-expensive-oil-price-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-06-02-the-good-side-of-expensive-oil-price-dot-dot-dot.md","source":"@site/blog/2008-06-02-the-good-side-of-expensive-oil-price-dot-dot-dot.md","title":"The good side of expensive oil price...","description":"We are all badly impacted with the price of oil. Yeah it really expensive,","date":"2008-06-02T00:00:00.000Z","formattedDate":"June 2, 2008","tags":[],"readingTime":3.24,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"The good side of expensive oil price...","categories":null},"prevItem":{"title":"Joining eXo Platform","permalink":"/blog/2008/06/17/joining-exo-platform"},"nextItem":{"title":"Seesmic in action on TechCrunch\'s \'great\' Twitter post....","permalink":"/blog/2008/05/26/seesmic-in-action-on-techcrunchs-great-twitter-post-dot-dot-dot"}},"content":"We are all badly impacted with the price of oil. Yeah it really expensive,\\nnow people is the US have a gas that is as expensive as France\'s\\nprices, shocking! ;).\\n\\nAnyway, I do not want to talk about cars and gas, but really on the\\ngood effect of the expensive oil. I do believe that expensive oil has\\ngood impacts, not for my bank account but for the \\"planet\\", and may be\\nindirectly for software industry.\\n\\n#### Change in transportation\\n\\nFirst of all, we are all forced to try to save gaz/enegy to save money. So people are thinking more about alternative to \\"individual car driving\\": carpooling, public\\ntransportations are more often used these days. I\\naddition, people are using public bikes that you can find\\nmore and more in cities now. Nantes, where I live has launched its new\\nprogram \\"[Bicloo](http://www.bicloo.nantesmetropole.fr/)\\". So not only people are changing\\nbut I am sure industrials will also adapt to this offering greener\\nalternative to oil for transportation. Personally, I would like to buy\\na cheap\\nelectric car to be able to go in the city, at least to reach the\\nvarious Park&Ride points. I have to say that I really like the[\\nGoogle RecharcheIT](http://www.google.org/recharge/) initiative, that is used to charge plug-in\\nhybrid cars... This is enough for the \\"transportation\\" and cars....\\n\\n#### Change in way we do work\\n\\nI believe also that expensive oil will have an impact on the way we do\\nwork: we must try to avoid traveling all the time. Some work could be\\ndone just using the Internet and good collaborative tools. When most of\\na work is about: reading mail, phone talks, and meetings all these\\nactivities could be done using \\"IT\\" isn\'t?\\n\\nI used to work from home a lot when I was working for Oracle HQ from\\nFrance: the working kit is quite simple and not that expensive: laptop,\\nfast Internet connection, VoIP phone, VPN and a Collaborative\\nSuite (webmail, shared calendar, web conference tools, social networks, ...). Using this simple set of tools I was as productive as any employee but without\\n traveling that much. I do not say that we should all work from home or smal local office, but we should try to do it. And limit our endless travels, that burns so much energy for CO2.... Yes all the IT is also consuming energy, but I do believe that it is probably easy to make IT industry \\"greener\\" than car industry... (I might be wrong...), but still at the office we will use computers/IT anyway..\\n\\nSo many of the software vendor are offering tools to make us more productive using the \\"network as the computer\\" -we all remember this-.\\nOne example of tools that in fact is \\"greener\\" is [eXo Platform](http://www.exoplatform.org). eXo provides all the tools for collaborative work: personalized portal, webmail, chat, shared calendar, ... and soon\\n  LiveRoom that is the Web conference/VoIP solution integrated to your   navigator (Flex based application). In addition to this, users can also\\n  virtualized their desktop using eXo WebOS, and access his work   environment from everywhere... I know that other solutions are available on the market, from various vendors such as Oracle, IBM,   Microsoft, I have chosen eXo first of all it is an open source project where any user can participate at least by asking from requirements   proposing ideas, but also and mainly because its offering is based on industry standards that will help integration to any existing IT   systems. (... and also because I am currently working closely with the eXo team...)\\n\\n#### IT for a greener world\\n\\nIn conlusion, we, IT folks can I have some impact on the environment by helping people to be more effective when doing remote work, avoiding useless commute... Clearly, the technologies are avaiable today to help us, to work efficiently from home or \\"virtual offices\\" the main constraints are coming from enterprise/management culture."},{"id":"/2008/05/26/seesmic-in-action-on-techcrunchs-great-twitter-post-dot-dot-dot","metadata":{"permalink":"/blog/2008/05/26/seesmic-in-action-on-techcrunchs-great-twitter-post-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-05-26-seesmic-in-action-on-techcrunchs-great-twitter-post-dot-dot-dot.md","source":"@site/blog/2008-05-26-seesmic-in-action-on-techcrunchs-great-twitter-post-dot-dot-dot.md","title":"Seesmic in action on TechCrunch\'s \'great\' Twitter post....","description":"Like many of you I am following TechCrunch, and one of the latest post was quite \\"amazing\\":","date":"2008-05-26T00:00:00.000Z","formattedDate":"May 26, 2008","tags":[],"readingTime":1.535,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Seesmic in action on TechCrunch\'s \'great\' Twitter post....","categories":null},"prevItem":{"title":"The good side of expensive oil price...","permalink":"/blog/2008/06/02/the-good-side-of-expensive-oil-price-dot-dot-dot"},"nextItem":{"title":"What is Twitter? It is a simple tool to turn off lights","permalink":"/blog/2008/05/21/what-is-twitter-it-is-a-simple-tool-to-turn-off-lights"}},"content":"![]( http://www.techcrunch.com/wp-content/themes/techcrunch/techcrunch.gif )\\nLike many of you I am following [TechCrunch](http://www.techcrunch.com/), and one of the latest post was quite \\"amazing\\":\\n\\n* see the [Twitter!](http://www.techcrunch.com/2008/05/25/twitter-2/) post from Michael Arrigton.More than the post itself, the comments that this single word generated is quite funny: so far we have 291 comments... As you can guess I have not read (all of) them... But something is interesting, you can see that many people have used [Seesmic ](http://www.seesmic.com/)to drop a video comment.\\n[![](http://api.seesmic.com/images/seesmic_logo.png)](http://www.seesmic.com/)\\n\\nI have to say that I am quite impressed by the large number of comments that have  been made using Seesmic.  Bravo to Loic Lemeur\'s vision for this tools... This is simply the new and easy way to really do the \\"read/write\\" Web, I should say \\"record/watch\\" Web.\\n\\nOne thing is still bothering me, how to I find interesting content that is saved using Seesmic?\\n\\nYes, I can use social features to subscribe to people that are sharing similar center of interest.  But I cannot find any folksonomy on Seesmic, am I missing it, or it is just not available yet?\\n\\nOr let\'s dream and imagine than Web search/index engine will be able to index these video content to help me to find interesting content -may be this exists but I have not found it yet-.  Loic, is it your next big feature to transform Seesmic in the next killer app?\\n\\nSo far I do not have the feeling that I missed anything -watch the comments and you will understand-, but it looks like people love to use the video to discuss/share with others, and I can understand why... We have moved from technical tools: Complex Web Authoring Platform, to easy Blog publishing, to pure video recording tool, so very easy indeed. I can imagine my mother posting a comment to a blog or video now... thanks to Seesmic!"},{"id":"/2008/05/21/what-is-twitter-it-is-a-simple-tool-to-turn-off-lights","metadata":{"permalink":"/blog/2008/05/21/what-is-twitter-it-is-a-simple-tool-to-turn-off-lights","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-05-21-what-is-twitter-it-is-a-simple-tool-to-turn-off-lights.md","source":"@site/blog/2008-05-21-what-is-twitter-it-is-a-simple-tool-to-turn-off-lights.md","title":"What is Twitter? It is a simple tool to turn off lights","description":"Lately, I have been presenting Web 2.0 tools to my coworkers, and explaining how it could be used to improve the way we do \\"things\\" (especially business). Here a list of some of the tools I am talking about blogs, wikis, second life, social networking/bookmarking/tagging/rating,, facebook, friendfeed, youtube, seesmic, feeddo, flock and many others depending of the questions and the mood of the moment...","date":"2008-05-21T00:00:00.000Z","formattedDate":"May 21, 2008","tags":[],"readingTime":0.53,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What is Twitter? It is a simple tool to turn off lights","categories":null},"prevItem":{"title":"Seesmic in action on TechCrunch\'s \'great\' Twitter post....","permalink":"/blog/2008/05/26/seesmic-in-action-on-techcrunchs-great-twitter-post-dot-dot-dot"},"nextItem":{"title":"JavaOne 2008: Groovy and Grails presentations and code","permalink":"/blog/2008/05/16/javaone-2008-groovy-and-grails-presentations-and-code"}},"content":"Lately, I have been presenting Web 2.0 tools to my coworkers, and explaining how it could be used to improve the way we do \\"things\\" (especially business). Here a list of some of the tools I am talking about blogs, wikis, second life, social networking/bookmarking/tagging/rating,, facebook, friendfeed, youtube, seesmic, feeddo, flock and many others depending of the questions and the mood of the moment...\\n\\nOne of the tool that I love to present is Twitter... and now I have an good example of why we all need to be on Twitter ;)\\n\\n[Control Lights with Twitter](http://www.vimeo.com/1025711?pg=embed&amp;sec=1025711) from [Justin Wickett](http://www.vimeo.com/user284499?pg=embed&amp;sec=1025711) on [Vimeo](http://vimeo.com?pg=embed&amp;sec=1025711).\\n\\nThanks to Justin from Twitter..."},{"id":"/2008/05/16/javaone-2008-groovy-and-grails-presentations-and-code","metadata":{"permalink":"/blog/2008/05/16/javaone-2008-groovy-and-grails-presentations-and-code","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-05-16-javaone-2008-groovy-and-grails-presentations-and-code.md","source":"@site/blog/2008-05-16-javaone-2008-groovy-and-grails-presentations-and-code.md","title":"JavaOne 2008: Groovy and Grails presentations and code","description":"Andres and Guillaume have posted on the Groovy Users list the pointers to many (if not all) the Groovy and Grails sessions of JavaOne 2008...\xa0You want to learn more, this is a great opportunity to do it so:","date":"2008-05-16T00:00:00.000Z","formattedDate":"May 16, 2008","tags":[],"readingTime":0.465,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaOne 2008: Groovy and Grails presentations and code","categories":"groovy conference"},"prevItem":{"title":"What is Twitter? It is a simple tool to turn off lights","permalink":"/blog/2008/05/21/what-is-twitter-it-is-a-simple-tool-to-turn-off-lights"},"nextItem":{"title":"Sun/Liferay: why? and what is the next step?","permalink":"/blog/2008/05/09/sun-slash-liferay-why-and-what-is-the-next-step"}},"content":"![](http://media.xircles.codehaus.org/_projects/groovy/_logos/medium.png )Andres and Guillaume have posted on the Groovy Users list the pointers to many (if not all) the Groovy and Grails sessions of JavaOne 2008...\xa0You want to learn more, this is a great opportunity to do it so:\\n\\n* [JavaOne 2008: Groovy and Grails presentations and code](http://permalink.gmane.org/gmane.comp.lang.groovy.user/32891)\\n\\nThanks to all of the authors... I would love to be there watching these session live... This post is the opportunity also to point you to this video from InfoQ of [Jason Rudolph doing a very nice introduction to Grails](http://www.infoq.com/presentations/rudolph-grails-intro) during last year QCon conference..."},{"id":"/2008/05/09/sun-slash-liferay-why-and-what-is-the-next-step","metadata":{"permalink":"/blog/2008/05/09/sun-slash-liferay-why-and-what-is-the-next-step","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-05-09-sun-slash-liferay-why-and-what-is-the-next-step.md","source":"@site/blog/2008-05-09-sun-slash-liferay-why-and-what-is-the-next-step.md","title":"Sun/Liferay: why? and what is the next step?","description":"Yesterday, I have been surprised when I saw the following announcement:","date":"2008-05-09T00:00:00.000Z","formattedDate":"May 9, 2008","tags":[],"readingTime":1.135,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Sun/Liferay: why? and what is the next step?","categories":"portal"},"prevItem":{"title":"JavaOne 2008: Groovy and Grails presentations and code","permalink":"/blog/2008/05/16/javaone-2008-groovy-and-grails-presentations-and-code"},"nextItem":{"title":"JavaOne 2008: my sessions choice...","permalink":"/blog/2008/05/01/javaone-2008-my-sessions-choice-dot-dot-dot"}},"content":"Yesterday, I have been surprised when I saw the following announcement:\\n\\n* [Sun Microsystems Joins Liferay Open Source Community / Sun\'s new Web platform to leverage core elements of Liferay Portal 5.0.](http://www.liferay.com/web/guest/about_us/news/sun)One interesting thing is  Brian Chan\'s blog entry about [Liferay and Sun](http://www.liferay.com/web/bchan/blog/-/blogs/liferay_and_sun?_33_redirect=%2Fweb%2Fguest%2Fcommunity%2Fblog) explaining how they have been working together so far... to fill the limitations of each other solutions.\\n\\nSo today what does that means? Liferay is leveraging the development power of Sun to implements standards (for example JSR-286). I have always been frustrated by the lack of standard support and \'real\' innovation in Liferay (compare to its competitors such as [eXo Platform](http://www.exoplatform.org/), and Jboss for example). In the other hand Sun will leverage the \\"tiny Liferay product\\" killing its own solution. Sun\'s portal is really to big without that benefits for developers/users (compare to its competitors, BEA,IBM, Oracle for example).\\n\\nSo what\'s the next step for this partnership? If Sun wants to push a real portal offering, it can only finish by a full acquisition of Liferay... even if it is stated that it is not the plan.\\n\\nLet\'s wait and see how this \\"WebSynergy\\" goes... However one thing is cool, it will put more visibility on Enteprise Portals. With all the Web 2.0 stuff: social computing, mashups, collaborative works/intelligence, the need for \\"Enterprise Portal\\" (I should add a 2.0) is back stronger than before..."},{"id":"/2008/05/01/javaone-2008-my-sessions-choice-dot-dot-dot","metadata":{"permalink":"/blog/2008/05/01/javaone-2008-my-sessions-choice-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-05-01-javaone-2008-my-sessions-choice-dot-dot-dot.md","source":"@site/blog/2008-05-01-javaone-2008-my-sessions-choice-dot-dot-dot.md","title":"JavaOne 2008: my sessions choice...","description":"I have attended or presented at JavaOne for the last 6 years when I was living in the SF Bay Area... But this year I won\'t be in San Francisco for JavaOne. As you can guess, I am sad about that... However, I still look at the schedule and events, and here what I would like to do:","date":"2008-05-01T00:00:00.000Z","formattedDate":"May 1, 2008","tags":[],"readingTime":3.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaOne 2008: my sessions choice...","categories":"groovy java conference"},"prevItem":{"title":"Sun/Liferay: why? and what is the next step?","permalink":"/blog/2008/05/09/sun-slash-liferay-why-and-what-is-the-next-step"},"nextItem":{"title":"Launch of Exo Enterprise WebOS","permalink":"/blog/2008/04/21/launch-of-exo-enterprise-webos"}},"content":"I have attended or presented at JavaOne for the last 6 years when I was living in the SF Bay Area... But this year I won\'t be in San Francisco for JavaOne. As you can guess, I am sad about that... However, I still look at the schedule and events, and here what I would like to do:\\n\\n*   [Groovy/Grails meetup](http://www.g2one.com/meetup/) organized by G2One and NFJS\\n*   Sun and Oracle General Sessions: Tuesday, Wednesday and Friday at 8:30am. I am particularly interested to see the Oracle\'s one \xa0talk about the Oracle/BEA deal... May be we will be able to learn more about the products roadmap\\n*   TS-6050 - Comparing JRuby and Groovy\\n*   TS-5274 - Groovy on a Cloud: Testing Java EE Platform Applications on Amazon EC2\\n*   BOF-5102 - Cooking Your Own Groovy Builder: A Step Forward into Domain-Specific Languages\\n*   TS-5793 - Groovy and Grails: Changing the Landscape of Java EE Platform Patterns\\n*   BOF-5101 - Boosting Your Testing Productivity with Groovy\\n*   TS-5764 - Grails in Depth\\n*   TS-6298 - Designing Graphical Model-Driven Applications: Lego MindStorm ... long time that I have not programmed/designed with my Legos...\\n*   BOF-4888 - Taming the Leopard: Extending OS X the Java Technology Way: would be great to see my ex-coworker talking about OS X and Java.. John and Tim are terrific developers\\n*   BOF-6400 - The Future of Guice. even if I have not used (yet) this API from Google I have been a big fan of Bob\'s work\\n*   TS-5657 - JavaFX Technology: Bring the Web with You--Multiple Interfaces to Games, Chat, and More\\n*   TS-4817 - The Java Platform Portlet Specification 2.0 (JSR 286)\\n*   TS-5343 - Enterprise JavaBeans (EJB) 3.1 Technology. As I am pushing more and more customer to use the standard JPA.. would be great to learn more about the next release of EJB\\n*   TS-6169 - Spring Framework 2.5: New and Notable... would like to see what will be said about SpringAppServer\\n*   TS-6072 - Advanced Enterprise Debugging Techniques\\n*   BOF-5634 -  Java EE Platform Connector Architecture 1.6 Overview. I have been using J2CA a lot lately when dealing with SOA in large IT department... So quite cool to have an update on this spec.\\n*   TS-5318 - Dealing with Asynchronicity in Java Technology-Based Web Services. A feature in the WS Stack that I have been pushing a lot...\\n*   TS-5616 - JSR 303: From a World of Constraints to Constrain the World\\n*   TS-6339 - Top 10 Patterns for Scaling Out Java Technology-Based Applications\\n*   TS-5706 - SCA and Java Platform, Enterprise Edition (Java EE Platform): Integration Inside\\n*   BOF-5495 - Untangling the Asynchronous Web\\n*   TS-5425 - JAX-RS: The Java API for RESTful Web Services\\n*   LAB-4500LT - Develop AJAX Based Portlets With OpenPortal and GWT\\n*   TS-6574 - How to Implement Your Own OpenSocial Container on the Java Platform\\n*   TS-6807 - What\u2019s New in Ajax\\n*   BOF-5661 - Comet: The Rise of Highly Interactive Web Sites\\n*   BOF-4922 - Writing Real-Time Web Applications, Using Google Web Toolkit and Comet\\n*   TS-5870 - The Best of Both Worlds with Java\u2122 Business Integration and Service Component Architecture\\n*   TS-5152 - Overview of the JavaFX Script Programming Language\\n*   TS-5572 - Groovy, the Red Pill: Metaprogramming--How to Blow the Mind of Developers on the Java Platform\\n*   TS-5815 - Going Mobile with JavaFX Script Technology, Groovy, and Google Android\\n*   TS-5535 - Tying Java Technologies Together the RESTful Way\\n\\nI have probably selected many conflicting sessions, not really an issue since I am not going there. That said, this year again JavaOne looks quite exciting and a lot of content again around Scripting Languages and Framework; Web2.0 related technologies and SOA.\\n\\nI hope that I will be there for the 2009 one ;)"},{"id":"/2008/04/21/launch-of-exo-enterprise-webos","metadata":{"permalink":"/blog/2008/04/21/launch-of-exo-enterprise-webos","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-04-21-launch-of-exo-enterprise-webos.md","source":"@site/blog/2008-04-21-launch-of-exo-enterprise-webos.md","title":"Launch of Exo Enterprise WebOS","description":"Last Friday (April 18th) I was attending the launch of \\"eXo WebOS\\" in Paris. Benjamin Mestrallet, creator of eXo has started the event with a presentation of the solution and its impact on the IT.","date":"2008-04-21T00:00:00.000Z","formattedDate":"April 21, 2008","tags":[],"readingTime":2.435,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Launch of Exo Enterprise WebOS","categories":"portal exo"},"prevItem":{"title":"JavaOne 2008: my sessions choice...","permalink":"/blog/2008/05/01/javaone-2008-my-sessions-choice-dot-dot-dot"},"nextItem":{"title":"Exposing a Database as a Web Service... with OracleAS and DB","permalink":"/blog/2008/03/25/exposing-a-database-as-a-web-service-dot-dot-dot-with-oracleas-and-db"}},"content":"Last Friday (April 18th) I was attending the launch of \\"eXo WebOS\\" in Paris. Benjamin Mestrallet, creator of eXo has started the event with a presentation of the solution and its impact on the IT.\\n\\nLike any internet user, each time I see eXo Web OS and applications I am very very impressed. They have pushed very far the use of AJAX based applications to offer a complete virtualization of the OS/Desktop, not in a Virtual Machine, but simply in your favorite browser.\\n\\n![](http://1.bp.blogspot.com/_aoQgQ1obiyE/SAxdf2YzmSI/AAAAAAAAADk/B9iAEMKI8yc/s320/webos001.png )\\n\\nAs you  can see in this screenshot, eXo is launching in your browser, a complete desktop, where I run many applications provided by eXo: Calendar, Forum, Mail, but a also a calculator gadget attached to the desktop itself. The easiest is to try the product, go to [eXoplatform site and download it](http://www.exoplatform.com/portal/public/en/quicktry).\\n\\n\x3c!-- truncate --\x3e\\n\\nAfter the presentation of the eXo new features, and vision, two customers have presented their project based on eXo Portal and Collaboration Suite:\\n\\n*   [M6](http://www.m6.fr/). M6 is one of the major French TV channel. M6 is using eXo for their new intranet. Some of the key point that I kept from this experience:\\n*   Flexibility/Agility of the platform: M6 IT has been able, with [Business&amp;Decision](http://www.interakting.com/), to answer end user needs in term of design/look&amp;feel. Quite important in media industry ;)\\n*   Extensibility: in addition to the pure publishing of personalized pages, eXo has showed lot of power. Creation of new services based on the core architecture of eXo (IoC based container), or simply by creation new Groovy script to capture events in the Java Content Repository. One example of such script, is the creation of a new script to automatically resize images when they are published in the repository.*   [Belgium Minister of Finances](http://minfin.fgov.be/), with [Bull Belgium](http://www.bull.com/) In addition to the classical portal usage of eXo. They have chosen to put in place for their 30 000 users, the brand new eXo Collaboration Suite (Mail, Calendar, Contacts) and even more exciting use the real time collaboration tools provided by eXo LiveRoom. This extension to eXo provides using a Flex based architecture exciting tools for collaborative work:\\n*   Video and Voice over IP for Web conferences\\n*   Shared Whiteboard allowing multiple people to see and modify documents and graphicsFor me these tools in addition to the eXo chat (Ajax based) are really exciting, and this for many reasons: The event was closed by a very passionate Round Table, moderated by Christian Faure (Atos Origin) with Adobe (Michael Chaize), eXo (Benjamin Mestrallet), Google (Dave Armstrong), Microsoft ( Christophe Lauer), Mozzila (Tristan Nitot) and Sun (Eric Mahe) have discussed many topics around Web/Internet, Standards, Open Source,  and Web OS... If you are french speaker I am inviting you to take a look to the recording of this event available on the eXo blog:\\n\\n*   [eXo WebOS launch Party: the Round Table Video with Adobe, eXo, Google, Mozilla, Microsoft and Sun Microsystems!](http://blog.exoplatform.org/2008/04/19/exo-webos-launch-party-the-round-table-video-with-adobe-exo-google-mozilla-microsoft-and-sun-microsystems/)"},{"id":"/2008/03/25/exposing-a-database-as-a-web-service-dot-dot-dot-with-oracleas-and-db","metadata":{"permalink":"/blog/2008/03/25/exposing-a-database-as-a-web-service-dot-dot-dot-with-oracleas-and-db","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-03-25-exposing-a-database-as-a-web-service-dot-dot-dot-with-oracleas-and-db.md","source":"@site/blog/2008-03-25-exposing-a-database-as-a-web-service-dot-dot-dot-with-oracleas-and-db.md","title":"Exposing a Database as a Web Service... with OracleAS and DB","description":"I am just cross posting this entry to react to this very interesting article:","date":"2008-03-25T00:00:00.000Z","formattedDate":"March 25, 2008","tags":[],"readingTime":1.75,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Exposing a Database as a Web Service... with OracleAS and DB","categories":"ws oracle"},"prevItem":{"title":"Launch of Exo Enterprise WebOS","permalink":"/blog/2008/04/21/launch-of-exo-enterprise-webos"},"nextItem":{"title":"My (Work) Life on a Key: XWiki and other portables applications","permalink":"/blog/2008/03/07/my-work-life-on-a-key-xwiki-and-other-portables-applications"}},"content":"I am just cross posting this entry to react to this very interesting article:\\n\\n* [Exposing a Database as a Web Service](http://www.developer.com/db/article.php/3735771) a developer.com article...\\n\\n####  Oracle Application Server Web Services\\n\\nIf you are an Oracle Application Server user, you may know that it is possible to expose database resources as Web Services using the OracleAS Web Services stack. You can for example using JAX-RPC based Web Service create service on a PL/SQL stored procedure, a SQL statement and even poste message on a queue (AQ). This is available in the Web Service Assembler (wsa) tool and also JDeveloper.\\n\\nAt the end when you have executed the wizard, you have a complete JavaEE application ready to be deloyed. All the JDBC calls and PL type mapping are done automatically by the wizard... very neat Take a look to the [Assembling Database Web Services](http://download.oracle.com/docs/cd/B32110_01/web.1013/b28974/devdbase.htm#BDCCBHFG) documentation.\\n\\nHere the archtecture schema of OracleAS Database Web Services:\\n\\n![]( http://1.bp.blogspot.com/_aoQgQ1obiyE/R-ni1lmTTxI/AAAAAAAAADc/Os4eMkzOGXM/s320/aswsv009.gif )\\n\\nIt is important to mention that such service can leverage the WS-* support of OracleAS and any JAX-RPC handler you want to add to the service.\\n\\n####  BPEL PM and Database Resources\\n\\nIn addition to a pure Java developer approach it is also possible to expose database resource as Web Service using Oracle BPEL PM, yeah... it could be overloaded, but still it is possible and very easy to do. See the chapter [BPEL: Communicating with a Database](http://download.oracle.com/docs/cd/B31017_01/core.1013/b28764/bpel006.htm#CIHCIDGF)\\n\\n#### Oracle RDBMS 11 NDWS\\n\\nOrale RDBMS introduced a new feature that is called: Native Oracle XML DB Web Services, that allows developer to directly expose Web Services from the DB. Take a look to the chapter [Using Native Oracle XML DB Web Services](http://download.oracle.com/docs/cd/B28359_01/appdev.111/b28369/xdb_web_services.htm#ADXDB3900).\\n\\nNote that in this case you do not have any WS-* support without another technical solution that could be Oracle Web Service Manager or any other solutions (such as a SOA appliance like for example IBM dataPower)\\n\\nhmm I have not used that much this feature since I have left Oracle... I wonder when Oracle will provide a OS X release that will allow me to use my computer without any VM..."},{"id":"/2008/03/07/my-work-life-on-a-key-xwiki-and-other-portables-applications","metadata":{"permalink":"/blog/2008/03/07/my-work-life-on-a-key-xwiki-and-other-portables-applications","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-03-07-my-work-life-on-a-key-xwiki-and-other-portables-applications.md","source":"@site/blog/2008-03-07-my-work-life-on-a-key-xwiki-and-other-portables-applications.md","title":"My (Work) Life on a Key: XWiki and other portables applications","description":"Now that I am back in consulting business, I often have customers where I am not allowed to connect my Mac on the network. Annoying!!!! But this is not a big issue since now it is easy to bring you environment on a USB key.","date":"2008-03-07T00:00:00.000Z","formattedDate":"March 7, 2008","tags":[],"readingTime":1.31,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"My (Work) Life on a Key: XWiki and other portables applications","categories":null},"prevItem":{"title":"Exposing a Database as a Web Service... with OracleAS and DB","permalink":"/blog/2008/03/25/exposing-a-database-as-a-web-service-dot-dot-dot-with-oracleas-and-db"},"nextItem":{"title":"Oracle Acquisitions... cannot remember all of them?","permalink":"/blog/2008/02/11/oracle-acquisitions-dot-dot-dot-cannot-remember-all-of-them"}},"content":"Now that I am back in consulting business, I often have customers where I am not allowed to connect my Mac on the network. Annoying!!!! But this is not a big issue since now it is easy to bring you environment on a USB key.\\n\\n#### XWi*key*: my wiki on a key\\n\\nOn a daily business, I have installed my personal [XWiki](http://www.xwiki.com/) on my 2Gb USB Key, and a JDK (for windows, on OSX I am using the default one). So with this solutions I have my personal CMS, Website and applications with me, and I can use it from any computer available. I work on any site, any meeting room directly on my Wiki even when I am not connected. I am using a packaged Entprise XWiki that comes with Jetty and HSQL, so it is a complete and self contained environment. I just changed the start and stop scripts to point to the JDK that is on the key. Nothing exceptional here, but it is very useful.\\n\\n#### Others portable applications\\n\\nI know that a Wiki is not enough most of the time to \\" bring your life with you\\", and you may want more, such as Open Office, Mail and Web clients, ... Some of the packages you can use:\\n\\n*   [PortableApps](http://portableapps.com/) take a look to [the list of applications](http://portableapps.com/apps) that are available in a portable fashion\\n*   [PenDriveApps](http://pendriveapps.com/)\\n*   [Framakey](http://www.framakey.org/En/Index)\\n\\nThe next test will be to run the OS from the key, I have not done it yet, have you? Some options for this:\\n\\n*   [PenDriveLinux](http://www.pendrivelinux.com/)\\n*   [FlashLinux](http://flashlinux.org.uk/)"},{"id":"/2008/02/11/oracle-acquisitions-dot-dot-dot-cannot-remember-all-of-them","metadata":{"permalink":"/blog/2008/02/11/oracle-acquisitions-dot-dot-dot-cannot-remember-all-of-them","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-02-11-oracle-acquisitions-dot-dot-dot-cannot-remember-all-of-them.md","source":"@site/blog/2008-02-11-oracle-acquisitions-dot-dot-dot-cannot-remember-all-of-them.md","title":"Oracle Acquisitions... cannot remember all of them?","description":"A friend of mine asked me some questions about Oracle and acquisitions. He could not, remember all of them and when they occurred...","date":"2008-02-11T00:00:00.000Z","formattedDate":"February 11, 2008","tags":[],"readingTime":0.46,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Acquisitions... cannot remember all of them?","categories":"oracle"},"prevItem":{"title":"My (Work) Life on a Key: XWiki and other portables applications","permalink":"/blog/2008/03/07/my-work-life-on-a-key-xwiki-and-other-portables-applications"},"nextItem":{"title":"Looking for a Wiki... take a look to WikiMatrix","permalink":"/blog/2008/02/08/looking-for-a-wiki-dot-dot-dot-take-a-look-to-wikimatrix"}},"content":"A friend of mine asked me some questions about Oracle and acquisitions. He could not, remember all of them and when they occurred...\\n\\nIf you have the same questions take a look to the Oracle Acquisition page on Oracle.com since 2005:\\n\\n* [Oracle Strategic Acquisition](http://www.oracle.com/corporate/acquisition.html)Some others are probably missing since in 2003/2004, Collaxa was acquired by Oracle. If you do not remember Collaxa was the first release of the BPEL Process Manager.\\n\\nYou can also take a look to a part of the site that I like that is the [Oracle\'s history.](http://www.oracle.com/corporate/story.html)"},{"id":"/2008/02/08/looking-for-a-wiki-dot-dot-dot-take-a-look-to-wikimatrix","metadata":{"permalink":"/blog/2008/02/08/looking-for-a-wiki-dot-dot-dot-take-a-look-to-wikimatrix","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-02-08-looking-for-a-wiki-dot-dot-dot-take-a-look-to-wikimatrix.md","source":"@site/blog/2008-02-08-looking-for-a-wiki-dot-dot-dot-take-a-look-to-wikimatrix.md","title":"Looking for a Wiki... take a look to WikiMatrix","description":"You are looking for a Wiki... take a look to this very nice online tools:","date":"2008-02-08T00:00:00.000Z","formattedDate":"February 8, 2008","tags":[],"readingTime":0.3,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Looking for a Wiki... take a look to WikiMatrix","categories":null},"prevItem":{"title":"Oracle Acquisitions... cannot remember all of them?","permalink":"/blog/2008/02/11/oracle-acquisitions-dot-dot-dot-cannot-remember-all-of-them"},"nextItem":{"title":"Oracle JAX-RPC: How to change the Character Encoding?","permalink":"/blog/2008/02/02/oracle-jax-rpc-how-to-change-the-character-encoding"}},"content":"You are looking for a Wiki... take a look to this very nice online tools:\\n\\n* [WikiMatrix](http://www.wikimatrix.org/)This tool can help you to find the best Wiki for you needs, free or not, hosted or not, ... and many other criteria.\\n\\nIn addition to the Wiki Matrix, this site offers the same features for:\\n\\n* [Blogs](http://www.weblogmatrix.org/)\\n* [Forums](http://www.forummatrix.org/)\\n* [Podcatchers](http://www.podcatchermatrix.org/)\\n\\nEnjoy!"},{"id":"/2008/02/02/oracle-jax-rpc-how-to-change-the-character-encoding","metadata":{"permalink":"/blog/2008/02/02/oracle-jax-rpc-how-to-change-the-character-encoding","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-02-02-oracle-jax-rpc-how-to-change-the-character-encoding.md","source":"@site/blog/2008-02-02-oracle-jax-rpc-how-to-change-the-character-encoding.md","title":"Oracle JAX-RPC: How to change the Character Encoding?","description":"By default Oracle Web Service client is sending the SOAP messages using an UTF-8 encoding. This is the recommendation of WS-I Basic Profile. To be exact it says UTF-8 or UTF-16.","date":"2008-02-02T00:00:00.000Z","formattedDate":"February 2, 2008","tags":[],"readingTime":0.63,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle JAX-RPC: How to change the Character Encoding?","categories":"ws java"},"prevItem":{"title":"Looking for a Wiki... take a look to WikiMatrix","permalink":"/blog/2008/02/08/looking-for-a-wiki-dot-dot-dot-take-a-look-to-wikimatrix"},"nextItem":{"title":"My first Sogeti Kickoff: a great experience","permalink":"/blog/2008/01/28/my-first-sogeti-kickoff-a-great-experience"}},"content":"By default Oracle Web Service client is sending the SOAP messages using an UTF-8 encoding. This is the recommendation of WS-I Basic Profile. To be exact it says UTF-8 or UTF-16.\\n\\nIt is quite simple to change this encoding...\\n\\nFirst you have to know that the JAX-RPC container will return the same character encoding than the one that is received. To change the character encoding, you just need to set the `ClientConstants.CHARACTER_SET_ENCODING` to the value you want to use. Here some simple client code:\\n\\n``` java\\nStub stub = (Stub)myPort.getPort();\\nstub._setProperty(oracle.webservices.ClientConstants.CHARACTER_SET_ENCODING, \\"UTF-16\\");\\n```\\n\\nMost of the Web Services stacks are offering the same kind of utility to do that, for example here the property you must set to do the same in IBM\'s JAX-RPC implementation: `com.ibm.wsspi.webservices.Constants.MESSAGE_CHARACTER_SET_ENCODING`."},{"id":"/2008/01/28/my-first-sogeti-kickoff-a-great-experience","metadata":{"permalink":"/blog/2008/01/28/my-first-sogeti-kickoff-a-great-experience","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-01-28-my-first-sogeti-kickoff-a-great-experience.md","source":"@site/blog/2008-01-28-my-first-sogeti-kickoff-a-great-experience.md","title":"My first Sogeti Kickoff: a great experience","description":"Last week, more than 400 people of Sogeti group were meeting at\xa0Les Fontaines, for the annual KickOff. It was for me the first big event since I have joined this company last October.","date":"2008-01-28T00:00:00.000Z","formattedDate":"January 28, 2008","tags":[],"readingTime":2.25,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"My first Sogeti Kickoff: a great experience","categories":"conference"},"prevItem":{"title":"Oracle JAX-RPC: How to change the Character Encoding?","permalink":"/blog/2008/02/02/oracle-jax-rpc-how-to-change-the-character-encoding"},"nextItem":{"title":"IBM ProjectZero: A new development model for commercial software?","permalink":"/blog/2008/01/23/ibm-projectzero-a-new-development-model-for-commercial-software"}},"content":"Last week, more than 400 people of Sogeti group were meeting at\xa0[Les Fontaines](http://www.les-fontaines.com/), for the annual KickOff. It was for me the first big event since I have joined this company last October.\\n\\n![Les Fontaines](http://4.bp.blogspot.com/_aoQgQ1obiyE/R568S_vsHII/AAAAAAAAADM/Ma3Y9Y5IHso/s200/back-terrace.jpg)Les Fontaines, is the training/conference center of the Cap Gemini Group located 30mn from Paris. I have to say that it is really a great place. Click on the [Les Fontaines](http://www.les-fontaines.com/) to see\xa0\xa0what I mean...\\n\\nFirst of all it was a great experience to meet my colleagues from all over the world\xa0, Sogeti is a company of\xa0\xa016,000+ people, all over Europe, in the USA and India. \xa0The main theme of this 2008 Kickoff was \\"Web 2.0\\", the reason why I was invited indeed. If most of the technical people are using\xa0internally\xa0and externally Web 2.0 tools such as wikis, blogs, ims, .. It was important to have during this kickoff an \\"overall\\" brainstorming about the use of this tools for the group, but also how we can leverage of knowledge to help customers to be more efficient in their business.\\n\\nAs part of this theme our marketing folks have invited external speakers:\\n\\n* [Rolf Jensen](http://dreamcompany.dk/index.php?id=105) : talking about the Dream Society, explaining that the next step for business is to add an\xa0emotional\xa0aspect into it, to add value... \xa0I am a emotional person, and I can tell that I am buying Rolf\'s idea, and like most of Apple fan you can tell how important are the emotions to do business...\\n* [Rod Bekstrom](http://www.beckstrom.com/), co-author of the well known book \\"[The Starfish and The Spider](http://www.starfishandspider.com/)\\" focusing on the fact an company\xa0must shift \xa0from a centralized organization to a decentralized one. You can listen to the presentation online on Rod\'s site.. I have to say that I soon as I have finished my current readings (RestFul Web Services-O\'Reillys &amp; Get Things Done) I will order this book. I really found some inspiration in Rod presentation, and his ideas could be immediately put in place in your organization or at your customer site.\\n* Last but not least, [Nick Donofrio](http://www-03.ibm.com/press/us/en/biography/10057.wss), EVP Technology and Innovation, talked about the important of innovation for the enterprise, and how we do not control the changes but we should adapt to it in a productive manner to stay at the top.\\n\\nHowever, I would have loved to see [Francois Nonnemacher](http://padawan.info/) on the stage too since I had the opportunity to see him talking about Web 2.0/Social Computing for the enterprise and he is terrific... and also because as far as I know he has worked at Cap Gemini few years back\\n\\nI really enjoyed all these presentations, in addition to the one that I have cited, we had many Sogeti\'s speakers including Sogeti CTO Michiel Boreel."},{"id":"/2008/01/23/ibm-projectzero-a-new-development-model-for-commercial-software","metadata":{"permalink":"/blog/2008/01/23/ibm-projectzero-a-new-development-model-for-commercial-software","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-01-23-ibm-projectzero-a-new-development-model-for-commercial-software.md","source":"@site/blog/2008-01-23-ibm-projectzero-a-new-development-model-for-commercial-software.md","title":"IBM ProjectZero: A new development model for commercial software?","description":"You have probably heard about IBM ProjectZero, this \\"incubator\\"project from IBM pushing a new way of building, assemble and run Web applications. I won\'t talk about the technology in this post, it will come later, but just comment about the way this project is developped: a Community-Driven Commercial Development process.","date":"2008-01-23T00:00:00.000Z","formattedDate":"January 23, 2008","tags":[],"readingTime":1.835,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"IBM ProjectZero: A new development model for commercial software?","categories":null},"prevItem":{"title":"My first Sogeti Kickoff: a great experience","permalink":"/blog/2008/01/28/my-first-sogeti-kickoff-a-great-experience"},"nextItem":{"title":"New Poll: Which \'Web 2.0\' Tools are you using for business?","permalink":"/blog/2008/01/22/new-poll-which-web-2-dot-0-tools-are-you-using-for-business"}},"content":"You have probably heard about [IBM ProjectZero](http://www.projectzero.org/), this \\"incubator\\"project from IBM pushing a new way of building, assemble and run Web applications. I won\'t talk about the technology in this post, it will come later, but just comment about the way this project is developped: a *Community-Driven Commercial Development* process.\\n\\nThe idea behind this \\"process\\" is to apply to a commercial software, ideas that are common \xa0in free software. As you see I am not talking about \\"open source\\" since Project Zero is open source, it is really about the way a \\"commercial\\" software is built...\\n\\nIf you take a look to the \\"[About](http://www.projectzero.org/wiki/bin/view/Main/About)\\" page of ProjectZero you can learn more about this process/ideas. Some key points:\\n\\n* the development team, mainly IBM folks, want\xa0 feedback, insight, suggestions, and criticism from the community. So we as the community can really be involved in the product in a stage where it is usually inside IBM labs without any visibility for non IBM employee... This is quite exciting to see that we can really discuss with the development team as they do the product, and we can see the product evolving based on \\"real\\" customer requirements/comments.\\n* so as \\"future\\" user of the production release I can discuss the features. As a developer of the \\"current\\" development release I can participate to the future product. So what you may be tempted to say, especially if you are a lot involved in open source and free projects, but you have to admit that it is quite a switch for commercial product.Project Zero is not the only project that is using this approach [Jazz](https://jazz.net/) from the rational team is also on this model.\xa0\\n\\nIn the same time if we consider, GlassFish, we do have more or less an equivalent, that is the JavaEE RI and the Sun Application Server... but here on Jazz or Project zero it is really happening on a brand new product that is not a reference implementation but new products part of a commercial R&amp;D lab...\\n\\nI am still a little confused to see what will be the license when release 1.0 goes out... Just wait and see.. hmm sorry I should say \\"just participate and see\\"."},{"id":"/2008/01/22/new-poll-which-web-2-dot-0-tools-are-you-using-for-business","metadata":{"permalink":"/blog/2008/01/22/new-poll-which-web-2-dot-0-tools-are-you-using-for-business","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-01-22-new-poll-which-web-2-dot-0-tools-are-you-using-for-business.md","source":"@site/blog/2008-01-22-new-poll-which-web-2-dot-0-tools-are-you-using-for-business.md","title":"New Poll: Which \'Web 2.0\' Tools are you using for business?","description":"I have just created a poll -see leftbar- asking to list the different tools you are using for business. In fact I would like to have some feedback of you use of Wiki, Blogs, Instant Messaging on you daily job.","date":"2008-01-22T00:00:00.000Z","formattedDate":"January 22, 2008","tags":[],"readingTime":0.925,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New Poll: Which \'Web 2.0\' Tools are you using for business?","categories":null},"prevItem":{"title":"IBM ProjectZero: A new development model for commercial software?","permalink":"/blog/2008/01/23/ibm-projectzero-a-new-development-model-for-commercial-software"},"nextItem":{"title":"Infoq news: \'Request: Sun Drop Support for JRuby","permalink":"/blog/2008/01/18/infoq-news-request-sun-drop-support-for-jruby"}},"content":"![](http://2.bp.blogspot.com/_aoQgQ1obiyE/R5Y9QvPDP0I/AAAAAAAAADE/loju_rH1Ie8/s200/Web+20.png) I have just created a poll -see leftbar- asking to list the different tools you are using for business. In fact I would like to have some feedback of you use of Wiki, Blogs, Instant Messaging on you daily job.\\n\\nMyself I am using all of this for work:\\n\\n*   Internal and external communication with Wikis\\n*   My blog that you are currently reading\\n*   Chat, I use probably more the chat than phone these days (thanks to http://meebo.com when I cannot connect with a rich client)\\n*   Social Networking: I am not necessarily  talking about big sites like LinkedIn/Facebook but more internal sites. For example, back at Oracle we had access to an internal Social Network site, that is now exposed at http://mix.oracle.com, and now at Sogeti, in the Cap Gemini group we do also have internal social networks.The idea behind this post/poll is to be able to discuss, with customers around Web 2.0 tools adoption in the enterprise. So do not hesitate to post a comment describing the tools and how you are using them for business.\\n\\nThanks for your vote ;)"},{"id":"/2008/01/18/infoq-news-request-sun-drop-support-for-jruby","metadata":{"permalink":"/blog/2008/01/18/infoq-news-request-sun-drop-support-for-jruby","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-01-18-infoq-news-request-sun-drop-support-for-jruby.md","source":"@site/blog/2008-01-18-infoq-news-request-sun-drop-support-for-jruby.md","title":"Infoq news: \'Request: Sun Drop Support for JRuby","description":"With the latest big news around Oracle-BEA and SUN-MySQL deals I have missed an interesting article on Infoq with the following title:","date":"2008-01-18T00:00:00.000Z","formattedDate":"January 18, 2008","tags":[],"readingTime":2.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Infoq news: \'Request: Sun Drop Support for JRuby","categories":null},"prevItem":{"title":"New Poll: Which \'Web 2.0\' Tools are you using for business?","permalink":"/blog/2008/01/22/new-poll-which-web-2-dot-0-tools-are-you-using-for-business"},"nextItem":{"title":"Which sites are you using for \'business\'? Another Web2.0 list...","permalink":"/blog/2008/01/04/which-sites-are-you-using-for-business-another-web2-dot-0-list-dot-dot-dot"}},"content":"With the latest big news around Oracle-BEA and SUN-MySQL deals I have missed an interesting article on Infoq with the following title:\\n\\n* [Request: Sun, Drop Support for JRuby](http://www.infoq.com/news/2008/01/sun_drop_jruby)\\n\\nI have to say that I do agree with Craig Wickesser asking Sun to Drop Support for JRuby.\\n\\n### Syntax Matters?\\n\\n*Yes syntax matters*, not only for the \\"beauty\\" of it, but also because of the investment that enterprises have made into it. We should not force people to completely remodel their brain all the time, for no gain.\xa0\\n\\n*I love [Groovy](http://groovy.codehaus.org/) language*, and one of the main reason is because it gives me the most bang for my buck. Java people can immediately catchup with the syntax, and step by step leverage powerful features available by dynamic languages and domain specific languages. I was hoping to see a great adoption by SUN... It is one thing to support scripting with the [JSR-223](http://jcp.org/en/jsr/detail?id=223), but SUN has to \\"endorse\\" a scripting, and from what could be seen today it is not Groovy nor Javascript. When we see all the marketing noise it is Ruby with [JRuby](http://jruby.codehaus.org/)... And I do not think that is necessary good for the Java platform.\\n\\n\\nDo not get me wrong, I think that is a great idea, and need for Java to be able to execute many languages, for example we see a lot of IBM WAS and BEA WL administrator using Python to administer their application server instance with Jython. But once again the \\"default\\" one should be close to Java and integrate with it as close as possible to reduce the impact on scalability and performances, and I do think that Groovy did a great job on these topics.\\n\\n### What about RubyOnRails?\\n\\nI am not a [RoR](http://rubyonrails.com/) expert, far far away from it, but I have learned it, and developed small applications with it, and I have to say that I love this framework.\xa0\xa0And I am sure that like many Java developers that used RoR, I was thinking: \xa0\\"If only I had the same productivity in my favorite platform: J2EE...\\".\\n\\nI was not expecting to run RoR application as-it-is, but more hoping that JavaEE will learn from RoR to simplify development...\xa0\xa0And... somebody did it, with [Grails](http://grails.codehaus.org/). [Grails](http://grails.codehaus.org/) takes inspiration from RoR, but in a \\"real\\" JavaEE environment, since it leverages key pieces of the current Java applications such as Hibernate and Spring, using the power of Groovy to glue all this together.\\n\\n### In conclusion...\\n\\nI do not know for you but yes I do think that SUN should drop support for [JRuby](http://jruby.codehaus.org/), and in place push a language more natural for existing Java Developers, I vote for [Groovy](http://groovy.codehaus.org/). In addition to the language itself, I also expect the JavaEE EG to provide a more productive way of developing \\"simple\\" Web applications. This is where I see [Grails](http://grails.org/) coming in the picture, but many other framework could do the job, taking advantage of some interesting concepts of RoR...\\n\\nAs Rick says, I (we?) am not looking for a Revolution but for an Evolution."},{"id":"/2008/01/04/which-sites-are-you-using-for-business-another-web2-dot-0-list-dot-dot-dot","metadata":{"permalink":"/blog/2008/01/04/which-sites-are-you-using-for-business-another-web2-dot-0-list-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2008-01-04-which-sites-are-you-using-for-business-another-web2-dot-0-list-dot-dot-dot.md","source":"@site/blog/2008-01-04-which-sites-are-you-using-for-business-another-web2-dot-0-list-dot-dot-dot.md","title":"Which sites are you using for \'business\'? Another Web2.0 list...","description":"Disclaimer: you do probably already know all these sites if you are like me spending lot of time on the Internet... (too much time?) But still, some people could be interested, like the customer I have been working with lately...","date":"2008-01-04T00:00:00.000Z","formattedDate":"January 4, 2008","tags":[],"readingTime":1.37,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Which sites are you using for \'business\'? Another Web2.0 list...","categories":null},"prevItem":{"title":"Infoq news: \'Request: Sun Drop Support for JRuby","permalink":"/blog/2008/01/18/infoq-news-request-sun-drop-support-for-jruby"},"nextItem":{"title":"Web Services and Files Exchange","permalink":"/blog/2007/12/20/web-services-and-files-exchange"}},"content":"Disclaimer: you do probably already know all these sites if you are like me spending lot of time on the Internet... (too much time?) But still, some people could be interested, like the customer I have been working with lately...\\n\\nDuring a presentation to a customer around Web 2.0 and Social Networking, which sites do I use myself and why they could be interested for customer and their business.... So I really quickly write up this list, and just want to share it here...\\n\\n\\nSocial networking... and many more. More and more important in business.\\n\\n* [http://facebook.com](http://facebook.com/)\\n* [http://linkedin.com](http://linkedin.com/)\\n\\nMultimedia site... I am sure you all use it daily. I have used this to share pics about events (internal or external).\\n\\n* [http://flickr.com](http://flickr.com/)\\n* [http://dailymotion.com](http://dailymotion.com/)\\n* [http://youtube.com](http://youtube.com/)\\n\\nSocial Bookmarking...\\n\\n* [http://del.icio.us](http://del.icio.us/)\\n\\nSocial links, content is rated by user.\\n\\n* [http://www.digg.com/](http://www.digg.com/)\\n* [http://www.technorati.com/](http://www.technorati.com/)\\n\\n\\nProject Management tools,\\n\\n* [http://www.basecamphq.com/](http://www.basecamphq.com/)\\n*  you need to look to all the [37signals](http://37signals.com/) applications. Very useful\\n\\nGoogle services are terrific, as user or developer.\\n\\n[http://docs.google.com](http://docs.google.com/)\\n[http://google.com/apps](http://google.com/apps)\\n[http://code.google.com](http://code.google.com/)\\n[http://maps.google.com](http://maps.google.com/)\\n\\n\\nPublic and free wiki that I used to communicate with customers, coworker and friends.\\n\\n* [http://wetpaint.com](http://wetpaint.com/)\\n* [http://wikidot.com](http://wikidot.com/)\\n\\n\\nNews and content syndication.\\n\\n* [http://www.netvibes.com/](http://www.netvibes.com/)\\n* [http://google.com/reader](http://google.com/reader)\\n\\nBlogs...\\n\\n[http://wordpress.org/](http://wordpress.org/)\\n[http://blogspot.com](http://blogspot.com/)\\n[http://typepad.com](http://typepad.com/)\\n\\n\\nI use these site as consumer, but these services are more and more important in customer business, and will be part of our job as developer/integrator.\\n\\n* [http://amazon.com](http://amazon.com/)\\n* [http://paypal.com](http://paypal.com/)\\n* [http://ebay.com](http://ebay.com/)\\n* [http://www.salesforce.com/](http://www.salesforce.com/)\\n\\n\\nHmm, when you see this list you can tell that I am a very happy Google user, and I have not talk about the other services that I use from Google: Gmail, GTalk, Analytics, Notebooks, and obviously the search..."},{"id":"/2007/12/20/web-services-and-files-exchange","metadata":{"permalink":"/blog/2007/12/20/web-services-and-files-exchange","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-12-20-web-services-and-files-exchange.md","source":"@site/blog/2007-12-20-web-services-and-files-exchange.md","title":"Web Services and Files Exchange","description":"SOAP based Web Services are now very common in the enterprise architecture, and quite often, applications that consume or publish services would need to send binary content such as images, PDF or Word documents (or anything you have in mind...). The SOAP and XML provide different way to achieve this. So what are the challenges around binary data exchange using SOAP based Web Services:","date":"2007-12-20T00:00:00.000Z","formattedDate":"December 20, 2007","tags":[],"readingTime":8.07,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Web Services and Files Exchange","categories":"ws soq"},"prevItem":{"title":"Which sites are you using for \'business\'? Another Web2.0 list...","permalink":"/blog/2008/01/04/which-sites-are-you-using-for-business-another-web2-dot-0-list-dot-dot-dot"},"nextItem":{"title":"Working on a large XML or SOA project: think about \'separation of concerns\'","permalink":"/blog/2007/12/10/working-on-a-large-xml-or-soa-project-think-about-separation-of-concerns"}},"content":"SOAP based Web Services are now very common in the enterprise architecture, and quite often, applications that consume or publish services would need to send binary content such as images, PDF or Word documents (or anything you have in mind...). The SOAP and XML provide different way to achieve this. So what are the challenges around binary data exchange using SOAP based Web Services:\\n\\n* The main goal of Web Services is **interoperability**; so when you are offering a service, you need to be careful about the technical choice you are making. SOAP has been one great success in term of interoperability. I am aware that REST is also a very good fit for that but since I talk about SOAP and later WS-* standards, I do not want to talk about REST more in this post, the only thing that you can put in your mind is before choosing to implement SOAP based Web Services, ask the following question to yourself: \\"do I really need SOAP services or REST would be enough?\\"... That said let\'s continue on SOAP and binary content exchange. When talking about binary content, the interoperability comes with some trade off around for example performance/message sized or impact on developer. This will be discussed later, but always keep in mind that interoperability is the key point of Web Services. If this is not the case on your project, that means you probably do not need to use SOAP that has an important overhead in general.\\n* **Performance** and **Scalability** is also quite important when you are building a service based application. Especially that often you cannot predict exactly how much a service will be used. We have to keep in mind that often services are build to be reusable, it is one of the basic best practices of development, so if the service is really \\"reused\\" it is important to keep it running with acceptable performances. This is why when talking about binary content, with SOAP it is important to talk about the impact of it on the size of the message and the processing cost.\\n* When using SOAP **Composability** is also quite important. In the context of binary content exchange with XML/SOAP it is important to support composability of the WS-* standard, and this in a performant manner. An example would be that a services that is  sing WS-Security to sign part of the messages should be able to sign the PDF document using the same standard.\\n* **Impact on development**: it is interesting also when choosing the way binary content should be exchanged with SOAP, to see how much impact it has on the development itself. Does a developer must import specific API to be sure that the binary content is properly sent/consumed by the server or client. Note: I will talk about Java here, and particularly about JAX-WS/JAX-RPC since it is the stacks that I know the much, but the remarks would be the same on all technologies.Let\'s now dive into the different options that are offered to a developer/architect to exchange document using SOAP:\\n\\n* XML Base64 encoding\\n* SOAP With Attachment (SwA) using MIME (Multipurpose Internet Mail Extensions)\\n* ~~SOAP With Attachment using DIME (Direct Internet Message Encapsulation)~~\\n* Message Transmission Optimization Mechanism (MTOM)\\n\\nFirst of all, I will not talk in detail about the 3rd point around SOAP with Attachment with DIME, for a simple reason: this approach has been pushed by Microsoft around 2003/2004 and it is now deprecated in favor of MTOM.\\n\\n### Base64 Encoding\\n\\nBase64 is part of the core XML capabilities, and when using it to exchange binary content in a SOAP message it has some very good advantages:\\n\\n* Since it is part of XML itself, it has a **great interoperability**, I can say that all stacks will be able to consume or send messaged that contains Base64 data.\\n* For the same reason it **does not have any impact on development**, most of the Java stacks will automatically use base64 encoding when byte[] paramters will be present.\\n* Always because of the fact that is it 100% XML based, the **composability** with other XML/WS-* standard **is very good**.\\n* So far everything looks great for this approach, but the trade off is the following:\\nBase64 encoding **is not efficient**, since \\"lot of\\" CPU will be used to encode and decode the binary content. In addition the size of the encoded data would be around 30% bigger than the binary content itself. (It can still be used for small dataset)\\n\\n### SOAP with Attachment (SwA)\\n\\nThe SOAP with Attachment specification is the first effort of the Web Services industry to solve the problem of binary content with SOAP. The idea is to  In addition to the W3C Note, the WS-Interoperability organization, has extend this recommendation to create a basic attachment profile to enforce the interoperability of it, using the SOAP with Attachment Reference (swaRef).\\n\\n* The good part of SwA and is the fact that it has been noted by the W3C but also adopted by the WS-I organization. But in fact the *interoperability is not that great*, mainly because none of the Microsoft Web Services solution support SwA. It is true that most of the Java stacks, starting with the standard JAX-RPC/JAX-WS is supporting SwA and swaRef but it is not enough to call it a good interoperability.\\n* The reason why Microsoft refused to implement it, and why it is only a W3C note (and not a recommendation) it is because  **SOAP with Attachment has poor composability**. The reason why it is hard to use WS-* standard with SwA, it is because it breaks some part of the model by ignoring the SOAP/XML processing and just put the document in the MIME header, and a simple reference to it into the SOAP message.\\n* **SOAP with Attachment is efficient*, because of the previous point. The SOAP stack does not really deal with the content and just stream it into the MIME header.\\n* When it is used with JAX-RPC and JAX-WS, **has an impact on the developer**, that must use specific Java API to build it service and put specific data types in the WSDL. The impact on development is not large, but still developper has to think about providing the good method signature or WSDL entry to enforce the use of SwA/swaRef in its service. Where I do believe most developers would expect this to be transparent.\\n\\n### Message Transmission Optimization Mechanism (MTOM)\\n\\nThe last mechanism is also based on MIME on the wire to exchange the binary content, but the way the message (SOAP+MIME) is build is totally different from the previous SwA approach. MTOM has been based on the \\"experience\\" of the others mechanisms, to be able to support composability without impacting the performance and the development.\\n\\n* **Interoperability is virtually great**. It is great because it has been pushed by major vendors such as IBM, Microsoft, BEA, Oracle and it is a [W3C recommendation](http://www.w3.org/TR/soap12-mtom/), so interoperability should be good. I put a \\"virtually\\", because to be interoperable the various Web Services stack must implement it, and it is not the case yet. Today, most of the latest stacks are supporting MTOM so it should not be an issue if you are starting a project.\\n* **Composability is perfect**, since MTOM does use the SOAP envelop but it provides an automatic and transparent optimization to put the binary content  on the MIME header. During the serialization of the message, the SOAP engine is working with the content with a temporary base64 representation of the content allowing all the WS-* operation needed, for example an XML signature, but without the overhead of dealing with base64 over the wire.\\n* **MTOM appears like the most efficient** way of dealing with large document and SOAP.\\n* Because MTOM is using the same approach than the pure XML base64 process, **it does not have any impact on development**. In fact this the the Web Service stack that choose to use base64 (embedding the document) or MTOM over the wire. And this could be done in conjunction with a WS-Policy. As you can see in the [WS-MTOMPolicy](http://www.w3.org/Submission/WS-MTOMPolicy/) this is not under the control of the developer but more under the control of the administrator and then the applications to choose or not to use MTOM.\\n\\n### But... Which one I should use?\\n\\nBased on the different points described earlier is looks like **MTOM is the way to go**; even if this is true it cannot be summarized to this. First of all MTOM is not supported by all the stacks, so if you cannot control the consumers of your services and cannot impose a modern stack, MTOM may not be the best approach. For me, the second on the list is the Base64 approach, because of high interoperability but it is important to remember that has an impact on performance/processing. I personnally would not push SwA because of its non support in the Microsoft world... As you know the world is not yet 100% Java based ;).\\n\\nLet\'s take a look on which stacks are supporting MTOM today:\\n\\n* JAX-WS reference implementation (and Metro)\\n* IBM Websphere 6.x with SOA Feature Pack\\n* BEA Weblogic 10* OracleAS 10gR3 (10.1.3.1) JAX-RPC and FWM 11 preview (JAX-RPC and JAX-WS)\\n* Axis2\\n* XFire\\n* JBossWSYou can find more information on these comparison matrices : [Apache WS Stack Comparison](http://wiki.apache.org/ws/StackComparison) and [Xfire Comparison Matrix](http://xfire.codehaus.org/Stack+Comparison). (these two are probably very interesting to keep... unfortunately they do not contains any MSFT data. I had one in the past, but cannot find it... if you have such matrix feel free to post it in comment.)"},{"id":"/2007/12/10/working-on-a-large-xml-or-soa-project-think-about-separation-of-concerns","metadata":{"permalink":"/blog/2007/12/10/working-on-a-large-xml-or-soa-project-think-about-separation-of-concerns","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-12-10-working-on-a-large-xml-or-soa-project-think-about-separation-of-concerns.md","source":"@site/blog/2007-12-10-working-on-a-large-xml-or-soa-project-think-about-separation-of-concerns.md","title":"Working on a large XML or SOA project: think about \'separation of concerns\'","description":"With XML and SOA becoming mainstream in the enterprise XML operation such as Schema validations, XSL transformations are now very common. These specific operations are CPU intensive and could become a performance bottleneck when directly applied on the middleware. It could be even worst now when using SOAP based Web Services and their related WS-* standards. For example with WS-Security, XML encryption and signature is now more and more used in SOA based applications.","date":"2007-12-10T00:00:00.000Z","formattedDate":"December 10, 2007","tags":[],"readingTime":3.73,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Working on a large XML or SOA project: think about \'separation of concerns\'","categories":"soa"},"prevItem":{"title":"Web Services and Files Exchange","permalink":"/blog/2007/12/20/web-services-and-files-exchange"},"nextItem":{"title":"Portal Project: Time to Think about your Social Networking Enterprise Strategy","permalink":"/blog/2007/12/03/portal-project-time-to-think-about-your-social-networking-enterprise-strategy"}},"content":"With XML and SOA becoming mainstream in the enterprise XML operation such as Schema validations, XSL transformations are now very common. These specific operations are CPU intensive and could become a performance bottleneck when directly applied on the middleware. It could be even worst now when using SOAP based Web Services and their related WS-* standards. For example with WS-Security, XML encryption and signature is now more and more used in SOA based applications.\\n\\nThis is why many enterprise architects are bow looking for solutions to improve performances of XML centric applications.\\n\\nOne of the think we learn when developing application, and that Aspect Oriented Programming has highlighted is the concept of \u201cseparation of concerns\u201d. It is key to keep that in mind also in global architecture in our case by separating the XML processing from the business logic. Hopefully it is most of the time done directly by the various Web Services framework you are using, you do not code the SOAP request/response, it is hidden by the Web Services framework.\\n\\nHowever, in the current application server, the full XML treatment is made directly in the container, for example the XML Encryption is made in the same container that the place where the pure business logic is executed. So let\u2019s find a solution to extract the most intensive XML processing into another part of the system.\\n\\nVendors have now in their catalog appliances that could do the job. The same way that today we are using SSL accelerators to deal with SSL encryption/decryption, we can put XML appliance to deal with the intensive CPU processing operation: XML validations, transformation, Ws-Security enforcing point,...\\n\\n\x3c!-- truncate --\x3e\\n\\n### Architecture Overview\\n\\nThe overall architecture could be represented using the following schema :\\n\\n![]( http://3.bp.blogspot.com/_aoQgQ1obiyE/R11pWKMLwEI/AAAAAAAAACU/Bvo623h0JCA/s400/soa-appliance.png )\\n\\nThe role of the XML/SOA Appliance varies a lot depending of your project:\\n\\n* *Simple XML firewall* to check the validity of the XML/SOAP messages\\n* *Web Services access control*: lookup enterprise directory to check authentication and authorization. This could be based on the WS-Security standards and its various tokens (username, SAML, ...)\\n* *Content generation and transformation*: the appliance can be used to serve various devices for example WAP cell phone or simple HTML Web Client. the XSL transformation is done in a very efficient way in the appliance directly.\\n* *Services Virtualization* : it is possible to route the different messages to various end point depending of simple rules. (business or IT system rules)\\n\\nAs you can see from an architecture point of view, XML appliances are very interesting to distribute the heavy processing of XML to some specific hardware. I have noticed that sometimes developers/architects hesitate to put another piece of hardware/software in their design, but I do think that in this specific case it is probably a good move.\\n\\nSeparating the concern is quite easy and very clean when dealing with XML processing, but also it will allow the overall architecture to be managed in a better way. This kind of appliance will allow administrators to centralize the management of policies, and transformations. But also a side effect of this is the simple fact that when dealing with Web Services, you can easily add WS-* support to many stacks that do not support \\"them\\".\\n\\n### XML/SOA Appliances Offering\\n\\nI have said earlier that vendors are offering such products, here some of the product that I have met or pushed:\\n\\n*   [IBM DataPower](http://www.ibm.com/software/integration/datapower/)\\n*   [Bee Ware i-Sentry](http://www.bee-ware.net/en/product/i-sentry/)\\n*   [Vordel](http://www.vordel.com/products/)\\n*   [Layer 7 XML Appliances](http://www.layer7tech.com/)\\n\\n###What\'s next?\\n\\nSome of you would probably raise the fact that the application server, especially when dealing with Web Services, must parse the XML/SOAP request even if this has been done by the appliance. Yes it is true, but I am sure that in a next future the vendors of such solution would optimize it by providing for example support for binary XML, or any other solution that will improve even more the performance of the overall IT in complex enterprise architecture. But for this application server must support binary XML first to avoid proprietary approaches.\\n\\nAnother point of view that I have not talk about is the possible support of such appliance around Web 2.0/Ajax optimization. I have not yet dive into this, but I am sure we can do very interesting things too.\\n\\nFinally if you have experiences with any XML/SOA appliance feel free to post a comment about it, it will help the readers to see the interest (or not) around this topic."},{"id":"/2007/12/03/portal-project-time-to-think-about-your-social-networking-enterprise-strategy","metadata":{"permalink":"/blog/2007/12/03/portal-project-time-to-think-about-your-social-networking-enterprise-strategy","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-12-03-portal-project-time-to-think-about-your-social-networking-enterprise-strategy.md","source":"@site/blog/2007-12-03-portal-project-time-to-think-about-your-social-networking-enterprise-strategy.md","title":"Portal Project: Time to Think about your Social Networking Enterprise Strategy","description":"Most of the enterprises these days have already put in place a portal -- with more or less success. These projects have started most of the time, with the goal of providing  personalized information to users and communities. When working in a Portal project you probably","date":"2007-12-03T00:00:00.000Z","formattedDate":"December 3, 2007","tags":[],"readingTime":3.575,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Portal Project: Time to Think about your Social Networking Enterprise Strategy","categories":"portal social web20"},"prevItem":{"title":"Working on a large XML or SOA project: think about \'separation of concerns\'","permalink":"/blog/2007/12/10/working-on-a-large-xml-or-soa-project-think-about-separation-of-concerns"},"nextItem":{"title":"Oracle Open World 2007: presentations available online","permalink":"/blog/2007/11/20/oracle-open-world-2007-presentations-available-online"}},"content":"Most of the enterprises these days have already put in place a portal -- with more or less success. These projects have started most of the time, with the goal of providing  personalized information to users and communities. When working in a Portal project you probably\\ndefine many objectives, that are represented from a technical point of view by the following features:\\n\\n* community and group of users\\n* easy content management, allowing people to communicate and share\\n* data integration of many types of data related to the information needed by each user/community.I do not see anything special there except that is exactly the same goals that what most of the Web2.0/Social Networking applications do have:\\n* User management and creation of the community is something that you do on a daily basis with FaceBook and LinkedIn (and any equivalent sites). The key here is the fact that it is the user that define its own community, not an administrator that does not know \u201cmy\u201d business that put me in a specific bucket.\\n* Content Management: Blogs are very good example of communication of a single person (or team) to the rest of the enterprise, (or the rest of the world). Wikis are tools helping you to share content with other people in a very efficient way. If you are working with OpenSource you see that most of them are using a wiki to communicate with user. This is true for the documentation, but also any type of content that is related to a project.\\n* Data Integration: Web 2.0 is all about RSS feeds and Mashups that are, at least today one of the most efficient way, from a user perspective, to integrate content.This is why I do believe that if today you are thinking about an enterprise portal for your organization, it is probably time to step down a little and think more about:\\n* the \u201centerprise social networking strategy\u201d, that is often also related to the \u201cWeb 2.0 enterprise strategy\u201d.So some ideas to approach this:\\n* create a community with some internet tools, for example start by creating a network in Facebook for your enterprise. Some if you will probably think that it is not productive for the enterprise... Hmm I have to say that it is not directly, but at least it helps people to be familiar with a new way of using the computer and the Internet. Companies I am working in or with do have their network already (Sogeti, CapGemini, IBM, Oracle, ...). An example of this is the way [Serena](http://blog.holtz.com/index.php/weblog/fir_interview_jeremy_burton_ceo_serena_software_on_facebook_fridays_novembe/) is using FaceBook as part of their intranet and as a tools to do better business. (and some people reactions to this: [FaceBook Friday:Bad Idea](http://valleywag.com/tech/bad-ideas/friday-is-facebook-day-318507.php))\\n* if your challenges are around content management start by installing a Wiki in house or using an internet one. I am sure you will be surprised to see the adoption and use in your team. I have many experiences where a regular \\"Portal/CMS\\" failed regarding the \\"community sharing\\" where Wikis have been a great success.\\n* if your challenges are around data integration, I will encourage you to learn more about Rest/RSS and other technologies that are used in mashups. It is true that this one is probably will need more effort from IT to provide the good content feed, but instead of giving the data already packaged in an HTML view (portlet?) do send only the XML using a proper format (RSS/ATOM) and give correct tools to the user, to see how then will be consuming it.\\n\\nI see this approach more business oriented, this is empowering the business user giving them an infrastructure to select their own tools. Portal the Darwin way kind of approach.\\n\\nSo if you have an existing portal, or more important if you are thinking about starting a Portal project, add the \u201cWeb 2.0/Social Networking strategy\u201d question to your plan. And to be honest, asking yourself this question about Web20/Social Network does not cost that much but could probably help yourself to satisfy your end users and customers...\\n\\nNote: It is voluntarily that I am mixing up the Web 2.0 (technologies) and the social networking (behavior), since these two are intimately linked. Web 2.0 being the set of tools and technologies facilitating the social networking."},{"id":"/2007/11/20/oracle-open-world-2007-presentations-available-online","metadata":{"permalink":"/blog/2007/11/20/oracle-open-world-2007-presentations-available-online","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-11-20-oracle-open-world-2007-presentations-available-online.md","source":"@site/blog/2007-11-20-oracle-open-world-2007-presentations-available-online.md","title":"Oracle Open World 2007: presentations available online","description":"For the first time since 2001, I was not participating at OOW... As you may know I have moved back to France and now working for a new company Sogeti. So I am quite happy to se that once again all the presentations are available for download there:","date":"2007-11-20T00:00:00.000Z","formattedDate":"November 20, 2007","tags":[],"readingTime":0.735,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Open World 2007: presentations available online","categories":null},"prevItem":{"title":"Portal Project: Time to Think about your Social Networking Enterprise Strategy","permalink":"/blog/2007/12/03/portal-project-time-to-think-about-your-social-networking-enterprise-strategy"},"nextItem":{"title":"A Groovier ADF with Oracle ADF 11","permalink":"/blog/2007/11/08/a-groovier-adf-with-oracle-adf-11"}},"content":"For the first time since 2001, I was not participating at OOW... As you may know I have moved back to France and now working for a new company [Sogeti](http://www.sogeti.com/). So I am quite happy to se that once again all the presentations are available for download there:\\n\\n* [OOW 2007 Content Catalog.](http://www28.cplan.com/cc176/catalog.jsp)\\n\\nI personnaly looked at OracleAS and Java presentations, and especially the one regarding SOA, performances, security and web Services:\\n\\n* [New Features of Java EE 5.0 and How to Use Them](http://cboracle:oraclec6@www28.cplan.com/cbo_export/PS_S291388_291388_176-1_FIN_v3.pdf)\\n\\n* [Maximizing Your Java Middleware Performance on Multicore Platforms](http://cboracle:oraclec6@www28.cplan.com/cbo_export/PS_S292174_292174_176-1_FIN_v2.pdf)\\n\\nSome other presentations that I would like to read/see:\\n\\n* Java EE/Java SE/Java Authorization Contract for Containers (JACC) Security in a Nutshell\\n* A New Approach to Diagnosing Java Application Performance\\n* Next-Generation Web Services Infrastructure and Interoperability\\n* Performance Management for SOA Applications\\n\\nAnd I will take more time do read some others..."},{"id":"/2007/11/08/a-groovier-adf-with-oracle-adf-11","metadata":{"permalink":"/blog/2007/11/08/a-groovier-adf-with-oracle-adf-11","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-11-08-a-groovier-adf-with-oracle-adf-11.md","source":"@site/blog/2007-11-08-a-groovier-adf-with-oracle-adf-11.md","title":"A Groovier ADF with Oracle ADF 11","description":"Steve Muench has published in Oracle Magazine an article about the use of Groovy in Oracle ADF. In this article you learn how you can do validation and calculation in you business service layer.","date":"2007-11-08T00:00:00.000Z","formattedDate":"November 8, 2007","tags":[],"readingTime":0.225,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"A Groovier ADF with Oracle ADF 11","categories":"groovy oracle jdeveloper"},"prevItem":{"title":"Oracle Open World 2007: presentations available online","permalink":"/blog/2007/11/20/oracle-open-world-2007-presentations-available-online"},"nextItem":{"title":"Guillaume Laforge Interview on JavaLobby","permalink":"/blog/2007/11/03/guillaume-laforge-interview-on-javalobby"}},"content":"![]( http://media.xircles.codehaus.org/_projects/groovy/_logos/medium.png )\\n\\nSteve Muench has published in Oracle Magazine an article about the use of Groovy in Oracle ADF. In this article you learn how you can do validation and calculation in you business service layer.\\n\\n* [Enhanced Calculation and Validation](http://www.oracle.com/technology/oramag/oracle/07-nov/o67frame.html)\\n* [Oracle ADF](http://www.oracle.com/technology/products/adf/index.html)"},{"id":"/2007/11/03/guillaume-laforge-interview-on-javalobby","metadata":{"permalink":"/blog/2007/11/03/guillaume-laforge-interview-on-javalobby","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-11-03-guillaume-laforge-interview-on-javalobby.md","source":"@site/blog/2007-11-03-guillaume-laforge-interview-on-javalobby.md","title":"Guillaume Laforge Interview on JavaLobby","description":"I am sure you have seen it before, but just in case... Geertjan from JavaLobby has interviewed yesterday Guillaume, in case you do not know.. Guillaume is the Project Leader of the Groovy project and co-founder of the G2One company.","date":"2007-11-03T00:00:00.000Z","formattedDate":"November 3, 2007","tags":[],"readingTime":0.445,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Guillaume Laforge Interview on JavaLobby","categories":"groovy"},"prevItem":{"title":"A Groovier ADF with Oracle ADF 11","permalink":"/blog/2007/11/08/a-groovier-adf-with-oracle-adf-11"},"nextItem":{"title":"The good and bad of PLC ethernet... in hotels","permalink":"/blog/2007/10/30/the-good-and-bad-of-plc-ethernet-dot-dot-dot-in-hotels"}},"content":"I am sure you have seen it before, but just in case... Geertjan from JavaLobby has interviewed yesterday Guillaume, in case you do not know.. Guillaume is the Project Leader of the Groovy project and co-founder of the G2One company.\\n\\nInterview: [What\'s so groovy about Groovy?](http://www.javalobby.org/java/forums/t103036.html).\\n\\nIn case you have not look at it, [G2One](http://www.g2one.com/) is offering many services around Groovy and Grails ... directly from the source since Guillaume, and Graeme Rocher are working there... So if you need any help on these nice technologies do not hesitate...."},{"id":"/2007/10/30/the-good-and-bad-of-plc-ethernet-dot-dot-dot-in-hotels","metadata":{"permalink":"/blog/2007/10/30/the-good-and-bad-of-plc-ethernet-dot-dot-dot-in-hotels","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-10-30-the-good-and-bad-of-plc-ethernet-dot-dot-dot-in-hotels.md","source":"@site/blog/2007-10-30-the-good-and-bad-of-plc-ethernet-dot-dot-dot-in-hotels.md","title":"The good and bad of PLC ethernet... in hotels","description":"I am traveling visiting partner and pleased to see that I have internet access in my room. Unfortunately it is not Wifi access but not that bad, but with PLC (Power line communication)...","date":"2007-10-30T00:00:00.000Z","formattedDate":"October 30, 2007","tags":[],"readingTime":0.515,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"The good and bad of PLC ethernet... in hotels","categories":null},"prevItem":{"title":"Guillaume Laforge Interview on JavaLobby","permalink":"/blog/2007/11/03/guillaume-laforge-interview-on-javalobby"},"nextItem":{"title":"Paris \'SOA Forum\' feedback; and  little comments about SOA projects","permalink":"/blog/2007/10/07/paris-soa-forum-feedback-and-little-comments-about-soa-projects"}},"content":"I am traveling visiting partner and pleased to see that I have internet access in my room. Unfortunately it is not Wifi access but not that bad, but with PLC (Power line communication)...\\n\\nSo I take my PLC Adapator at the concierge. Plug it in my room and start to surf the web. Performance is good... Now I am looking for another electric plug for my MacBook AC... oh maaaan! I cannot find any other free plug.. except in the bathroom. This is really annoying and stupid... Sorry for this little post about a small and annoying experience.. alone in my PLC bedroom."},{"id":"/2007/10/07/paris-soa-forum-feedback-and-little-comments-about-soa-projects","metadata":{"permalink":"/blog/2007/10/07/paris-soa-forum-feedback-and-little-comments-about-soa-projects","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-10-07-paris-soa-forum-feedback-and-little-comments-about-soa-projects.md","source":"@site/blog/2007-10-07-paris-soa-forum-feedback-and-little-comments-about-soa-projects.md","title":"Paris \'SOA Forum\' feedback; and  little comments about SOA projects","description":"This week I have attended a SOA conference in Paris the SOA Forum. (I was not there in 2006). This event is not a technical event targeted towards developers but mainly oriented for IT managers and decision makers. This day was well attended, around 200 people. The content and more important the questions and round tables provide a good snapshot of how SOA is adopted.","date":"2007-10-07T00:00:00.000Z","formattedDate":"October 7, 2007","tags":[],"readingTime":3.795,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Paris \'SOA Forum\' feedback; and  little comments about SOA projects","categories":"soa conference"},"prevItem":{"title":"The good and bad of PLC ethernet... in hotels","permalink":"/blog/2007/10/30/the-good-and-bad-of-plc-ethernet-dot-dot-dot-in-hotels"},"nextItem":{"title":"Free Burma: International Bloggers\' Day for Burma on the 4th of October","permalink":"/blog/2007/10/03/free-burma-international-bloggers-day-for-burma-on-the-4th-of-october"}},"content":"This week I have attended a SOA conference in Paris the [SOA Forum](http://forumsoa.fr/). (I was not there in 2006). This event is not a technical event targeted towards developers but mainly oriented for IT managers and decision makers. This day was well attended, around 200 people. The content and more important the questions and round tables provide a good snapshot of how SOA is adopted.\\n\\nIf last year, based on comments that I can get, the message was \\"*What &amp; Why SOA?*\\" this year I have the feeling that most of the audience was really familiarized with the SOA concepts -as I said earlier it is not a technical conference- and now they are more asking \\"*How and When SOA?*\\". Lot of discussions were about how to I start the projects, since in many case the SOA will impact the whole IT, and even more the full enterprise.\\n\\n\x3c!-- truncate --\x3e\\n\\n**Thoughts about SOA project &amp; approach**\\n\\nIt is hard to say if the best way to start with SOA is starting from the Top (C level) or from the IT department on a departmental project. To be honest I think it will depend of each organizations; and depends of \\"why\\" SOA is a good fit for the enterprise.\\n\\nI am tempted to say that if the choice is made for a \\"Time-To-Market\\" reason I believe that the project will start from a specific business need and be implemented in a \\"bottom-up\\" fashion., meaning the IT will quickly put together some services to give some agility to the business. This is something that I have seen many times in the telco industry.\\n\\nAt the other end, when the key factor is about rationalization of the business processes over the whole enterprise, the project is often manage at the IT management level if not even higher. This because this kind of approach will have impact on many departments/people.\\n\\nBasically like in any project it is important to have:\\n\\n*   good communication between actors that could be developers or departments\\n*   share the same goals\\n*   have an understanding of the technologies that will be used and their constraints.If for the point 1 &amp; 2 this is management that deals with this. For the 3rd one, we are closer to the technology where we can probably share the most -saying that because I believe that most of you, reader, are technical people. At the end of the day we do not build system with slideshows, but with products/solutions. So it is part of our job to take time to understand the pros/cons, limitations of the different products that will be involved in the project.\\n\\nOne example, is this week I have visited a customer and this customer wants to put in place an ESB to provide services to the different departments of the enterprise. Discussing at the global level of the architecture we all agreed on the different needs such as: connectivity to heterogeneous systems, transformation, routing, ...\\n\\nThen the customer talk about \\"Web Services\\" again and again, this is where I have to say, I always try to slow down the discussion to set the expectation at the correct level, for example talking about impact on reliability, security of the HTTP/SOAP based Web Services. Don\'t get me wrong I am not saying that it is not possible to achieve correct QoS (Quality of Services) with Web Services but it could have an impact on the product choice, for example supporting WS-RM, WS-Security or even using proprietary approach for stateful Web Services, ... And the same comments, questions could occur with other part of the stack.\\n\\nIn conclusion, independently of the type of approach you are taking to put in place a Service Oriented Architecture, you will need sometimes to really understand well the product/solution you will be using to implement it. And for each of the options you will be choosing take some time to estimate the pros/cons and limitations of it. The same way you are taking time to list the different services, their granularity,  their QoS, you need to take some time to analyze the different solution. For example when you choose to use an ESB, BPEL engine with their connectivity capabilities, what are the best way to connect to a system (SOAP, JMS, JDBC, JCA, Java, ..), how to code the logic (Java, BPEL, business rules, ...) and for each of this question think about the impact of it on your system. For example, how portable will be my code/business process, and is it important for me?"},{"id":"/2007/10/03/free-burma-international-bloggers-day-for-burma-on-the-4th-of-october","metadata":{"permalink":"/blog/2007/10/03/free-burma-international-bloggers-day-for-burma-on-the-4th-of-october","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-10-03-free-burma-international-bloggers-day-for-burma-on-the-4th-of-october.md","source":"@site/blog/2007-10-03-free-burma-international-bloggers-day-for-burma-on-the-4th-of-october.md","title":"Free Burma: International Bloggers\' Day for Burma on the 4th of October","description":"Free Burma!","date":"2007-10-03T00:00:00.000Z","formattedDate":"October 3, 2007","tags":[],"readingTime":0.01,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Free Burma: International Bloggers\' Day for Burma on the 4th of October","categories":null},"prevItem":{"title":"Paris \'SOA Forum\' feedback; and  little comments about SOA projects","permalink":"/blog/2007/10/07/paris-soa-forum-feedback-and-little-comments-about-soa-projects"},"nextItem":{"title":"Derek Sivers\'s blog: 7 reasons I switched back to PHP after 2 years on Rails","permalink":"/blog/2007/09/25/derek-siverss-blog-7-reasons-i-switched-back-to-php-after-2-years-on-rails"}},"content":"[![Free Burma!](http://freeburma.s3.amazonaws.com/free_burma_02.jpg)](http://www.free-burma.org/)"},{"id":"/2007/09/25/derek-siverss-blog-7-reasons-i-switched-back-to-php-after-2-years-on-rails","metadata":{"permalink":"/blog/2007/09/25/derek-siverss-blog-7-reasons-i-switched-back-to-php-after-2-years-on-rails","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-09-25-derek-siverss-blog-7-reasons-i-switched-back-to-php-after-2-years-on-rails.md","source":"@site/blog/2007-09-25-derek-siverss-blog-7-reasons-i-switched-back-to-php-after-2-years-on-rails.md","title":"Derek Sivers\'s blog: 7 reasons I switched back to PHP after 2 years on Rails","description":"The new post on the Ruby section of O\'Reilly authored by Derek Sivers is quite interesting, starting with the title:","date":"2007-09-25T00:00:00.000Z","formattedDate":"September 25, 2007","tags":[],"readingTime":2.26,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Derek Sivers\'s blog: 7 reasons I switched back to PHP after 2 years on Rails","categories":"java grails groovy php rails"},"prevItem":{"title":"Free Burma: International Bloggers\' Day for Burma on the 4th of October","permalink":"/blog/2007/10/03/free-burma-international-bloggers-day-for-burma-on-the-4th-of-october"},"nextItem":{"title":"IDTVG and Co: how to merge meetic and expedia","permalink":"/blog/2007/09/18/idtvg-and-co-how-to-merge-meetic-and-expedia"}},"content":"The new post on the [Ruby section of O\'Reilly](http://www.oreillynet.com/ruby/) authored by Derek Sivers is quite interesting, starting with the title:\\n\\n* [ 7 reasons I switched back to PHP after 2 years on Rails](http://www.oreillynet.com/ruby/blog/2007/09/7_reasons_i_switched_back_to_p_1.html)I am far away of being a PHP expert, or even a Ruby one, but I have the impression that I could post a similar title with Java instead of PHP. If this is true that Java EE could look a little complex for a start -this is probably less true today with the new JavaEE simplifications-. Yes... when you compare Rails and Java alone it is more complex but we should not forget that Java is now more a platform than a simple programming language. And many developers and companies have built very productive solution on this platform.\\n\\nSince I am talking about Ruby on Rails here, it is important to mention again Grails and Groovy that provide on the Java platform a simple and productive way to\\ndevelop applications, and time to get back to the different reasons mentioned by Derek in his post:\\n\\n#### 1 - \u201cIS THERE ANYTHING RAILS/RUBY CAN DO THAT <strike>PHP</strike> JAVA CAN\u2019T DO? \u2026 (thinking)\u2026 NO.\u201d\\n\\nI believe that we will all agree on the fact that you can do anything you want in Java; Web applications, mobile applications, operating systems, rdbms, ... the only limit is your brain! -and your skills ;) -\\n\\n#### 2 - OUR ENTIRE COMPANY\u2019S STUFF WAS IN <strike>PHP</strike> JAVA: DON\u2019T UNDERESTIMATE INTEGRATION\\n\\nI think this is one of the key point here. Enterprise is using JavaEE a lot and it is part of the IT, moving to another technology will be expensive even if development is faster. In addition, the developers, administrators are used to develop and manage Java based applications.\\n\\nAnd I do not want to talk about how complex it could be when you are building a Rails application on an existing database, designed from a pure Entity/Relation methodology....\\n\\n#### 3, 4 5 - I have nothing special to say here...\\n\\n#### 6 - I LOVE SQL\\n\\nI still see a lot of developers using SQL directly in Java programs. The nice thing about Java is the fact that based on your skills and what you like to do you can choose the way you want to access the database, simple SQL, powerful O-R Mappings, ...\\n\\n#### 7 - PROGRAMMING LANGUAGES ARE LIKE GIRLFRIENDS: THE NEW ONE IS BETTER BECAUSE *YOU* ARE BETTER\\n\\nI love this reason, but nothing special to say, I let you read the original post.\\n\\nAs you can see from the number of comments in Derek\'s blog -no times to read all of them- this entry generates lot of reactions."},{"id":"/2007/09/18/idtvg-and-co-how-to-merge-meetic-and-expedia","metadata":{"permalink":"/blog/2007/09/18/idtvg-and-co-how-to-merge-meetic-and-expedia","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-09-18-idtvg-and-co-how-to-merge-meetic-and-expedia.md","source":"@site/blog/2007-09-18-idtvg-and-co-how-to-merge-meetic-and-expedia.md","title":"IDTVG and Co: how to merge meetic and expedia","description":"With all the Web 2.0/Social Networking noise we are all used to \\"virtually socialize\\", but now you can use it to do more and improve the quality of your trips.","date":"2007-09-18T00:00:00.000Z","formattedDate":"September 18, 2007","tags":[],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"IDTVG and Co: how to merge meetic and expedia","categories":null},"prevItem":{"title":"Derek Sivers\'s blog: 7 reasons I switched back to PHP after 2 years on Rails","permalink":"/blog/2007/09/25/derek-siverss-blog-7-reasons-i-switched-back-to-php-after-2-years-on-rails"},"nextItem":{"title":"Prototype Windows: the best way to create \'Web 2.0\' windows","permalink":"/blog/2007/09/15/prototype-windows-the-best-way-to-create-web-2-dot-0-windows"}},"content":"With all the Web 2.0/Social Networking noise we are all used to \\"virtually socialize\\", but now you can use it to do more and improve the quality of your trips.\\n\\nThe site [IDTGV and Co](http://idtgvandco.idtgv.com/) (only in French so far) allows you to enter your profile and hobbies. Then when you do a reservation for the TGV (high speed train) you can match it up with other travelers. I have not tried this service yet since I am mainly traveling in the north of France but I will for sure try it as soon as I can.... Let me know if you have tested it."},{"id":"/2007/09/15/prototype-windows-the-best-way-to-create-web-2-dot-0-windows","metadata":{"permalink":"/blog/2007/09/15/prototype-windows-the-best-way-to-create-web-2-dot-0-windows","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-09-15-prototype-windows-the-best-way-to-create-web-2-dot-0-windows.md","source":"@site/blog/2007-09-15-prototype-windows-the-best-way-to-create-web-2-dot-0-windows.md","title":"Prototype Windows: the best way to create \'Web 2.0\' windows","description":"I am sure that most of you already know the \\"Prototype Windows\\"  project that provides a very powerful way to create windows and dialogs inside your Web pages. If you do not know it, take a look it\'s awesome!","date":"2007-09-15T00:00:00.000Z","formattedDate":"September 15, 2007","tags":[],"readingTime":0.49,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Prototype Windows: the best way to create \'Web 2.0\' windows","categories":"ajax javascript"},"prevItem":{"title":"IDTVG and Co: how to merge meetic and expedia","permalink":"/blog/2007/09/18/idtvg-and-co-how-to-merge-meetic-and-expedia"},"nextItem":{"title":"Java 101: Generate a unique identifier with java.util.UUID","permalink":"/blog/2007/08/07/java-101-generate-a-unique-identifier-with-java-dot-util-dot-uuid"}},"content":"I am sure that most of you already know the \\"[Prototype Windows](http://prototype-window.xilinus.com/)\\"  project that provides a very powerful way to create windows and dialogs inside your Web pages. If you do not know it, take a look it\'s awesome!\\n\\nSebastien Gruhier, aka Mr Proto, and others have done a terrific job allowing developers to integrate exciting features with few lines of code. Find more on the [PWC site](http://prototype-window.xilinus.com/) or on [Mr Proto\'s blog](http://blog.xilinus.com/pwc).\\n\\n![]( http://3.bp.blogspot.com/_aoQgQ1obiyE/Ruvo6E3lYfI/AAAAAAAAAB0/fk6OCadNOCA/s400/pwcdemo.png )\\n\\nIn addition to the Windows API, you can also take a look to the Tranparent Message, Prototype Carousel and Prototype Graphics projects"},{"id":"/2007/08/07/java-101-generate-a-unique-identifier-with-java-dot-util-dot-uuid","metadata":{"permalink":"/blog/2007/08/07/java-101-generate-a-unique-identifier-with-java-dot-util-dot-uuid","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-08-07-java-101-generate-a-unique-identifier-with-java-dot-util-dot-uuid.md","source":"@site/blog/2007-08-07-java-101-generate-a-unique-identifier-with-java-dot-util-dot-uuid.md","title":"Java 101: Generate a unique identifier with java.util.UUID","description":"A friend of mine was asking me how to generate a unique ID for his application... As you probably already know Java SE 5 has introduced the java.util.UUID class to easily generate Universally Unique Identifier (UUID). As usual  Wikipedia is a great starting point to learn more about UUID.","date":"2007-08-07T00:00:00.000Z","formattedDate":"August 7, 2007","tags":[],"readingTime":0.52,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Java 101: Generate a unique identifier with java.util.UUID","categories":"java"},"prevItem":{"title":"Prototype Windows: the best way to create \'Web 2.0\' windows","permalink":"/blog/2007/09/15/prototype-windows-the-best-way-to-create-web-2-dot-0-windows"},"nextItem":{"title":"OracleAS OC4J and Java EE releases....","permalink":"/blog/2007/08/02/oracleas-oc4j-and-java-ee-releases-dot-dot-dot"}},"content":"A friend of mine was asking me how to generate a unique ID for his application... As you probably already know Java SE 5 has introduced the java.util.UUID class to easily generate Universally Unique Identifier (UUID). As usual  [Wikipedia](http://en.wikipedia.org/wiki/UUID) is a great starting point to learn more about UUID.\\n\\nGenerating the unique ID is as simple as calling the method `UUID.randomUUID()` in the class. This will give a new instance of UUID that you can now manipulate; for example do a toString() to get the UUID string representation as describe in the specifications; for example `5462dc18-4653-42d1-b4e4-22fc970a6ce5`\\n\\nResources:\\n\\n*   [UUID on Wikipedia](http://en.wikipedia.org/wiki/UUID)\\n*   [java.util.UUID Documentation](http://java.sun.com/javase/6/docs/api/java/util/UUID.html)"},{"id":"/2007/08/02/oracleas-oc4j-and-java-ee-releases-dot-dot-dot","metadata":{"permalink":"/blog/2007/08/02/oracleas-oc4j-and-java-ee-releases-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-08-02-oracleas-oc4j-and-java-ee-releases-dot-dot-dot.md","source":"@site/blog/2007-08-02-oracleas-oc4j-and-java-ee-releases-dot-dot-dot.md","title":"OracleAS OC4J and Java EE releases....","description":"This morning I was helping a customer to debug some Web Services deployment issues, when I simply realized that the error was coming from the fact that he was trying to deploy a JAX-RPC service on a OracleAS 10gR2 server; where it is not supported/available.","date":"2007-08-02T00:00:00.000Z","formattedDate":"August 2, 2007","tags":[],"readingTime":0.995,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OracleAS OC4J and Java EE releases....","categories":"oc4j oracle"},"prevItem":{"title":"Java 101: Generate a unique identifier with java.util.UUID","permalink":"/blog/2007/08/07/java-101-generate-a-unique-identifier-with-java-dot-util-dot-uuid"},"nextItem":{"title":"Visiting Paris with Velib (new bike transit system)","permalink":"/blog/2007/07/18/visiting-paris-with-velib-new-bike-transit-system"}},"content":"This morning I was helping a customer to debug some Web Services deployment issues, when I simply realized that the error was coming from the fact that he was trying to deploy a JAX-RPC service on a OracleAS 10gR2 server; where it is not supported/available.\\n\\nThis give me the opportunity to clarify the different versions of the Oracle Application Server and their related Java Enterprise Edition release support.\\n\\n<table class=\\"tug\\" cellpadding=\\"2\\" cellspacing=\\"2\\">\\n  <tbody>\\n    <tr>\\n      <td class=\\"tug\\">Oracle AS Release</td>\\n      <td class=\\"tug\\">J2EE/Java EE Release</td>\\n      <td class=\\"tug\\">Comments/Extensions</td>\\n      <td class=\\"tug\\">Certification Matrix</td>\\n    </tr>\\n    <tr>\\n      <td class=\\"tug\\">11.1.0.0</td>\\n      <td class=\\"tug\\">Java EE 5.0</td>\\n      <td class=\\"tug\\">* Currently available as technology preview</td>\\n      <td class=\\"tug\\"></td>\\n    </tr>\\n    <tr>\\n      <td class=\\"tug\\">10.1.3.x (Oracle AS 10gR3)</td>\\n      <td class=\\"tug\\">J2EE 1.4</td>\\n      <td class=\\"tug\\">\\n        * Support of EJB 3/JPA * Web Services Annotations * MTOM support\\n        (10.1.3.1) * Integration of Spring 2.0 (10.1.3.2) * Integrationf of\\n        Oracle Coherence (10.1.3.3)\\n      </td>\\n      <td class=\\"tug\\">\\n        [OTN Certicification\\n        Matrix](http://www.oracle.com/technology/software/products/ias/files/oracle_soa_certification_101310.html)\\n      </td>\\n    </tr>\\n    <tr>\\n      <td class=\\"tug\\">10.1.2.x (Oracle AS 10gR2)</td>\\n      <td class=\\"tug\\">J2EE 1.3</td>\\n      <td class=\\"tug\\"></td>\\n      <td class=\\"tug\\">\\n        [OTN Certicification\\n        Matrix](http://www.oracle.com/technology/software/products/ias/files/as_certification_r2_101202.html)\\n      </td>\\n    </tr>\\n    <tr>\\n      <td class=\\"tug\\">9.0.4.x</td>\\n      <td class=\\"tug\\">J2EE 1.3</td>\\n      <td class=\\"tug\\"></td>\\n      <td class=\\"tug\\">\\n        [OTN Certicification\\n        Matrix](http://www.oracle.com/technology/software/products/ias/files/as-certification-904.html)\\n      </td>\\n    </tr>\\n    <tr>\\n      <td class=\\"tug\\">9.0.3.x</td>\\n      <td class=\\"tug\\">J2EE 1.3</td>\\n      <td class=\\"tug\\"></td>\\n      <td class=\\"tug\\">\\n        [OTN Certicification\\n        Matrix](http://www.oracle.com/technology/software/products/ias/files/as_certification_r2_101202.html)\\n      </td>\\n    </tr>\\n  </tbody>\\n</table>"},{"id":"/2007/07/18/visiting-paris-with-velib-new-bike-transit-system","metadata":{"permalink":"/blog/2007/07/18/visiting-paris-with-velib-new-bike-transit-system","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-07-18-visiting-paris-with-velib-new-bike-transit-system.md","source":"@site/blog/2007-07-18-visiting-paris-with-velib-new-bike-transit-system.md","title":"Visiting Paris with Velib (new bike transit system)","description":"I\xa0was in Paris last week end so I took this as a good opportunity to\xa0visit the city using the new \\"Velib\\" system. Velib is a new self\xa0service rental bike system that the city of Paris and JCDecaux put in\xa0place on July 15th. So you can take and return any of the 10,000+ bikes in any of the 750 locations all around Paris.\xa0I believe that\xa0Paris wants to a total number of 20,000 bikes for \xa01,400\xa0locations.","date":"2007-07-18T00:00:00.000Z","formattedDate":"July 18, 2007","tags":[],"readingTime":1.425,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Visiting Paris with Velib (new bike transit system)","categories":null},"prevItem":{"title":"OracleAS OC4J and Java EE releases....","permalink":"/blog/2007/08/02/oracleas-oc4j-and-java-ee-releases-dot-dot-dot"},"nextItem":{"title":"BPEL in Cluster: In which node my process is working","permalink":"/blog/2007/07/10/bpel-in-cluster-in-which-node-my-process-is-working"}},"content":"![](http://3.bp.blogspot.com/_aoQgQ1obiyE/Rp3kJ-2QaVI/AAAAAAAAABU/BmlcbzgcwCk/s200/photos_velib_visuel_article.gif )\\n\\nI\xa0was in Paris last week end so I took this as a good opportunity to\xa0visit the city using the new \\"Velib\\" system. Velib is a new self\xa0service rental bike system that the city of Paris and JCDecaux put in\xa0place on July 15th. So you can take and return any of the 10,000+ bikes in any of the 750 locations all around Paris.\xa0I believe that\xa0Paris wants to a total number of 20,000 bikes for \xa01,400\xa0locations.\\n\\nThe concept is really simple, you can\xa0take a bike for 1 euro day and it is free for 30mn, supplet of 1 euro\xa0will be charge for additional 30 mn then 2euros for the next one, ...\xa0The idea is to use the bike for short periode of time, return it to a\xa0station and take another one after 5mn. So I have been enjoying the\xa0city all Sunday for only 1 euro, it was really nice to be able to ride under the Eiffel tower, les Invalides, Place Vendome on all these very\xa0nice and romantic Parisian places. Not only is it probably one of the\xa0best way to visit Paris as a touris but also it is good for the\xa0environment...\\n\\nOne the flip side, I need to say that\xa0Parisian drivers are not that nice with bikers, even with the 230 miles\xa0of bike lanes...\\n\\n\\nFrom a technical point of view,\xa0the rental system is really nice, \xa0the bike stations have a\xa0nice and simple to use system allowing you to take a bike, see your\xa0balance and they are all connected together allowing you to take and\xa0return bike anywhere in Paris. I will try to find more information\xa0about this application that is probably quite fun, and hopefully not\\nlike the[\xa0www.velib.paris.fr\xa0](http:/www.velib.paris.fr)site, the stations are available in multiple languages."},{"id":"/2007/07/10/bpel-in-cluster-in-which-node-my-process-is-working","metadata":{"permalink":"/blog/2007/07/10/bpel-in-cluster-in-which-node-my-process-is-working","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-07-10-bpel-in-cluster-in-which-node-my-process-is-working.md","source":"@site/blog/2007-07-10-bpel-in-cluster-in-which-node-my-process-is-working.md","title":"BPEL in Cluster: In which node my process is working","description":"I was helping a customer with his BPEL in cluster and we needed to follow the flow and on which machine the instance was running.","date":"2007-07-10T00:00:00.000Z","formattedDate":"July 10, 2007","tags":[],"readingTime":0.565,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"BPEL in Cluster: In which node my process is working","categories":"soa oracle"},"prevItem":{"title":"Visiting Paris with Velib (new bike transit system)","permalink":"/blog/2007/07/18/visiting-paris-with-velib-new-bike-transit-system"},"nextItem":{"title":"Using your Built-In iSight with Parallels (and VMWare)","permalink":"/blog/2007/06/04/using-your-built-in-isight-with-parallels-and-vmware"}},"content":"I was helping a customer with his BPEL in cluster and we needed to follow the flow and on which machine the instance was running.\\n\\nI simply use a `bpel:exec` activity with the following code:\\n\\n\\n```java\\njava.net.InetAddress addr = null;\\n\\ntry {\\n  addr = java.net.InetAddress.getLocalHost();\\n}\\ncatch (Exception e) {\\n  System.out.println(\\"EXCEPTION :\\"+ e);\\n}\\nsetVariableData(\\"HostNameVariable\\", addr.getCanonicalHostName());\\n```\\n\\nThis code is just an example of what could be done. Here I am using java.net code API and put the result in a BPEL global variable using the `setVariableData()` method. Obviously you will use appropriate code to differenciate the different nodes for example the name of the OC4J instance, hostname, ... or any interesting value."},{"id":"/2007/06/04/using-your-built-in-isight-with-parallels-and-vmware","metadata":{"permalink":"/blog/2007/06/04/using-your-built-in-isight-with-parallels-and-vmware","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-06-04-using-your-built-in-isight-with-parallels-and-vmware.md","source":"@site/blog/2007-06-04-using-your-built-in-isight-with-parallels-and-vmware.md","title":"Using your Built-In iSight with Parallels (and VMWare)","description":"It is probably an old tipe, but I have just configured my Windows XP Parallels VMs","date":"2007-06-04T00:00:00.000Z","formattedDate":"June 4, 2007","tags":[],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using your Built-In iSight with Parallels (and VMWare)","categories":"osx apple"},"prevItem":{"title":"BPEL in Cluster: In which node my process is working","permalink":"/blog/2007/07/10/bpel-in-cluster-in-which-node-my-process-is-working"},"nextItem":{"title":"Endangered Peregrine Falcons Return to Oracle HQ","permalink":"/blog/2007/05/25/endangered-peregrine-falcons-return-to-oracle-hq"}},"content":"It is probably an old tipe, but I have just configured my Windows XP Parallels VMs\\nto use my MacBook buit-in iSight Web Cam using the driver that you can find here:\\n- [Download iSight Driver for XP](http://www.maconlysource.com/?p=885)\\n\\nThe site states that it does not work with Parallels, but I am using the release Build 3170 RC3 and I do not have any issue.\\n\\n1. Start your VM\\n2. Download the Driver &amp; Unip zip\\n\\n3. Grab the Built-In iSight using the Devices > USB  menu\\n\\n4. Point the Windows Devices Manager Installer to the location of the Unzipped folder\\n5. and you are done..."},{"id":"/2007/05/25/endangered-peregrine-falcons-return-to-oracle-hq","metadata":{"permalink":"/blog/2007/05/25/endangered-peregrine-falcons-return-to-oracle-hq","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-05-25-endangered-peregrine-falcons-return-to-oracle-hq.md","source":"@site/blog/2007-05-25-endangered-peregrine-falcons-return-to-oracle-hq.md","title":"Endangered Peregrine Falcons Return to Oracle HQ","description":"Peregrine falcons have returned to Oracle\'s campus in Redwood Shores, California. You can watch the newborns on the Oracle Falconcam.","date":"2007-05-25T00:00:00.000Z","formattedDate":"May 25, 2007","tags":[],"readingTime":0.145,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Endangered Peregrine Falcons Return to Oracle HQ","categories":null},"prevItem":{"title":"Using your Built-In iSight with Parallels (and VMWare)","permalink":"/blog/2007/06/04/using-your-built-in-isight-with-parallels-and-vmware"},"nextItem":{"title":"Oracle Development Kit for Spring","permalink":"/blog/2007/05/21/oracle-development-kit-for-spring"}},"content":"Peregrine falcons have returned to Oracle\'s campus in Redwood Shores, California. You can watch the newborns on the *Oracle Falconcam*.\\n\\n[![](http://oracleimg.com/admin/images/ocom/hp/news/news_image_falcon.jpg)](http://oracleimg.com/admin/images/ocom/hp/news/news_image_falcon.jpg)\\n\\nRead more and watch videos on [Oracle\'s Site](http://www.oracle.com/corporate/community/stories/falcons-return.html)."},{"id":"/2007/05/21/oracle-development-kit-for-spring","metadata":{"permalink":"/blog/2007/05/21/oracle-development-kit-for-spring","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-05-21-oracle-development-kit-for-spring.md","source":"@site/blog/2007-05-21-oracle-development-kit-for-spring.md","title":"Oracle Development Kit for Spring","description":"To help you better understand the support Oracle offers to Spring developers, everything you need to get started with Spring on Oracle Containers for Java EE is now available in the Oracle Development Kit for Spring. Whether you\'re new to Spring or an old hand, you\'re sure to benefit from the contents of the Development Kit:","date":"2007-05-21T00:00:00.000Z","formattedDate":"May 21, 2007","tags":[],"readingTime":0.655,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Development Kit for Spring","categories":"spring oracle java"},"prevItem":{"title":"Endangered Peregrine Falcons Return to Oracle HQ","permalink":"/blog/2007/05/25/endangered-peregrine-falcons-return-to-oracle-hq"},"nextItem":{"title":"OracleAS &amp; JDeveloper: JavaEE 5.0 Technology Preview","permalink":"/blog/2007/05/16/oracleas-and-jdeveloper-javaee-5-dot-0-technology-preview"}},"content":"To help you better understand the support Oracle offers to Spring developers, everything you need to get started with Spring on Oracle Containers for Java EE is now available in the Oracle Development Kit for Spring. Whether you\'re new to Spring or an old hand, you\'re sure to benefit from the contents of the Development Kit:\\n\\n* A comprehensive set of How-To examples illustrating Spring-OC4J integration\\n* The Oracle Developer Depot Web application, which enables you to deploy and run the How-To projects on OC4J\\n* A Spring extension to JDeveloper, Oracle\'s free Java IDE\\n* Supporting documentation and white papers\\n\\nThe Kit is available as [a self-extracting ZIP archive](http://download-hq.oracle.com/otn/esd/other/ODK4Spring.zip). Note that you will need Oracle Containers for Java EE (10.1.3.2) or higher to use all of the How-To projects provided."},{"id":"/2007/05/16/oracleas-and-jdeveloper-javaee-5-dot-0-technology-preview","metadata":{"permalink":"/blog/2007/05/16/oracleas-and-jdeveloper-javaee-5-dot-0-technology-preview","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-05-16-oracleas-and-jdeveloper-javaee-5-dot-0-technology-preview.md","source":"@site/blog/2007-05-16-oracleas-and-jdeveloper-javaee-5-dot-0-technology-preview.md","title":"OracleAS &amp; JDeveloper: JavaEE 5.0 Technology Preview","description":"During JavaOne last week Oracle has announced the availability of a the new JavaEE 5.0 Technology Preview of Oracle Containers for JavaEE and JDeveloper (release 11).","date":"2007-05-16T00:00:00.000Z","formattedDate":"May 16, 2007","tags":[],"readingTime":0.24,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OracleAS &amp; JDeveloper: JavaEE 5.0 Technology Preview","categories":"java oracle jdeveloper oc4j"},"prevItem":{"title":"Oracle Development Kit for Spring","permalink":"/blog/2007/05/21/oracle-development-kit-for-spring"},"nextItem":{"title":"Oracle Develop 2007: conference near you from Oracle","permalink":"/blog/2007/04/03/oracle-develop-2007-conference-near-you-from-oracle"}},"content":"During JavaOne last week Oracle has announced the availability of a the new JavaEE 5.0 Technology Preview of Oracle Containers for JavaEE and JDeveloper (release 11).\\n\\nFind more information on OTN:\\n\\n* [OC4J 11: Java EE 5 Technology Preview](http://www.oracle.com/technology/tech/java/oc4j/11/index.html)\\n\\n* [JDeveloper 11 Technology preview](http://www.oracle.com/technology/products/jdev/11/index.html) includes a Mac release"},{"id":"/2007/04/03/oracle-develop-2007-conference-near-you-from-oracle","metadata":{"permalink":"/blog/2007/04/03/oracle-develop-2007-conference-near-you-from-oracle","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-04-03-oracle-develop-2007-conference-near-you-from-oracle.md","source":"@site/blog/2007-04-03-oracle-develop-2007-conference-near-you-from-oracle.md","title":"Oracle Develop 2007: conference near you from Oracle","description":"Oracle is has started to deliver the \\"Oracle Developer 2007\\" conference. In this two days even you will learn more about multiple technologies: Enterprise Java,","date":"2007-04-03T00:00:00.000Z","formattedDate":"April 3, 2007","tags":[],"readingTime":0.685,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Develop 2007: conference near you from Oracle","categories":"conference oracle"},"prevItem":{"title":"OracleAS &amp; JDeveloper: JavaEE 5.0 Technology Preview","permalink":"/blog/2007/05/16/oracleas-and-jdeveloper-javaee-5-dot-0-technology-preview"},"nextItem":{"title":"Online Public Libraries: Google Books and now.. Europeana","permalink":"/blog/2007/03/29/online-public-libraries-google-books-and-now-dot-europeana"}},"content":"Oracle is has started to deliver the \\"Oracle Developer 2007\\" conference. In this two days even you will learn more about multiple technologies: Enterprise Java,\\nSOA, .NET, Databases and PL/SQL, as well as Ajax, PHP, Spring, and more.\\n\\nThis event is organized in 4 different tracks for developers:\\n\\n* [Java](http://www.oracle.com/technology/events/develop2007/java.html)\\n* [SOA](http://www.oracle.com/technology/events/develop2007/soa.html)\\n* [.Net](http://www.oracle.com/technology/events/develop2007/dotnet.html)\\n* [Database](http://www.oracle.com/technology/events/develop2007/database.html)\\n\\nFind an Oracle Develop event near you:\\n\\n* May 14-15, 2007 | [Seoul, Lotte Jamsil](http://www.oracle.com/technology/events/develop2007/registration-seoul.html)\\n* May 17-18, 2007 | [Bangalore, The Grand Ashok](http://www.oracle.com/technology/events/develop2007/registration-bangalore.html)\\n* May 22-23, 2007 | [Beijing, China World Hotel](http://www.oracle.com/technology/global/cn/events/develop2007)\\n* June 18-19, 2007 | [Munich, Arabella Sheraton](http://www.oracle.com/technology/events/develop2007/registration-munich.html)\\n* June 21-22, 2007 | [Prague, Prague Conference Center](http://www.oracle.com/technology/events/develop2007/registration-prague.html)\\n* June 26-27, 2007 | [London, ExCeL](http://www.oracle.com/technology/events/develop2007/registration-london.html)\\nI will be presenting the Java Developer conference and SOA in Munich, Prague\\nand London at the end of June... see you there !"},{"id":"/2007/03/29/online-public-libraries-google-books-and-now-dot-europeana","metadata":{"permalink":"/blog/2007/03/29/online-public-libraries-google-books-and-now-dot-europeana","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-29-online-public-libraries-google-books-and-now-dot-europeana.md","source":"@site/blog/2007-03-29-online-public-libraries-google-books-and-now-dot-europeana.md","title":"Online Public Libraries: Google Books and now.. Europeana","description":"I am sure that you know about Google Books//books.google.com/","date":"2007-03-29T00:00:00.000Z","formattedDate":"March 29, 2007","tags":[],"readingTime":0.605,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Online Public Libraries: Google Books and now.. Europeana","categories":"google"},"prevItem":{"title":"Oracle Develop 2007: conference near you from Oracle","permalink":"/blog/2007/04/03/oracle-develop-2007-conference-near-you-from-oracle"},"nextItem":{"title":"Grails vs Rails Performance Benchmark","permalink":"/blog/2007/03/23/grails-vs-rails-performance-benchmark"}},"content":"I am sure that you know about Google Books: [http://books.google.com/](http://books.google.com/)\\n\\nEven if I truly believe that Google wants the good by publishing all this information for free - We all remember the sentence: \\"Don\'t be evil\\" that is part of the [Google code of Conduct](http://investor.google.com/conduct.html)-. I think it is not a good idea to have only one service that provide access to the \\"world culture\\"... Competition is always good for consumer...\\n\\nThe French National Library ([BNF](http://www.bnf.fr/)) and some other public libraries (Hungarian, Portugal) have created a new site to offer a similar service in beta:  [http://www.europeana.eu](http://www.europeana.eu/). It is true that currently the list of books is really small compare to Google but I am inviting you to use both of then."},{"id":"/2007/03/23/grails-vs-rails-performance-benchmark","metadata":{"permalink":"/blog/2007/03/23/grails-vs-rails-performance-benchmark","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-23-grails-vs-rails-performance-benchmark.md","source":"@site/blog/2007-03-23-grails-vs-rails-performance-benchmark.md","title":"Grails vs Rails Performance Benchmark","description":"Graeme, the Grails Project Lead has published a very interesting benchmark of Grails versus Rails applications, that is available on the Grails wiki:","date":"2007-03-23T00:00:00.000Z","formattedDate":"March 23, 2007","tags":[],"readingTime":0.585,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Grails vs Rails Performance Benchmark","categories":"grails groovy java rails"},"prevItem":{"title":"Online Public Libraries: Google Books and now.. Europeana","permalink":"/blog/2007/03/29/online-public-libraries-google-books-and-now-dot-europeana"},"nextItem":{"title":"Tangosol is joining the Oracle family","permalink":"/blog/2007/03/23/tangosol-is-joining-the-oracle-family"}},"content":"Graeme, the Grails Project Lead has published a very interesting benchmark of Grails versus Rails applications, that is available on the Grails wiki:\\n\\n*   [Grails vs Rails Performance Benchmarking](http://grails.org/Grails+vs+Rails+Benchmark)I let you read and comment on the Wiki the results... Grails development team is open to comment and improvement of the bench to capture as much information as possible.\\n\\nIt is also important to note that performances, productivity are very important when choosing a development environment; but something that is also key for enterprises, is the fact that Grails/Groovy \\"are\\" Java/J2EE based. This means that a Grails application is packaged, deployed, administered and monitored like any applications that already exist in the information system on J2EE application servers."},{"id":"/2007/03/23/tangosol-is-joining-the-oracle-family","metadata":{"permalink":"/blog/2007/03/23/tangosol-is-joining-the-oracle-family","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-23-tangosol-is-joining-the-oracle-family.md","source":"@site/blog/2007-03-23-tangosol-is-joining-the-oracle-family.md","title":"Tangosol is joining the Oracle family","description":"During The Server Side Symposium in Vegas, Thomas Kurian announced that Oracle is acquiring Tangosol, during the keynote, Cameron Purdy (Tangosol\'s CEO) has demonstrated how cool Coherence can be, and they have presented how it could be used in the context of XTP (Extreme Transaction Processing).","date":"2007-03-23T00:00:00.000Z","formattedDate":"March 23, 2007","tags":[],"readingTime":0.345,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Tangosol is joining the Oracle family","categories":"java oracle"},"prevItem":{"title":"Grails vs Rails Performance Benchmark","permalink":"/blog/2007/03/23/grails-vs-rails-performance-benchmark"},"nextItem":{"title":"Using Groovy within Oracle Data Integrator","permalink":"/blog/2007/03/18/using-groovy-within-oracle-data-integrator"}},"content":"During The Server Side Symposium in Vegas, Thomas Kurian announced that Oracle is acquiring [Tangosol](http://tangosol.com/), during the keynote, Cameron Purdy (Tangosol\'s CEO) has demonstrated how cool Coherence can be, and they have presented how it could be used in the context of XTP (Extreme Transaction Processing).\\n\\nSoon more to come about this in the different technical side and blog of the Oracle\'s community.\\n\\n*   See [Oracle and Tangosol page](http://www.oracle.com/tangosol/index.html)"},{"id":"/2007/03/18/using-groovy-within-oracle-data-integrator","metadata":{"permalink":"/blog/2007/03/18/using-groovy-within-oracle-data-integrator","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-18-using-groovy-within-oracle-data-integrator.md","source":"@site/blog/2007-03-18-using-groovy-within-oracle-data-integrator.md","title":"Using Groovy within Oracle Data Integrator","description":"As you may remember, Oracle acquired few months back a data integration solution from Sunopsis, and has integrated it in our product under the name \\"Oracle Data Integrator\\" (ODI).","date":"2007-03-18T00:00:00.000Z","formattedDate":"March 18, 2007","tags":[],"readingTime":0.67,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using Groovy within Oracle Data Integrator","categories":"groovy oracle"},"prevItem":{"title":"Tangosol is joining the Oracle family","permalink":"/blog/2007/03/23/tangosol-is-joining-the-oracle-family"},"nextItem":{"title":"JavaEE 5 Features of OracleAS 10gR3","permalink":"/blog/2007/03/07/javaee-5-features-of-oracleas-10gr3"}},"content":"As you may remember, Oracle acquired few months back a data integration solution from Sunopsis, and has integrated it in our product under the name \\"[Oracle Data Integrator](http://www.oracle.com/technology/products/oracle-data-integrator/index.html)\\" (ODI).\\n\\nOne of the first thing that I look when we got this product was the support for Scripting technologies, since it makes lot of sense to have such support in any ETL. And yes ODI has support for Jython (thought BSF). So when the dev team joined Oracle, I\'ve asked the following question \\"did you try to use Groovy?\\"... I personnaly did not take the time to test, but one of the developer did and document it in his blog:\\n\\"[Using Groovy and ODI](http://thierrychantier.blogspot.com/2007/04/groovy-and-odi.html)\\" from Thierry. When I get a chance I will provide a more complete sample of using Groovy in Oracle Data Integrator."},{"id":"/2007/03/07/javaee-5-features-of-oracleas-10gr3","metadata":{"permalink":"/blog/2007/03/07/javaee-5-features-of-oracleas-10gr3","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-07-javaee-5-features-of-oracleas-10gr3.md","source":"@site/blog/2007-03-07-javaee-5-features-of-oracleas-10gr3.md","title":"JavaEE 5 Features of OracleAS 10gR3","description":"OracleAS 10g R3 (10.1.3.x) is a certified J2EE 1.4 container, but OracleAS provides already support to some of the features of the Java Enterprise Edition 5","date":"2007-03-07T00:00:00.000Z","formattedDate":"March 7, 2007","tags":[],"readingTime":0.935,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaEE 5 Features of OracleAS 10gR3","categories":"javaee oracle oc4j"},"prevItem":{"title":"Using Groovy within Oracle Data Integrator","permalink":"/blog/2007/03/18/using-groovy-within-oracle-data-integrator"},"nextItem":{"title":"Oracle donates Toplink to Eclipse","permalink":"/blog/2007/03/06/oracle-donates-toplink-to-eclipse"}},"content":"[OracleAS 10g R3 (10.1.3.x)](http://otn.oracle.com/products/oc4j) is a certified J2EE 1.4 container, but OracleAS provides already support to some of the features of the Java Enterprise Edition 5: JavaEE 5. One of the key driver of the new EE version was simplification of the development and deployment applications. Here is the list of the JavaEE 5 features that are supported in OracleAS 10gR3 that will simplify the development of applications *(in comparison to a standard J2EE 1.x development)*:\\n\\n*   Java Persistence API (JPA) and EJB 3.0 ([documention](http://download-uk.oracle.com/docs/cd/B32110_01/web.1013/b28221/toc.htm))\\n\\n*   Support of shared library at the EAR level (`&lt;library-directory&gt; / applib`). ([documentation](http://download.oracle.com/docs/cd/B31017_01/web.1013/b28952/classload.htm#BABGAABD)). This comes in addition to the OracleAS 10gR3 classloader framework that allows administrators to create, and version shared libraries that can be used into applications by referencing them in the deployment plan.\\n\\n*   Annotations Based Web Services (JSR181)  that could be used for Java classes and EJB3 Session Beans ([documentation](http://download.oracle.com/docs/cd/B32110_01/web.1013/b28974/devannotation.htm#CHDFJBEH))\\n\\n*   Referencing resources using annotations in the Web container: `@EJB, @Resource, @Resources, @PostConstruct, @PreDestroy, @PersistenceUnit, @PersistenceContext, @WebServiceRef, @DeclaresRoles, @RunAs`  ([documentation](http://download.oracle.com/docs/cd/B32110_01/web.1013/b28959/annotations.htm#sthref248))\\nHope that this small summary will give you the opportunity to test some of the features of OracleAS 10g."},{"id":"/2007/03/06/oracle-donates-toplink-to-eclipse","metadata":{"permalink":"/blog/2007/03/06/oracle-donates-toplink-to-eclipse","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-06-oracle-donates-toplink-to-eclipse.md","source":"@site/blog/2007-03-06-oracle-donates-toplink-to-eclipse.md","title":"Oracle donates Toplink to Eclipse","description":"Oracle has announced yesterday during  EclipseCon in Santa Clara (California) that it open-sources Oracle Toplink as part of the Eclipse Project.","date":"2007-03-06T00:00:00.000Z","formattedDate":"March 6, 2007","tags":[],"readingTime":0.87,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle donates Toplink to Eclipse","categories":"eclipse jpa toplink"},"prevItem":{"title":"JavaEE 5 Features of OracleAS 10gR3","permalink":"/blog/2007/03/07/javaee-5-features-of-oracleas-10gr3"},"nextItem":{"title":"Netbeans : OC4J support available","permalink":"/blog/2007/03/02/netbeans-oc4j-support-available"}},"content":"Oracle has announced yesterday during  [EclipseCon](http://www.eclipsecon.org/) in Santa Clara (California) that it *open-sources* Oracle Toplink as part of the Eclipse Project.\\n\\nThe idea is to open source all the features of Toplink (ORM/JPA, OXM/JAXB, and EIS support), but also features that are currently under development and will be part of the Eclipse project:\\n\\n*   Service Data Objects (SDO) implementation and SDO Data Access Service (DAS) that leverages JPA for use with SDO.*   XR (XML-Relational) that provides a completely metadata driven approach to obtaining relational data as XML.\\n*   DBWS which exposes XR as a web service. With DBWS, you can easily build web services that access relational data without any programming.\\n\\nOne part will be kept by Oracle, this is the \\"glue\\" code that is used for the integration with OracleAS that is probably not useful for anybody but Oracle.\\nFor details check out the [press release](http://www.oracle.com/corporate/press/2007_mar/OpenSource-TopLink.html) and the [FAQ](http://www.oracle.com/technology/tech/eclipse/pdf/eclipselink-faq.pdf).\\n\\nIf you are at EclipseCon do not hesitate to visit the Oracle booth to learn more about this great news for the Java developers."},{"id":"/2007/03/02/netbeans-oc4j-support-available","metadata":{"permalink":"/blog/2007/03/02/netbeans-oc4j-support-available","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-03-02-netbeans-oc4j-support-available.md","source":"@site/blog/2007-03-02-netbeans-oc4j-support-available.md","title":"Netbeans : OC4J support available","description":"It has been a long time that I did not look in the update center and development wiki of Netbeans. And I have been very pleased to see that it is possible now to register OC4J 10g as a server in Netbeans 5.5 (and 6.0). To add it in your environment just do a :","date":"2007-03-02T00:00:00.000Z","formattedDate":"March 2, 2007","tags":[],"readingTime":0.51,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Netbeans : OC4J support available","categories":"netbeans oracle oc4j"},"prevItem":{"title":"Oracle donates Toplink to Eclipse","permalink":"/blog/2007/03/06/oracle-donates-toplink-to-eclipse"},"nextItem":{"title":"Spring and Oracle: I21 blog entry about Oracle contribution to Spring","permalink":"/blog/2007/02/27/spring-and-oracle-i21-blog-entry-about-oracle-contribution-to-spring"}},"content":"It has been a long time that I did not look in the update center and development wiki of Netbeans. And I have been very pleased to see that it is possible now to register OC4J 10g as a server in Netbeans 5.5 (and 6.0). To add it in your environment just do a :\\n\\n*   Tools > Update Center\\n*   Select \\"Netbeans Update Center Beta\\"\\n*   Select OC4J 10gYou can then configure a new Server and run/stop the server from your IDE.\\n\\n![]( http://2.bp.blogspot.com/_aoQgQ1obiyE/RehrkeHe9xI/AAAAAAAAAAw/0DruPeS4ijQ/s1600-h/nb-002.png )\\n\\nYou can follow the development of this plugin directly in the [Netbeans Wiki OC4J Support](http://wiki.netbeans.org/wiki/view/OC4JSupport) page."},{"id":"/2007/02/27/spring-and-oracle-i21-blog-entry-about-oracle-contribution-to-spring","metadata":{"permalink":"/blog/2007/02/27/spring-and-oracle-i21-blog-entry-about-oracle-contribution-to-spring","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-27-spring-and-oracle-i21-blog-entry-about-oracle-contribution-to-spring.md","source":"@site/blog/2007-02-27-spring-and-oracle-i21-blog-entry-about-oracle-contribution-to-spring.md","title":"Spring and Oracle: I21 blog entry about Oracle contribution to Spring","description":"Rod Johnson in the Interface21 team blog entry named \\"Oracle Contributing Oracle Application Server Integration Code to Spring Framework\\" described the contribution of Oracle to Spring framework.","date":"2007-02-27T00:00:00.000Z","formattedDate":"February 27, 2007","tags":[],"readingTime":0.485,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Spring and Oracle: I21 blog entry about Oracle contribution to Spring","categories":"spring oracle java"},"prevItem":{"title":"Netbeans : OC4J support available","permalink":"/blog/2007/03/02/netbeans-oc4j-support-available"},"nextItem":{"title":"OC4J tip: changing the server information","permalink":"/blog/2007/02/26/oc4j-tip-changing-the-server-information"}},"content":"Rod Johnson in the Interface21 team blog entry named \\"[Oracle Contributing Oracle Application Server Integration Code to Spring Framework](http://blog.interface21.com/main/2007/02/27/oracle-contributing-oracle-application-server-integration-code-to-spring-framework/)\\" described the contribution of Oracle to Spring framework.\\n\\nThis contribution allows Spring applications to leverage the OracleAS transaction manager and its features. In addition to this contribution and some previous investments of Oracle in Spring, see the [Oracle and Spring](http://www.oracle.com/technology/tech/java/spring.html) page of OTN; Oracle continues to invest more and more in Spring, as contributor or as user since, as stated by Rod, Spring has an important roles in  [Oracle Fusion Middleware](http://www.oracle.com/products/middleware/index.html), and its upcoming [Services Component Architecture](http://www.osoa.org/) offering."},{"id":"/2007/02/26/oc4j-tip-changing-the-server-information","metadata":{"permalink":"/blog/2007/02/26/oc4j-tip-changing-the-server-information","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-26-oc4j-tip-changing-the-server-information.md","source":"@site/blog/2007-02-26-oc4j-tip-changing-the-server-information.md","title":"OC4J tip: changing the server information","description":"When you are running OC4J in stand alone mode you are using the HTTP server that is bundle with it. This HTTP server returns by default for the HTTP information the following information Oracle Containers for J2EE","date":"2007-02-26T00:00:00.000Z","formattedDate":"February 26, 2007","tags":[],"readingTime":0.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OC4J tip: changing the server information","categories":"oc4j"},"prevItem":{"title":"Spring and Oracle: I21 blog entry about Oracle contribution to Spring","permalink":"/blog/2007/02/27/spring-and-oracle-i21-blog-entry-about-oracle-contribution-to-spring"},"nextItem":{"title":"The first International Grails eXchange 2007","permalink":"/blog/2007/02/22/the-first-international-grails-exchange-2007"}},"content":"When you are running OC4J in stand alone mode you are using the HTTP server that is bundle with it. This HTTP server returns by default for the HTTP information the following information: *Server: Oracle Containers for J2EE*\\n\\nIf you want to change that you just need to set the `http.server.header` property. For example,\\n\\n```\\njava -Dhttp.server.header=\\"My blog on Oracle\\" -jar oc4j.jar\\n```\\n\\nwill now look like:\\n\\n```\\nHTTP/1.1 200 OK\\nDate: Mon, 26 Feb 2007 21:52:53 GMT\\nServer: My blog on Oracle\\nLast-Modified: Mon, 09 Oct 2006 19:17:10 GMT\\nAccept-Ranges: bytes\\nContent-Length: 19882\\nConnection: close\\nContent-Type: text/html\\n```\\n\\n\\nThanks to James Kirsh for this *very useful* tip..."},{"id":"/2007/02/22/the-first-international-grails-exchange-2007","metadata":{"permalink":"/blog/2007/02/22/the-first-international-grails-exchange-2007","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-22-the-first-international-grails-exchange-2007.md","source":"@site/blog/2007-02-22-the-first-international-grails-exchange-2007.md","title":"The first International Grails eXchange 2007","description":"Packed with presentations on Grails, Groovy, Ajax & Web 2.0 and JavaEE and the core technologies that support the Grails technology, the first Grails eXchange conference (London from May 30th to June 1st) will be the place to be for any member of the Groovy/Grails community... You can already register on the conference web site//www.grails-exchange.com","date":"2007-02-22T00:00:00.000Z","formattedDate":"February 22, 2007","tags":[],"readingTime":0.495,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"The first International Grails eXchange 2007","categories":"groovy grails"},"prevItem":{"title":"OC4J tip: changing the server information","permalink":"/blog/2007/02/26/oc4j-tip-changing-the-server-information"},"nextItem":{"title":"Groovy 101: Extracting XML from your database","permalink":"/blog/2007/02/20/groovy-101-extracting-xml-from-your-database"}},"content":"Packed with presentations on Grails, Groovy, Ajax & Web 2.0 and JavaEE and the core technologies that support the Grails technology, the first [Grails eXchange](http://www.grails-exchange.com/) conference (London from May 30th to June 1st) will be the place to be for any member of the Groovy/Grails community... You can already register on the conference web site: [http://www.grails-exchange.com](http://www.grails-exchange.com/)\\n\\n\\nCome to see [speakers](http://www.grails-exchange.com/speakers) including Grails project lead [Graeme Rocher](http://www.grails-exchange.com/graeme-rocher), Groovy project lead [Guillaume LaForge](http://www.grails-exchange.com/guillaume-laforge), creator of Spring [Rod Johnson](http://www.grails-exchange.com/rod-johnson), Dojo\'s [Alex Russel and Dylan Schieman](http://www.grails-exchange.com/alex-russel-dylan-schiemann) as well as speakers from technology-leading organisations such as [Google](http://www.grails-exchange.com/joe-walnes), [Interface 21](http://www.grails-exchange.com/rob-harrop), [JBoss](http://www.grails-exchange.com/manik-surtani), [Oracle](http://www.grails-exchange.com/tugdual-grall) &amp; [Sun Microsystems](http://www.grails-exchange.com/geertjan-weilenga)"},{"id":"/2007/02/20/groovy-101-extracting-xml-from-your-database","metadata":{"permalink":"/blog/2007/02/20/groovy-101-extracting-xml-from-your-database","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-20-groovy-101-extracting-xml-from-your-database.md","source":"@site/blog/2007-02-20-groovy-101-extracting-xml-from-your-database.md","title":"Groovy 101: Extracting XML from your database","description":"In the previous entry I showed how you can easily take an XML feed and insert the content in the database. Let\'s do the opposite now, meaning taking the data out of your database as XML. In this post I am using the Sql Dataset again but to create an XML document, using the Groovy MarkupBuilder.","date":"2007-02-20T00:00:00.000Z","formattedDate":"February 20, 2007","tags":[],"readingTime":0.87,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Groovy 101: Extracting XML from your database","categories":"groovy"},"prevItem":{"title":"The first International Grails eXchange 2007","permalink":"/blog/2007/02/22/the-first-international-grails-exchange-2007"},"nextItem":{"title":"Groovy 101: Importing XML in your database","permalink":"/blog/2007/02/18/groovy-101-importing-xml-in-your-database"}},"content":"In the previous entry I showed how you can easily take an XML feed and insert the content in the database. Let\'s do the opposite now, meaning taking the data out of your database as XML. In this post I am using the Sql Dataset again but to create an XML document, using the Groovy MarkupBuilder.\\n\\n``` groovy\\nimport groovy.sql.Sql;\\nimport groovy.xml.MarkupBuilder;\\n\\ndef sql = Sql.newInstance(\\"jdbc:oracle:thin:@//tgrall-linux:1521/XE\\",\\n\\"HR\\", \\"HR\\", \\"oracle.jdbc.driver.OracleDriver\\")\\ndef set = sql.dataSet(\\"EMPLOYEES\\");\\n\\ndef writer = new StringWriter()\\ndef xml = new MarkupBuilder(writer)\\n\\nxml.employees() {\\n  set.each { emp ->\\n    employee(first: emp.first_name , last: emp.last_name) {\\n      email( emp.email )\\n    }\\n\\n  }\\n}\\n\\nprintln writer.toString();\\n```\\n\\nAs you can see, I use the builder to create XML Elements and attributes employee(first: emp.first_name , last: emp.last_name), I do reference the current record of the dataset (emp), and all this in very simple and concise code.\\nThis will give a result like:\\n\\n``` xml\\n<employees>\\n  <employee first=\\"\'Steven\'\\" last=\\"\'King\'\\">\\n    <email>SKING</email>\\n  </employee>\\n  <employee first=\\"\'Neena\'\\" last=\\"\'Kochhar\'\\">\\n    <email>NKOCHHAR</email>\\n  </employee>\\n  <employee first=\\"\'Lex\'\\" last=\\"\'De\\">\\n    <email>LDEHAAN</email>\\n  </employee>\\n  ...\\n</employees>\\n```\\n\\nSo once again quite simple."},{"id":"/2007/02/18/groovy-101-importing-xml-in-your-database","metadata":{"permalink":"/blog/2007/02/18/groovy-101-importing-xml-in-your-database","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-18-groovy-101-importing-xml-in-your-database.md","source":"@site/blog/2007-02-18-groovy-101-importing-xml-in-your-database.md","title":"Groovy 101: Importing XML in your database","description":"A friend of mine was looking for an easy way to import some XML content in his database. You have many ways to do it. But the easiest for a Java/Groovy developer is to use Groovy, and I have create this small example for him.","date":"2007-02-18T00:00:00.000Z","formattedDate":"February 18, 2007","tags":[],"readingTime":1.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Groovy 101: Importing XML in your database","categories":"groovy"},"prevItem":{"title":"Groovy 101: Extracting XML from your database","permalink":"/blog/2007/02/20/groovy-101-extracting-xml-from-your-database"},"nextItem":{"title":"Oracle Application Server 10gR3: iHat","permalink":"/blog/2007/02/15/oracle-application-server-10gr3-ihat"}},"content":"A friend of mine was looking for an easy way to import some XML content in his database. You have many ways to do it. But the easiest for a Java/Groovy developer is to use Groovy, and I have create this small example for him.\\n\\nGroovy provides really simple solution to parse XML and manipulate your database. The following sample reads an RSS new feed and import the title and link in a table named NEWS that contains two columns TITLE and LINK.\\n\\n``` groovy\\nimport groovy.sql.Sql;\\n\\ndef rssFeed = \\"http://www.javablogs.com/ViewDaysBlogs.action?view=rss\\";\\ndef xmlFeed      = new XmlParser().parse(rssFeed);\\n\\ndef sql = Sql.newInstance(\\"jdbc:oracle:thin:@//tgrall-linux:1521/XE\\",\\n\\"GROOVY\\",\\n\\"GROOVY\\",\\n\\"oracle.jdbc.driver.OracleDriver\\")\\ndef set = sql.dataSet(\\"NEWS\\");\\n\\n(0..&lt; xmlFeed.channel.item.size()).each {\\n  def item = xmlFeed.channel.item.get(it);\\n  def title = item.title.value[0];\\n  def link = item.link.value[0];\\n  println(\\"Importing $title ...\\");\\n  set.add(TITLE: title, LINK: link);\\n}\\n\\n```\\n\\nFirst I create a Groovy SQL object and a DataSet to manipulate my data `sql.dataSet(\\"NEWS\\")`. Do not forget, if like me you are using Oracle database, to add the Oracle JDBC driver to your classpath ;-)\\n\\nThen I create a loop on each items of the RSS feed I am using: `(0..&gt; xmlFeed.channel.item.size()).each {...}`. As you see, Groovy XML help me to parse, and navigate the XML document.\\n\\nLike any Groovy iterator you have access to an implicit object available in the loop \\"`it`\\", so I can get the item using the Groovy XML : `xmlFeed.channel.item.get(it)`\\n\\nThen you can get the different values you want of the  item element.Using the dataset.add method, you can insert them in the table.This is done using the  value pairs notation *column:value*, this looks like: `set.add(TITLE: title, LINK: link)`"},{"id":"/2007/02/15/oracle-application-server-10gr3-ihat","metadata":{"permalink":"/blog/2007/02/15/oracle-application-server-10gr3-ihat","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-15-oracle-application-server-10gr3-ihat.md","source":"@site/blog/2007-02-15-oracle-application-server-10gr3-ihat.md","title":"Oracle Application Server 10gR3: iHat","description":"When preparing a complex topology, where you have multiple HTTP servers ,talking with many OC4J instances, it is sometimes hard to understand what is going on. Oracle Application Server Control provides the complete view of your topologies in different pages. If you want to have a quick overview of your topology, you may want a more graphical view of your Oracle Application Server instance.","date":"2007-02-15T00:00:00.000Z","formattedDate":"February 15, 2007","tags":[],"readingTime":1.535,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Application Server 10gR3: iHat","categories":"oracle"},"prevItem":{"title":"Groovy 101: Importing XML in your database","permalink":"/blog/2007/02/18/groovy-101-importing-xml-in-your-database"},"nextItem":{"title":"Oracle Web Services: Sharing Session between client calls","permalink":"/blog/2007/02/15/oracle-web-services-sharing-session-between-client-calls"}},"content":"When preparing a complex topology, where you have multiple HTTP servers ,talking with many OC4J instances, it is sometimes hard to understand what is going on. Oracle Application Server Control provides the complete view of your topologies in different pages. If you want to have a quick overview of your topology, you may want a more graphical view of your Oracle Application Server instance.\\n\\nOne of the tool that I use a lot to present OracleAS and its components is [OracleAS Hi-Av Tool 10g (10.1.3)](http://www.oracle.com/technology/products/ias/utilities/ihat1013.zip) also known as **iHat**. This utility uses Oracle Process & Notification Manager (OPMN) to gather information of all the components used in your topology, representing it in a nice graphical viewer. In addition to use iHat to show the different components, I do also use that to validate my configuration.\\n\\n![iHat View](http://2.bp.blogspot.com/_aoQgQ1obiyE/RdSOpbcqAJI/AAAAAAAAAAk/huSO5c9xg9o/s1600/ihat001.png)\\n\\nIn this case I am showing a specific instance, that contains 3 OC4Js instance, with 2 of them that are in the same group.  When using iHat you will notice that you can, in addition to have some monitoring information start, stop, restart the different components directly from the view.\\n\\n*How do you install and start iHat?*\\n\\n\\n1- Download iHat from  [Utilities for Oracle Application Server 10_g_ OTN page](http://www.oracle.com/technology/products/ias/utilities/index.html)\\n\\n2- Unzip it, and this becomes the `$IHAT_HOME`\\n\\n3- You have an ORACLE_HOME that is pointing to one of the OracleAS instance, so you can start iHat using the follling command:\\n\\n```\\njava -cp $ORACLE_HOME/opmn/lib/optic.jar:$IHAT_HOME/ihat.jar oracle.ias.opmn.ihat.WebServer 8181 $ORACLE_HOME\\n```\\n\\nUsing this command, iHat is starting an HTTP server on port 8181, and use the OPMN configuration of the `$ORACLE_HOME` that I you have entered as parameter. iHat provided other parameters such as the host-name and OPMN port if you want to connect remotely without dependency on the `$ORACLE_HOME`. All this, is documented in the readme file located in the iHat folder."},{"id":"/2007/02/15/oracle-web-services-sharing-session-between-client-calls","metadata":{"permalink":"/blog/2007/02/15/oracle-web-services-sharing-session-between-client-calls","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-15-oracle-web-services-sharing-session-between-client-calls.md","source":"@site/blog/2007-02-15-oracle-web-services-sharing-session-between-client-calls.md","title":"Oracle Web Services: Sharing Session between client calls","description":"OracleAS Web Services Runtime provides a support for stateful Web Services that is based on HTTP /Servlet session. Some people will probably say that Web Services should not be stateful, or at least not based on the protocol... However, today most of Web Services are using HTTP, and in some specific cases it is very useful to be able to have a state.","date":"2007-02-15T00:00:00.000Z","formattedDate":"February 15, 2007","tags":[],"readingTime":3.04,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Web Services: Sharing Session between client calls","categories":"ws java"},"prevItem":{"title":"Oracle Application Server 10gR3: iHat","permalink":"/blog/2007/02/15/oracle-application-server-10gr3-ihat"},"nextItem":{"title":"New Groovy Plugin for Oracle JDeveloper 10g","permalink":"/blog/2007/02/07/new-groovy-plugin-for-oracle-jdeveloper-10g"}},"content":"OracleAS Web Services Runtime provides a support for stateful Web Services that is based on HTTP /Servlet session. Some people will probably say that Web Services should not be stateful, or at least not based on the protocol... However, today most of Web Services are using HTTP, and in some specific cases it is very useful to be able to have a state.\\n\\nIn this post, I am not explaining how to enable stateful services and clients, since it is documented in the [Java Classes and Stateful Web Wervices](http://download.oracle.com/docs/cd/B31017_01/web.1013/b28974/devjavaclass.htm#BEIDDAFG) chapter of the developer guide. Here I am show you how you can using client side programming share the same state (session) between different web services calls (even different services running in the same server side application).\\n\\nThe tip used here is about the association of cookies to the client instance (JAX-WS Stub or Call object). Here the code you have to write to do that using DII, will be very similar when using static Stub\\n\\n1- Enable the state management\\n\\n``` java\\n...\\nService service = sf.createService(qName);\\nQName port = new QName(\\"CartService\\");\\nCall call = service.createCall(port);\\ncall.setProperty(Stub.SESSION_MAINTAIN_PROPERTY, Boolean.valueOf(true));  // this is necessary to be able to manipulate cookie\\n...\\n\\n```\\n\\n 2- Create a Map that contains the Cookies and assign it to the call (or Stub)\\n\\n``` java\\n\\n...\\n Map cookieMap = new HashMap();\\ncall.setProperty(ClientConstants.COOKIE_MAP, cookieMap);\\n...\\n\\n```\\n\\nThis specific step associates a map that will contains all the cookie with the call/stub instance. You will be able then to manipulate the Map to get or set the cookies.\\n\\n 3- How to get the `JSESSION` cookie\\n\\n``` java\\nprivate Cookie getJSessionCookie(Call call) {\\n  Map cookies = (Map)call.getProperty(ClientConstants.COOKIE_MAP);\\n\\n  if (cookies != null && !cookies.isEmpty()) {\\n    Iterator it =  cookies.values().iterator();\\n    while (it.hasNext()) {\\n      Cookie cookie = (Cookie)it.next();\\n      if (cookie.getName().equals(\\"JSESSIONID\\")) {\\n        return cookie;\\n      }\\n    }\\n  }\\n\\n  return null;\\n}\\n```\\n\\nNote that the Cookie object is an instance of Oracle `HTTPClient.Cookie`.\\n\\n4- Utilizing the Cookie\\n\\nSo now you have all the information to be able to get the Session information when the stateful conversation has started;\\n\\nIn this example each time the call.invoke() is done, a counter is incremented on the server.\\n\\n``` java\\nCall call = service.createCall(port);\\ncall.setProperty(Stub.SESSION_MAINTAIN_PROPERTY, Boolean.valueOf(true));  // this is necessary to be able to manipulate cookie\\nMap cookieMap = new HashMap();\\ncall.setProperty(ClientConstants.COOKIE_MAP, cookieMap);\\n... // The session will only be created after the first invoke\\ncall.invoke(...); // counter = 1 call.invoke(...); // counter = 2 since on the same session\\n\\n... // the session is now created so you can get the cookie\\nCookie mySession = getJSessionCookie(call)\\n...\\n```\\n\\n\\nYou can now use the cookie in another call using the following code:\\n\\n``` java\\nmySession ..  // was extracted from the call #1\\n...// now I am creating a new call instance (myNewCall) that could be in another class\\nCall myNewCall = service.createCall(port);\\nmyNewCall.setProperty(Stub.SESSION_MAINTAIN_PROPERTY, Boolean.valueOf(true));  // this is necessary to be able to manipulate cookie\\nMap cookieMap = new HashMap();\\n// add the cookie to the map this will add the cookie to the HTTP request so it will be associated to the same session (/state)\\ncookieMap.put(mySession,mySession);// associate the cookie Map to the call\\nmyNewCall.setProperty(ClientConstants.COOKIE_MAP, cookieMap);\\n...\\nmyNewCall.invoke;  // counter = 3 since we share the same session\\n...\\n\\n```\\n\\n\\nUsing this sample you have 2 instances of a client calling a service and reusing the same session -state-.  You can also use the same approach to have 2 different clients talking to different services and share the same session. To do that you will have on the server side to use the HTTP Session directly to store your data between calls, and share it between services."},{"id":"/2007/02/07/new-groovy-plugin-for-oracle-jdeveloper-10g","metadata":{"permalink":"/blog/2007/02/07/new-groovy-plugin-for-oracle-jdeveloper-10g","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-02-07-new-groovy-plugin-for-oracle-jdeveloper-10g.md","source":"@site/blog/2007-02-07-new-groovy-plugin-for-oracle-jdeveloper-10g.md","title":"New Groovy Plugin for Oracle JDeveloper 10g","description":"You can find a first release of the Groovy extension for JDeveloper on the Groovy site:","date":"2007-02-07T00:00:00.000Z","formattedDate":"February 7, 2007","tags":[],"readingTime":0.275,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New Groovy Plugin for Oracle JDeveloper 10g","categories":"groovy jdeveloper"},"prevItem":{"title":"Oracle Web Services: Sharing Session between client calls","permalink":"/blog/2007/02/15/oracle-web-services-sharing-session-between-client-calls"},"nextItem":{"title":"Grails 0.4 Released...","permalink":"/blog/2007/01/31/grails-0-dot-4-released-dot-dot-dot"}},"content":"You can find a first release of the Groovy extension for JDeveloper on the Groovy site:\\n\\n* [http://groovy.codehaus.org/Oracle+JDeveloper+Plugin](http://groovy.codehaus.org/Oracle+JDeveloper+Plugin)\\n\\n\\nThe plugin creates a new JDeveloper library for Groovy 1.0, allows you to create and run scripts. I hope to be able to provide more feature such as syntax coloring,  structure recognition, ... lot of ideas here..."},{"id":"/2007/01/31/grails-0-dot-4-released-dot-dot-dot","metadata":{"permalink":"/blog/2007/01/31/grails-0-dot-4-released-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-31-grails-0-dot-4-released-dot-dot-dot.md","source":"@site/blog/2007-01-31-grails-0-dot-4-released-dot-dot-dot.md","title":"Grails 0.4 Released...","description":"The Grails developer team is pleased to announce the release 0.4 of Grails. The release can be","date":"2007-01-31T00:00:00.000Z","formattedDate":"January 31, 2007","tags":[],"readingTime":0.72,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Grails 0.4 Released...","categories":"groovy grails"},"prevItem":{"title":"New Groovy Plugin for Oracle JDeveloper 10g","permalink":"/blog/2007/02/07/new-groovy-plugin-for-oracle-jdeveloper-10g"},"nextItem":{"title":"Oracle JDeveloper (10.1.3.2) / WebCenter available for download","permalink":"/blog/2007/01/30/oracle-jdeveloper-10-dot-1-3-dot-2-slash-webcenter-available-for-download"}},"content":"![](http://grails.codehaus.org/images/grails_logo.jpg)The Grails developer team is pleased to announce the release 0.4 of [Grails](http://grails.codehaus.org/). The release can be\\nobtained from the [downloads page](http://grails.org/Download).\\n\\nNotable improvements include:\\n\\n*   ORM enhancements with support for more relationship types, easy transactions and criteria building, constraints to SQL schema mappings, and upgrade to Hibernate 3.2\\n*   All-new non-invasive plugin system for writing reusable plugins for Grails applications\\n*   Greater Spring integration thanks to a new syntax for scripting Spring ([http://grails.org/Spring+Bean+Builder](http://grails.org/Spring+Bean+Builder)) and an upgrade to Spring 2.0\\n*   Easier unit testing of controllers and custom taglibs\\n*   Validation improvements, including support for inherited constraints and simplified size constraint handling\\n*   Automatic encoding of unsafe HTML characters and URL parameters in all scaffolding and taglibs\\n*   Fixes to support Grails on more containers such as GlassFish and over 200 issues and bugs resolved in JIRA\\n*   Grails now ships with Groovy 1.0!"},{"id":"/2007/01/30/oracle-jdeveloper-10-dot-1-3-dot-2-slash-webcenter-available-for-download","metadata":{"permalink":"/blog/2007/01/30/oracle-jdeveloper-10-dot-1-3-dot-2-slash-webcenter-available-for-download","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-30-oracle-jdeveloper-10-dot-1-3-dot-2-slash-webcenter-available-for-download.md","source":"@site/blog/2007-01-30-oracle-jdeveloper-10-dot-1-3-dot-2-slash-webcenter-available-for-download.md","title":"Oracle JDeveloper (10.1.3.2) / WebCenter available for download","description":"Oracle JDeveloper (10.1.3.2.0) extends the SOA development features from  the previous release by introducing the Oracle WebCenter extension.  Oracle WebCenter Suite combines the standards-based, declarative development of JavaServer Faces (JSF), the flexibility and power of portals, and a set of integrated Web 2.0 services to boost end-user productivity.   Oracle WebCenter Suite provides the tools and services to embed portlets, content, customizable components, Web 2.0 content,","date":"2007-01-30T00:00:00.000Z","formattedDate":"January 30, 2007","tags":[],"readingTime":1.3,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle JDeveloper (10.1.3.2) / WebCenter available for download","categories":"jdeveloper"},"prevItem":{"title":"Grails 0.4 Released...","permalink":"/blog/2007/01/31/grails-0-dot-4-released-dot-dot-dot"},"nextItem":{"title":"JDJ Review: Oracle JDeveloper An IDE Worth a Second Look","permalink":"/blog/2007/01/27/jdj-review-oracle-jdeveloper-an-ide-worth-a-second-look"}},"content":"Oracle JDeveloper (10.1.3.2.0) extends the SOA development features from  the previous release by introducing the Oracle WebCenter extension.  Oracle WebCenter Suite combines the standards-based, declarative development of JavaServer Faces (JSF), the flexibility and power of portals, and a set of integrated Web 2.0 services to boost end-user productivity.   Oracle WebCenter Suite provides the tools and services to embed portlets, content, customizable components, Web 2.0 content,\\ncollaboration and communication services directly into your JavaServer Faces (JSF) application.  These WebCenter features are in addition to the large set of new features that were introduced in Oracle JDeveloper 10.1.3.0 and 10.1.3.1.0.  For more information on WebCenter, visit the\\n[Oracle WebCenter Suite page on OTN](http://www.oracle.com/technology/products/webcenter/index.html).\\n\\n* Build Portlets\\n    * Build JSR 168 portlets\\n    * Build Oracle PDK-Java portlets\\n    * Expose JavaServer Faces (JSF) applications as portlets\\n    * Preconfigured OC4J within Oracle JDeveloper\\n* Consume Portlets\\n    * Consume JSR 168 portlets through WSRP\\n    * Consume other WSRP portlets\\n    * Consume Oracle PDK-Java portlets\\n    * Inter-component communication\\n* Business User Web 2.0 Enterprise Mashup Tools\\n    * Rich Text / Blog Portlet\\n    * WebClipping Portlet\\n    * OmniPortlet\\n* Runtime Customization\\n    * showDetailFrame\\n    * panelCustomizable\\n* Integrate Content using JCR 1.0\\n    * Access content using data controls\\n    * Access to many content repositories\\n* WebCenter Services\\n    * Secure Enterprise Search (SES)\\n    * Oracle Communication and Mobility Server (OCMS)\\n    * Oracle Content Database\\n    * Discussions\\n    * Wiki\\n* Declarative Security\\n    * ADF Security Wizard\\n    * Authorization Editor\\n* Lifecycle Tool\\n    * Embedded Lifecycle Tool\\n    * Command Line Lifecycle Tool\\n    * ANT Tasks\\n\\nSome links:\\n\\n* [New Features](http://www.oracle.com/technology/software/products/jdev/htdocs/soft10132.html)\\n* [Download](http://www.oracle.com/technology/software/products/jdev/htdocs/soft10132.html)"},{"id":"/2007/01/27/jdj-review-oracle-jdeveloper-an-ide-worth-a-second-look","metadata":{"permalink":"/blog/2007/01/27/jdj-review-oracle-jdeveloper-an-ide-worth-a-second-look","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-27-jdj-review-oracle-jdeveloper-an-ide-worth-a-second-look.md","source":"@site/blog/2007-01-27-jdj-review-oracle-jdeveloper-an-ide-worth-a-second-look.md","title":"JDJ Review: Oracle JDeveloper An IDE Worth a Second Look","description":"As the saying goes you never get a second chance at a first impression. In general, that\'s true, but if you\'ve been thoroughly revitalized, matured, and cosmetically re-engineered, shouldn\'t you get a second shot at that first impression? I\'d argue that\'s true of Oracle\'s Java/J2EE Workbench, Oracle JDeveloper.","date":"2007-01-27T00:00:00.000Z","formattedDate":"January 27, 2007","tags":[],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JDJ Review: Oracle JDeveloper An IDE Worth a Second Look","categories":"jdeveloper"},"prevItem":{"title":"Oracle JDeveloper (10.1.3.2) / WebCenter available for download","permalink":"/blog/2007/01/30/oracle-jdeveloper-10-dot-1-3-dot-2-slash-webcenter-available-for-download"},"nextItem":{"title":"Apple released beta version of Dashcode","permalink":"/blog/2007/01/22/apple-released-beta-version-of-dashcode"}},"content":"As the saying goes you never get a second chance at a first impression. In general, that\'s true, but if you\'ve been thoroughly revitalized, matured, and cosmetically re-engineered, shouldn\'t you get a second shot at that first impression? I\'d argue that\'s true of Oracle\'s Java/J2EE Workbench, Oracle JDeveloper.\\n\\n* [Java Product Review \u2014 Oracle JDeveloper An IDE Worth a Second Look](http://java.sys-con.com/read/313602.htm)"},{"id":"/2007/01/22/apple-released-beta-version-of-dashcode","metadata":{"permalink":"/blog/2007/01/22/apple-released-beta-version-of-dashcode","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-22-apple-released-beta-version-of-dashcode.md","source":"@site/blog/2007-01-22-apple-released-beta-version-of-dashcode.md","title":"Apple released beta version of Dashcode","description":"Dashcode is a new application for developing Dashboard widgets coming in Mac OS X 10.5 (Leopard).  This tools is already available as developer preview on Tiger.","date":"2007-01-22T00:00:00.000Z","formattedDate":"January 22, 2007","tags":[],"readingTime":0.4,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Apple released beta version of Dashcode","categories":"apple osx"},"prevItem":{"title":"JDJ Review: Oracle JDeveloper An IDE Worth a Second Look","permalink":"/blog/2007/01/27/jdj-review-oracle-jdeveloper-an-ide-worth-a-second-look"},"nextItem":{"title":"Using Oracle Data Integrator on Mac OS X","permalink":"/blog/2007/01/18/using-oracle-data-integrator-on-mac-os-x"}},"content":"Dashcode is a new application for developing Dashboard widgets coming in Mac OS X 10.5 (Leopard).  This tools is already available as developer preview on Tiger.\\n\\nI have installed it and it is really great, easy to use, helping you to create good looking widgets. As usual, like all the Apple development tools it is really intuitive. More than a long blog entry on this I am just pointing you to the DashCode page on the Apple site:\\n\\n* [Dashcode](http://developer.apple.com/tools/dashcode/)"},{"id":"/2007/01/18/using-oracle-data-integrator-on-mac-os-x","metadata":{"permalink":"/blog/2007/01/18/using-oracle-data-integrator-on-mac-os-x","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-18-using-oracle-data-integrator-on-mac-os-x.md","source":"@site/blog/2007-01-18-using-oracle-data-integrator-on-mac-os-x.md","title":"Using Oracle Data Integrator on Mac OS X","description":"Oracle just released on OTN a new product \\"Oracle Data Integrator\\" (ODI), I wanted","date":"2007-01-18T00:00:00.000Z","formattedDate":"January 18, 2007","tags":[],"readingTime":1.44,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using Oracle Data Integrator on Mac OS X","categories":"apple osx oracle"},"prevItem":{"title":"Apple released beta version of Dashcode","permalink":"/blog/2007/01/22/apple-released-beta-version-of-dashcode"},"nextItem":{"title":"OTN Article: Install Oracle RAC 10g on Oracle Enterprise Linux Using VMware Server","permalink":"/blog/2007/01/15/otn-article-install-oracle-rac-10g-on-oracle-enterprise-linux-using-vmware-server"}},"content":"Oracle just released on OTN a new product \\"[Oracle Data Integrator](http://www.oracle.com/technology/products/oracle-data-integrator/index.html)\\" (ODI), I wanted\\nto quickly take a look to the product, so I have downloaded it and\\ninstalled it on my Mac. This product is a 100% Java based solution that\\nyou can quickly installed on mac following these steps:\\n\\n1- Download and unzip ODI from [OTN download page](http://www.oracle.com/technology/software/products/ias/htdocs/101310.html) (bottom).\\n\\n2- Open a terminal Window and go to the folder where you have unzipped\\nODI, you should have the following content:\\n\\n```\\n- external\\n- index.htm\\n- oracledi\\n- oracledilwd\\n- oracledimn\\n- setup\\n```\\n\\nOpen the index.html and select the Getting Started Guide, this will\\nhelp you to learn more about ODI using a comprehensive scenario.\\n\\n3- Setup the environment variables:\\n\\n```\\nexport ODI_JAVA_HOME=/Library/Java/Home/   (need to be Java 5)\\n\\nexport ODI_HOME=<path to ODI installation folder>/odi/oracled\\n```\\n\\n\\n4- Go to the `$ODI_HOME/bin`\\n\\n```\\ncd $ODI_HOME/bin\\n```\\n\\n5- Start the HSQL databases that contain the sample application and data:\\n\\n```\\n./startdemo.sh &\\n```\\n\\nThis command starts 3 different instances: repo (metadata repository), src (source db), trg (target db) that are used in the Getting Started guide. To stop the DB run the script `./stopdemo.sh`.\\n\\n6- You can now start the designer too using the command:\\n\\n```\\n./designer.sh &\\n```\\n\\nSelect the Getting Started project and when you are in the designer switch to the Mac OS X look and feel ;-), using the Menu \\"Look And Feel &gt; Standard &gt; Mac OS X\\".\\n\\n![]( http://farm1.static.flickr.com/139/361446290_8481172875_o.png )\\n\\nThese are the first steps to start with Oracle Data Integrator, you can now follow the Getting Started Guide, to learm more about the product, and since your environment is set you can run any of the command documented in this guide."},{"id":"/2007/01/15/otn-article-install-oracle-rac-10g-on-oracle-enterprise-linux-using-vmware-server","metadata":{"permalink":"/blog/2007/01/15/otn-article-install-oracle-rac-10g-on-oracle-enterprise-linux-using-vmware-server","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-15-otn-article-install-oracle-rac-10g-on-oracle-enterprise-linux-using-vmware-server.md","source":"@site/blog/2007-01-15-otn-article-install-oracle-rac-10g-on-oracle-enterprise-linux-using-vmware-server.md","title":"OTN Article: Install Oracle RAC 10g on Oracle Enterprise Linux Using VMware Server","description":"Install Oracle RAC 10g on Oracle Enterprise Linux using VMware Server is a very interesting article on using Oracle RAC using Oracle Linux on VMWare.","date":"2007-01-15T00:00:00.000Z","formattedDate":"January 15, 2007","tags":[],"readingTime":0.125,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OTN Article: Install Oracle RAC 10g on Oracle Enterprise Linux Using VMware Server","categories":null},"prevItem":{"title":"Using Oracle Data Integrator on Mac OS X","permalink":"/blog/2007/01/18/using-oracle-data-integrator-on-mac-os-x"},"nextItem":{"title":"Oracle JDeveloper Features Matrix","permalink":"/blog/2007/01/08/oracle-jdeveloper-features-matrix"}},"content":"[Install Oracle RAC 10_g_ on Oracle Enterprise Linux using VMware Server](http://www.oracle.com/technology/pub/articles/chan-ubl-vmware.html) is a very interesting article on using Oracle RAC using Oracle Linux on VMWare."},{"id":"/2007/01/08/oracle-jdeveloper-features-matrix","metadata":{"permalink":"/blog/2007/01/08/oracle-jdeveloper-features-matrix","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-08-oracle-jdeveloper-features-matrix.md","source":"@site/blog/2007-01-08-oracle-jdeveloper-features-matrix.md","title":"Oracle JDeveloper Features Matrix","description":"Not familiar with Oracle JDeveloper?","date":"2007-01-08T00:00:00.000Z","formattedDate":"January 8, 2007","tags":[],"readingTime":0.27,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle JDeveloper Features Matrix","categories":"jdeveloper"},"prevItem":{"title":"OTN Article: Install Oracle RAC 10g on Oracle Enterprise Linux Using VMware Server","permalink":"/blog/2007/01/15/otn-article-install-oracle-rac-10g-on-oracle-enterprise-linux-using-vmware-server"},"nextItem":{"title":"How to use SOAP Compression using JAX-RPC","permalink":"/blog/2007/01/07/how-to-use-soap-compression-using-jax-rpc"}},"content":"Not familiar with Oracle JDeveloper?\\n\\nTake a look to the new Oracle JDeveloper 10g (10.1.3.1) matrix, this document exposes in a simple matrix the different features of the Oracle Java, J2EE, SOA development environment...\\n\\n* [Oracle JDeveloper 10.1.3.1.0 Feature Matrix](http://www.oracle.com/technology/products/jdev/101/collateral/101/featurematrix_10131.html)\\n\\nYou can find all the information and download [this free Java IDE from OTN.](http://otn.oracle.com/products/jdev)"},{"id":"/2007/01/07/how-to-use-soap-compression-using-jax-rpc","metadata":{"permalink":"/blog/2007/01/07/how-to-use-soap-compression-using-jax-rpc","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-07-how-to-use-soap-compression-using-jax-rpc.md","source":"@site/blog/2007-01-07-how-to-use-soap-compression-using-jax-rpc.md","title":"How to use SOAP Compression using JAX-RPC","description":"HTTP compression has improved a lot the download time of content from","date":"2007-01-07T00:00:00.000Z","formattedDate":"January 7, 2007","tags":[],"readingTime":2.575,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to use SOAP Compression using JAX-RPC","categories":null},"prevItem":{"title":"Oracle JDeveloper Features Matrix","permalink":"/blog/2007/01/08/oracle-jdeveloper-features-matrix"},"nextItem":{"title":"Groovy 1.0 released","permalink":"/blog/2007/01/02/groovy-1-dot-0-released"}},"content":"HTTP compression has improved a lot the download time of content from\\nservers. In the context of Web Service it could be very interesting to\\nalso use HTTP compression to improve the network traffic. Firs, I am\\nexplaining how to compress a SOAP response when you have a Web Service\\nrunning in Oracle Containers for J2EE (OC4J) using a generic servlet\\nfilter. I have to give credit to [http://www.thomas-bayer.com/](http://www.thomas-bayer.com/)\\nsince he has created the Filter and documented how to do such thing\\nusing Axis.&nbsp;\\n\\nSo you can take a look to the following article for more details, you\\ncan read the 2 following article, or jump to the next paragraph that\\nexplains how to configure your JAX-RPC based service to send compressed\\nHTTP response.\\n\\n*   [How to Use SOAP Compression with Apache Axis](http://www.thomas-bayer.com/soap-compression-howto.htm)\\n*   [2Way HTTP Compression Servlet Filter](http://www.thomas-bayer.com/gzip-compression-filter.htm)\\n\\nIn this sample I am showing how to compress the SOAP response\\nusing a servlet filter, it is also possible to use some other Oracle\\ninfrastructure element to achieve that such as Oracle HTTP\\nServer/Apache, or Oracle Webcache.\\n\\n#### 1- Install the compression filter library in your application\\n\\nDownload the compression filter library [2wayfilter-1.2.jar](http://www.thomas-bayer.com/resources/gzip-compression-filter/2wayfilter-1.2.jar)\\nand copy it into the Web application\'s` WEB-INF/lib` folder\\n\\n#### 2- Configure your application to use the filter\\n\\nThe configuration of a servlet filter is done using\\nthe web.xml where you reference which servlet or URL will be using the\\nfilter. As you may knowin JAX-RPC, the HTTP endpoint of a service are\\nexposed as servlet and defined in the web.xml. You can choose to\\ncompress all the endpoint/URL or create a new servlet mapping, that\\nwill become a new SOAP endpoint and only compress this one. If you take\\nthe option of creating a new endpoint keep in mind that it will not be\\nadded to the WSDL automatically, so the client application will have to\\npoint explicitly to the compressed endpoint URL to take benefits of it.\\n\\n``` xml\\n<web-app xmlns=\\"http://java.sun.com/xml/ns/j2ee\\" xmlns:xsi=\\"http://www.w3.org/2001/XMLSchema-instance\\"\\n  xsi:schemaLocation=\\"http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd\\"\\n  version=\\"2.4\\">\\n  <servlet>\\n    <description>Web Service CustomerServiceSoapHttpPort</description>\\n    <display-name>Web Service CustomerServiceSoapHttpPort</display-name>\\n    <servlet-name>CustomerServiceSoapHttpPort</servlet-name>\\n    <servlet-class>demo.oracle.CustomerServiceImpl</servlet-class>\\n    <load-on-startup>1</load-on-startup>\\n  </servlet>\\n  <servlet-mapping>\\n    <servlet-name>CustomerServiceSoapHttpPort</servlet-name>\\n    <url-pattern>CustomerServiceSoapHttpPort</url-pattern>\\n  </servlet-mapping>\\n\\n  \x3c!-- New servlet mapping to handle compressed SOAP Messages --\x3e\\n  <servlet-mapping>\\n    <servlet-name>CustomerServiceSoapHttpPort</servlet-name>\\n    <url-pattern>CompressedCustomerServiceSoapHttpPort</url-pattern>\\n  </servlet-mapping>\\n\\n\\n  \x3c!-- Filter definition with mapping on the compressed endpoint --\x3e\\n  <filter>\\n    <filter-name>2WayFilter</filter-name>\\n    <filter-class>com.osmoticweb.gzipfilter.GZIP2WayFilter</filter-class>\\n  </filter>\\n  <filter-mapping>\\n    <filter-name>2WayFilter</filter-name>\\n    <url-pattern>CompressedCustomerServiceSoapHttpPort</url-pattern>\\n  </filter-mapping>\\n\\n</web-app>\\n```\\n\\nYou can now package and deploy your application.\\n\\n#### 3- Create & Invoke the service\\n\\nIn this basic configuration you have only changed the servlet that is\\nthe HTTP endpoint of your service. So the compressed endpoint is not\\npresent in the WSDL, but you can use any of the URL to create your\\nproxy.\\n\\nWhen you have created your proxy, if you want to access the endpoint\\nthat will return the compressed response you must be sure that you are\\ncalling the correct endpoint. You can set the endpoint using the `setEndpoint` method, of your Web\\nService client.\\n\\nThis is it!\\n\\nI will in a next post explain how you can using the Oracle Web Service\\nclient API send a compressed request that will have to be uncompressed\\non the server using the same filter."},{"id":"/2007/01/02/groovy-1-dot-0-released","metadata":{"permalink":"/blog/2007/01/02/groovy-1-dot-0-released","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2007-01-02-groovy-1-dot-0-released.md","source":"@site/blog/2007-01-02-groovy-1-dot-0-released.md","title":"Groovy 1.0 released","description":"Groovy release 1.0 is here now !","date":"2007-01-02T00:00:00.000Z","formattedDate":"January 2, 2007","tags":[],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Groovy 1.0 released","categories":"groovy"},"prevItem":{"title":"How to use SOAP Compression using JAX-RPC","permalink":"/blog/2007/01/07/how-to-use-soap-compression-using-jax-rpc"},"nextItem":{"title":"SOAP Client: A simple Web Services testing tools for Mac","permalink":"/blog/2006/12/26/soap-client-a-simple-web-services-testing-tools-for-mac"}},"content":"Groovy release 1.0 is here now !\\n\\nYou can find the release at the following location:\\n\\n* [http://dist.codehaus.org/groovy/distributions/](http://dist.codehaus.org/groovy/distributions/)\\n\\nCongratulations to all the Groovy developers, and users that have done a great job with this language that is here in production. And it is interesting to see that more and more projects are using Groovy as part of their infrastructure to simplify development:\\n\\n* SOAPUI\\n* XWiki\\n* Spring 2 integration\\n* Grails\\n* ... and many development teams in custom projects\\n\\nSo once again happy new year to all... and enjoy it with Groovy !"},{"id":"/2006/12/26/soap-client-a-simple-web-services-testing-tools-for-mac","metadata":{"permalink":"/blog/2006/12/26/soap-client-a-simple-web-services-testing-tools-for-mac","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-12-26-soap-client-a-simple-web-services-testing-tools-for-mac.md","source":"@site/blog/2006-12-26-soap-client-a-simple-web-services-testing-tools-for-mac.md","title":"SOAP Client: A simple Web Services testing tools for Mac","description":"I was discussing with a friend about SOAP testing tools. We all know SOAP UI that is a very powerful one, but I am also using a very simple one developed on Mac for Mac (cocoa based application), this application is SOAP Client:","date":"2006-12-26T00:00:00.000Z","formattedDate":"December 26, 2006","tags":[],"readingTime":0.25,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"SOAP Client: A simple Web Services testing tools for Mac","categories":null},"prevItem":{"title":"Groovy 1.0 released","permalink":"/blog/2007/01/02/groovy-1-dot-0-released"},"nextItem":{"title":"VMWare finally on Mac (Beta)","permalink":"/blog/2006/12/23/vmware-finally-on-mac-beta"}},"content":"I was discussing with a friend about SOAP testing tools. We all know SOAP UI that is a very powerful one, but I am also using a very simple one developed on Mac for Mac (cocoa based application), this application is SOAP Client:\\n\\n* [SOAP Client Site](http://www.ditchnet.org/soapclient/)\\n\\n![]( http://farm1.static.flickr.com/151/333678730_abc9923589_o.png )"},{"id":"/2006/12/23/vmware-finally-on-mac-beta","metadata":{"permalink":"/blog/2006/12/23/vmware-finally-on-mac-beta","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-12-23-vmware-finally-on-mac-beta.md","source":"@site/blog/2006-12-23-vmware-finally-on-mac-beta.md","title":"VMWare finally on Mac (Beta)","description":"As Mac user I sometimes need to use Windows (too often...) or Linux computer, and for this I have been using either my PC or Parallels. Parallels is great but in my daily job my coworker are mainly using VMWare images....","date":"2006-12-23T00:00:00.000Z","formattedDate":"December 23, 2006","tags":[],"readingTime":0.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"VMWare finally on Mac (Beta)","categories":"apple osx"},"prevItem":{"title":"SOAP Client: A simple Web Services testing tools for Mac","permalink":"/blog/2006/12/26/soap-client-a-simple-web-services-testing-tools-for-mac"},"nextItem":{"title":"A nice christmas present for Groovy and Grails project","permalink":"/blog/2006/12/22/a-nice-christmas-present-for-groovy-and-grails-project"}},"content":"![](http://www.vmware.com/img/logo_top.gif ) As Mac user I sometimes need to use Windows (too often...) or Linux computer, and for this I have been using either my PC or [Parallels](http://www.parallels.com). Parallels is great but in my daily job my coworker are mainly using VMWare images....\\n\\nVMWare has now open the [VMware Virtualization for Mac Beta Program](http://www.vmware.com/products/beta/fusion/). If you like me need virtualization jump on it and give feedback..."},{"id":"/2006/12/22/a-nice-christmas-present-for-groovy-and-grails-project","metadata":{"permalink":"/blog/2006/12/22/a-nice-christmas-present-for-groovy-and-grails-project","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-12-22-a-nice-christmas-present-for-groovy-and-grails-project.md","source":"@site/blog/2006-12-22-a-nice-christmas-present-for-groovy-and-grails-project.md","title":"A nice christmas present for Groovy and Grails project","description":"The groovy project gets funding for its development. Big Sky is hiring [Jochen","date":"2006-12-22T00:00:00.000Z","formattedDate":"December 22, 2006","tags":[],"readingTime":0.71,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"A nice christmas present for Groovy and Grails project","categories":"groovy grails"},"prevItem":{"title":"VMWare finally on Mac (Beta)","permalink":"/blog/2006/12/23/vmware-finally-on-mac-beta"},"nextItem":{"title":"Jesus Rodriguez: Interoperability between OracleAS and between Windows Communication Foundation (WCF)","permalink":"/blog/2006/12/20/jesus-rodriguez-interoperability-between-oracleas-and-between-windows-communication-foundation-wcf"}},"content":"The groovy project gets funding for its development. Big Sky is hiring [Jochen\\nTheodorou](http://blackdragsview.blogspot.com/) one of the Groovy commiter. For the people that do\\nnot know Big Sky. Big Sky is the company behind the the [No Fluff Just Stuff](http://www.nofluffjuststuff.com/index.jsp) symposium tour. Talking about this\\nsymposium, in 2007, Groovy and Grails will have a dedicated track.\\n\\nMore about this funding:\\n\\n* [eWeek news](http://www.eweek.com/article2/0,1895,2074908,00.asp) article\\n* Jay Zimmerman (founder of Big Sky) and Jochen Theodorou\'s [interviews/article](http://www.infoq.com/news/2006/12/groovy-sponsorship) from infoQ.\\n\\nIn addition to this very good news, here some other activities around\\nGroovy and Grails:\\n\\n* Releases of Groovy 1.0 and Grails 0.4\\n* Two books on [Groovy](http://groovy.canoo.com/gina) and one on [Grails](http://www.amazon.com/Definitive-Guide-Grails/dp/1590597583/sr=8-2/qid=1165386946?ie=UTF8&amp;s=books&amp;tag2=abougroo-20).\\n* A dedicated Groovy and Grails website: [aboutGroovy.com](http://www.aboutgroovy.com/)\\n* Also a dedicated Groovy and Grails conference: the[Grails eXchange 2007](http:/http://skillsmatter.com/grailsexchange)\\n* And the third Groovy Developer Conference in Paris at the end of January"},{"id":"/2006/12/20/jesus-rodriguez-interoperability-between-oracleas-and-between-windows-communication-foundation-wcf","metadata":{"permalink":"/blog/2006/12/20/jesus-rodriguez-interoperability-between-oracleas-and-between-windows-communication-foundation-wcf","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-12-20-jesus-rodriguez-interoperability-between-oracleas-and-between-windows-communication-foundation-wcf.md","source":"@site/blog/2006-12-20-jesus-rodriguez-interoperability-between-oracleas-and-between-windows-communication-foundation-wcf.md","title":"Jesus Rodriguez: Interoperability between OracleAS and between Windows Communication Foundation (WCF)","description":"Jesus Rodrigues has published on his blog multiple articles on interoperability between OracleAS and Microsoft  WCF:","date":"2006-12-20T00:00:00.000Z","formattedDate":"December 20, 2006","tags":[],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Jesus Rodriguez: Interoperability between OracleAS and between Windows Communication Foundation (WCF)","categories":"ws"},"prevItem":{"title":"A nice christmas present for Groovy and Grails project","permalink":"/blog/2006/12/22/a-nice-christmas-present-for-groovy-and-grails-project"},"nextItem":{"title":"Web Conference: Groovy &amp; Grails UG London","permalink":"/blog/2006/12/05/web-conference-groovy-and-grails-ug-london"}},"content":"Jesus Rodrigues has published on [his blog](http://weblogs.asp.net/gsusx/default.aspx) multiple articles on interoperability between OracleAS and Microsoft  WCF:\\n\\n**MTOM Interoperability:**\\n\\n *   [MTOM Interoperability between Oracle Application Server and Windows Communication Foundation Part1: From WCF to Oracle](http://weblogs.asp.net/gsusx/archive/2006/12/14/mtom-interoperability-between-oracle-application-server-and-windows-communication-foundation-part1-from-wcf-to-oracle.aspx)\\n *   [MTOM Interoperability between Oracle Application Server and Windows Communication Foundation Part2: From Oracle to WCF](http://weblogs.asp.net/gsusx/archive/2006/12/19/mtom-interoperability-between-oracle-application-server-and-windows-communication-foundation-part2-from-oracle-to-wcf.aspx)\\n\\n**WS-Security Interoperability:**\\n\\n *   [WCF Oracle Application Server WS-Security interoperability Part1: from WCF to Oracle](http://weblogs.asp.net/gsusx/archive/2006/10/20/WCF-Oracle-Application-Server-WS_2D00_Security-interoperability-Part1_3A00_-from-WCF-to-Oracle.aspx)\\n *   [WCF Oracle Application Server WS-Security Interoperability Part 2: from Oracle to WCF](http://weblogs.asp.net/gsusx/archive/2006/11/02/WCF-Oracle-Application-Server-WS_2D00_Security-Interoperability-Part1_3A00_-from-Oracle-to-WCF.aspx)\\n\\n In addition Jesus did also publish previously articles on OracleAS BPEL Process Manager and Microsoft:\\n\\n *   [WS-Addressing interoperability between Oracle BPEL Process Manager and Microsoft Windows Communication Foundation](http://weblogs.asp.net/gsusx/archive/2006/06/01/WS_2D00_Addressing-interoperability-between-Oracle-BPEL-Process-Manager-and-Microsoft-Windows-Communication-Foundation.aspx)"},{"id":"/2006/12/05/web-conference-groovy-and-grails-ug-london","metadata":{"permalink":"/blog/2006/12/05/web-conference-groovy-and-grails-ug-london","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-12-05-web-conference-groovy-and-grails-ug-london.md","source":"@site/blog/2006-12-05-web-conference-groovy-and-grails-ug-london.md","title":"Web Conference: Groovy &amp; Grails UG London","description":"The London Groovy and Grails User Group will be holding their next meeting on Wednesday, 6th December 2006 at Skills Matter in London and for the first time ever the meeting will be available via a live web conference, so don\'t worry if you are not in London!","date":"2006-12-05T00:00:00.000Z","formattedDate":"December 5, 2006","tags":[],"readingTime":0.835,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Web Conference: Groovy &amp; Grails UG London","categories":"groovy grails conference"},"prevItem":{"title":"Jesus Rodriguez: Interoperability between OracleAS and between Windows Communication Foundation (WCF)","permalink":"/blog/2006/12/20/jesus-rodriguez-interoperability-between-oracleas-and-between-windows-communication-foundation-wcf"},"nextItem":{"title":"Calling a Web Services Protected using HTTP Basic","permalink":"/blog/2006/12/04/calling-a-web-services-protected-using-http-basic"}},"content":"The London Groovy and Grails User Group will be holding their next meeting on Wednesday, 6th December 2006 at Skills Matter in London and for the first time ever the meeting will be available via a live web conference, so don\'t worry if you are not in London!\\n\\n\\nSpeaking at this month\'s meeting will be Graeme Rocher, Grails Project Lead and CTO at Skills Matter. During his talk entitled; Grails Dynamic Tags: Making Tag Libraries Agile, Graeme will discuss Groovy Server Pages and its support for the creation of dynamic tag libraries without the need for configuration.\\n\\n\\nJohn Wilson, Groovy Committer, will also be presenting at this meeting. During his talk, entitled; The MetaClass: How Groovy works Under the Hood, John will shed light on the MetaClass so you can better understand its\' function and see how to use it to get your Groovy programs smaller, clearer and faster.![](http://skillsmatter.com/images/misc/groovy-logo.png)\\n\\n\\nFor more information on attending this meeting or signing up for the web conference, please go to: [http://skillsmatter.com/groovy-grails-ug](http://skillsmatter.com/groovy-grails-ug)"},{"id":"/2006/12/04/calling-a-web-services-protected-using-http-basic","metadata":{"permalink":"/blog/2006/12/04/calling-a-web-services-protected-using-http-basic","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-12-04-calling-a-web-services-protected-using-http-basic.md","source":"@site/blog/2006-12-04-calling-a-web-services-protected-using-http-basic.md","title":"Calling a Web Services Protected using HTTP Basic","description":"WS-Security provides a way to protect Web Services at the message level (SOAP) and it is","date":"2006-12-04T00:00:00.000Z","formattedDate":"December 4, 2006","tags":[],"readingTime":0.47,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Calling a Web Services Protected using HTTP Basic","categories":"ws java"},"prevItem":{"title":"Web Conference: Groovy &amp; Grails UG London","permalink":"/blog/2006/12/05/web-conference-groovy-and-grails-ug-london"},"nextItem":{"title":"OC4J: Sending system level message in the console window","permalink":"/blog/2006/11/28/oc4j-sending-system-level-message-in-the-console-window"}},"content":"WS-Security provides a way to protect Web Services at the message level (SOAP) and it is\\nindependent of the protocol used (HTTP, JMS, ...). However, some services are still using HTTP based authentication for protection. JAX-RPC and its Oracle implementation provides a way to set the username\\nand password in the client (Stub) using some properties on the Stub.\\n\\n\\n``` java\\n((Stub)port)._setProperty(Stub.USERNAME_PROPERTY, \\"username\\");\\n((Stub)port)._setProperty(Stub.PASSWORD_PROPERTY, \\"password\\");  \\n```\\n\\nThat\'s it...\\n\\nTheses properties are shortcuts to the standard JAX-RPC properties:\\n\\n``` java\\njavax.xml.rpc.security.auth.username\\njavax.xml.rpc.security.auth.password\\n```\\n\\nThis code is the same when you are using the Call interface."},{"id":"/2006/11/28/oc4j-sending-system-level-message-in-the-console-window","metadata":{"permalink":"/blog/2006/11/28/oc4j-sending-system-level-message-in-the-console-window","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-11-28-oc4j-sending-system-level-message-in-the-console-window.md","source":"@site/blog/2006-11-28-oc4j-sending-system-level-message-in-the-console-window.md","title":"OC4J: Sending system level message in the console window","description":"OracleAS 10gR3, so OC4J standalone, is using the standard Java logging framework. Some of the","date":"2006-11-28T00:00:00.000Z","formattedDate":"November 28, 2006","tags":[],"readingTime":1.93,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OC4J: Sending system level message in the console window","categories":"oc4j oracle"},"prevItem":{"title":"Calling a Web Services Protected using HTTP Basic","permalink":"/blog/2006/12/04/calling-a-web-services-protected-using-http-basic"},"nextItem":{"title":"Download the The Oracle Technology Network (OTN) \'Greatest Hits\'","permalink":"/blog/2006/11/20/download-the-the-oracle-technology-network-otn-greatest-hits"}},"content":"OracleAS 10gR3, so OC4J standalone, is using the standard [Java logging framework](http://java.sun.com/j2se/1.4.2/docs/guide/util/logging/overview.html). Some of the\\nbenefits are easy configuration, and extensibility. The configuration of the level of logging of the different loggers has been exposes in\\nthe Oracle Application Server Console. To see the logger configuration, click on the Administration Tab and then Logger Configuration, you can then configure the different loggers.\\n\\n![]( http://static.flickr.com/106/304730770_8db9728d3a_o.png )\\n\\n\\nBy default the logger will write all the information in the default log.xml file, and for application lever logger it will go in the application.log. You may want to send the information in the console during development to debug/analyze your application. This is done using the configuration of the Handler. This information is currently not available in the Application Server Console, so I am documenting in the next steps how\\nto send the information in the console (terminal window).\\n\\nThe configuration of the OracleAS Logging is saved in the `$ORACLE_HOME/j2ee/home/config/j2ee-logging.xml` file. In this file you  can see that Oracle has defined\\nvarious handlers where information can be sent:\\n\\n* `console-handler` : Log the information in the console (the one we want to use in this sample)\\n* `oc4j-handler` : the default handler for most of the loggers, saving the information in the $ORACLE_HOME/j2ee/home/log/oc4j/log.xml using the Oracle Logger formatting\\n* `oracle-webservices-management-auditing-handler` : the handler used by the Web Services Auditing feature in the $ORACLE_HOME/j2ee/home/log/wsmgmt/auditing/log.xml\\n* `oracle-webservices-management-logging-handler` : the handler used by the Web Service Logging feature in the $ORACLE_HOME/j2ee/home/log/wsmgmt/logging/log.xml\\n\\nAs you may know, OracleAS Web Service provides out of the box support for Auditing of the SOAP messages. You just need to go in the administration page of the Web Service and enable the auditing. By default the messages are logged in the auditing log pointed above. But during development it is really interesting to see the SOAP Messages in the console without having to configure a Proxy to capture the request/response. The easiest way to do that is to edit the `j2ee-logging.xml` file and associate the `console-handler` to the auditing logger using the following configuration:\\n\\n\\n``` xml\\n<logger name=\\"oracle.webservices.management.auditing\\" level=\\"NOTIFICATION:1\\" useParentHandlers=\\"false\\">\\n<handler name=\\"oracle-webservices-management-auditing-handler\\"/>\\n<handler name=\\"console-handler\\"/>\\n</logger>\\n```\\n\\nby doing this you will see the SOAP Message in the OC4J console that is running in your system.\\n\\n![]( http://static.flickr.com/103/308538842_ed574a1c08_o.png )\\n\\nYou can also use this configuration with any loggers available in OC4J."},{"id":"/2006/11/20/download-the-the-oracle-technology-network-otn-greatest-hits","metadata":{"permalink":"/blog/2006/11/20/download-the-the-oracle-technology-network-otn-greatest-hits","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-11-20-download-the-the-oracle-technology-network-otn-greatest-hits.md","source":"@site/blog/2006-11-20-download-the-the-oracle-technology-network-otn-greatest-hits.md","title":"Download the The Oracle Technology Network (OTN) \'Greatest Hits\'","description":"The Oracle Technology Network (OTN) \\"Greatest Hits\\" is a compilation of the most popular technical articles, software downloads, podcasts, sample code, and documentation, we\'ve published in a given 12-month period. The compilation provides you with convenient access to the \\"best\\" of OTN as well as an insight into the interests of the Oracle developer and DBA communities.","date":"2006-11-20T00:00:00.000Z","formattedDate":"November 20, 2006","tags":[],"readingTime":0.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Download the The Oracle Technology Network (OTN) \'Greatest Hits\'","categories":null},"prevItem":{"title":"OC4J: Sending system level message in the console window","permalink":"/blog/2006/11/28/oc4j-sending-system-level-message-in-the-console-window"},"nextItem":{"title":"IBM article:  JAX-RPC vs JAX-WS","permalink":"/blog/2006/11/13/ibm-article-jax-rpc-vs-jax-ws"}},"content":"The Oracle Technology Network (OTN) \\"Greatest Hits\\" is a compilation of the most popular technical articles, software downloads, podcasts, sample code, and documentation, we\'ve published in a given 12-month period. The compilation provides you with convenient access to the \\"best\\" of OTN as well as an insight into the interests of the Oracle developer and DBA communities.\\n\\n* [Oracle Technology Network (OTN) \\"Greatest Hits\\" page](http://www.oracle.com/technology/community/greatest-hits.html)."},{"id":"/2006/11/13/ibm-article-jax-rpc-vs-jax-ws","metadata":{"permalink":"/blog/2006/11/13/ibm-article-jax-rpc-vs-jax-ws","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-11-13-ibm-article-jax-rpc-vs-jax-ws.md","source":"@site/blog/2006-11-13-ibm-article-jax-rpc-vs-jax-ws.md","title":"IBM article:  JAX-RPC vs JAX-WS","description":"I was discussing with a customer not familiar with the JAX-WS standard. I was writing him a mail explaining the difference when I found this nice article on the IBM DeveloperWorks library:","date":"2006-11-13T00:00:00.000Z","formattedDate":"November 13, 2006","tags":[],"readingTime":0.445,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"IBM article:  JAX-RPC vs JAX-WS","categories":"ws"},"prevItem":{"title":"Download the The Oracle Technology Network (OTN) \'Greatest Hits\'","permalink":"/blog/2006/11/20/download-the-the-oracle-technology-network-otn-greatest-hits"},"nextItem":{"title":"JDeveloper: What are my System Properties?","permalink":"/blog/2006/10/13/jdeveloper-what-are-my-system-properties"}},"content":"I was discussing with a customer not familiar with the JAX-WS standard. I was writing him a mail explaining the difference when I found this nice article on the IBM DeveloperWorks library:\\n\\n* [Web services hints and tips: JAX-RPC vs JAX-WS](http://www-128.ibm.com/developerworks/webservices/library/ws-tip-jaxwsrpc.html target=)\\n\\n\\n\\nIt is an opportunity for me to remind OracleAS users that the 10.1.3.1 stack in addition to the JAX-RPC support also provides support for:\\n\\n* Attachments with MTOM, Soap with Attachment and DIME\\n* Annotations based development (JSR-181) that is part of JAX-WS\\n* WS-Security and WS-Reliability"},{"id":"/2006/10/13/jdeveloper-what-are-my-system-properties","metadata":{"permalink":"/blog/2006/10/13/jdeveloper-what-are-my-system-properties","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-10-13-jdeveloper-what-are-my-system-properties.md","source":"@site/blog/2006-10-13-jdeveloper-what-are-my-system-properties.md","title":"JDeveloper: What are my System Properties?","description":"I have been using JDeveloper","date":"2006-10-13T00:00:00.000Z","formattedDate":"October 13, 2006","tags":[],"readingTime":0.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JDeveloper: What are my System Properties?","categories":"jdeveloper"},"prevItem":{"title":"IBM article:  JAX-RPC vs JAX-WS","permalink":"/blog/2006/11/13/ibm-article-jax-rpc-vs-jax-ws"},"nextItem":{"title":"Using HTTPS with Web Services","permalink":"/blog/2006/10/13/using-https-with-web-services"}},"content":"I have been using JDeveloper\\nfor many years, since the first release ;-). But I\'ve never payed\\nattention to a simple and very useful feature. When you click the Help\\n&gt; About menu you can access all the System properties of the\\nJava VM used by Jdeveloper  by clicking on the Properties tab\\n\\n![]( http://static.flickr.com/109/268402753_19bd74c01b_o.png  JDeveloper System Property Viewer )\\n\\nThanks to Gerard for the tip...."},{"id":"/2006/10/13/using-https-with-web-services","metadata":{"permalink":"/blog/2006/10/13/using-https-with-web-services","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-10-13-using-https-with-web-services.md","source":"@site/blog/2006-10-13-using-https-with-web-services.md","title":"Using HTTPS with Web Services","description":"Prerequisites:","date":"2006-10-13T00:00:00.000Z","formattedDate":"October 13, 2006","tags":[],"readingTime":3.695,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using HTTPS with Web Services","categories":"ws"},"prevItem":{"title":"JDeveloper: What are my System Properties?","permalink":"/blog/2006/10/13/jdeveloper-what-are-my-system-properties"},"nextItem":{"title":"Come to Oracle Open World and watch Mr Spring and Mr Apache speak","permalink":"/blog/2006/10/12/come-to-oracle-open-world-and-watch-mr-spring-and-mr-apache-speak"}},"content":"*Prerequisites:*\\n\\nIn this article you have\\n\\n* already a Web Service deployed in OC4J that is running on the default HTTP port. The WSDL and Endpoint are available. In my sample\\nthe non secure Web Service endpoint is: `http://127.0.0.1:8888/math-service/MathServiceSoapHttpPort`\\n\\n#### Add HTTPS to OC4J\\n\\n##### Creating of the Keystore\\n\\nThe first thing to do to secure OC4J would be to create a new keystore\\nthat will contain the different certificates. The easiest way to do\\nthat for a Java developer is to use [SUN\'s keytool](http://java.sun.com/j2se/1.5.0/docs/tooldocs/solaris/keytool.html):\\n\\n```\\nkeytool -genkey -alias oracle-server -dname \\"CN=Tug Grall, OU=Blog O=Grall And Co L=Redwood Shores, S=CA, C=US\\" -keyalg RSA -keypass welcome -storepass welcome -keystore server.keystore\\n```\\n\\nYou can copy the `server.keystore` into the `$ORACLE_HOME/j2ee/home/config` to simplify the next steps.\\n\\n\\n\\n#### Configuring OC4J\\n\\nOC4J stand alone is using the notion of Web-Site to expose HTTP resources (Web Applications). The default-web-site is define is he\\n`$ORACLE_HOME/j2ee/home/config/default-web-site.xml`. To secure an OC4J you can follow the steps describe in the [OC4J Security guide](http://download.oracle.com/docs/cd/B25221_04/web.1013/b14429/configssl.htm) that I have summarized in the following section.\\n\\nWhat we want to achieve for the purpose of the demonstration is to have OC4J using HTTP and HTTPS, on port 8888 and 4443 for example.\\n\\n1. Copy `default-web-site.xml` to `secure-web-site.xml`\\n2. Edit the secure-web-site.xml:\\n    * Change the `web-site` tag by changing the port to `4443` and adding the element `secure=\\"true\\"`\\n    * Add the `ssl-config` element and point this to the new created keystore.\\n\\nThe file looks like:\\n\\n``` xml\\n<web-site   xmlns:xsi=\\"http://www.w3.org/2001/XMLSchema-instance\\"\\n  xsi:noNamespaceSchemaLocation=\\"http://xmlns.oracle.com/oracleas/schema/web-site-10_0.xsd\\"\\n  port=\\"4443\\"\\n  secure=\\"true\\"\\n  display-name=\\"OC4J 10g (10.1.3) Default Web Site\\"\\n  schema-major-version=\\"10\\"\\n  schema-minor-version=\\"0\\" >\\n  ...\\n  <ssl-config keystore=\\"server.keystore\\" keystore-password=\\"welcome\\" />\\n  ...\\n</web-site>\\n```\\n\\n3. Import the new Web site in your OC4J instance by editing the `$ORACLE_HOME/j2ee/home/server.xml`\\nfile. You need to add or replace the web-site tag. In my case I want to\\nadd the secure web site to my instance so the configuration looks like:\\n\\n``` xml\\n...\\n<web-site default=\\"true\\" path=\\"./default-web-site.xml\\" />\\n<web-site path=\\"./secure-web-site.xml\\" />\\n...\\n```\\n\\nSince we have copied the file from the default-web-site, all applications are available using HTTP and HTTPS\\n\\n#### Start OC4J and test the HTTPS port\\n\\nStart OC4J using the standard Java command or shell script, I am adding the Java Network debug flag that would help you to see what is\\nhappening at the SSL level.\\n\\n```\\njava -Djavax.net.debug=ssl -jar oc4j.jar\\n```\\n\\nYou should be able to access the service WSDL using the HTTPS port for example in my case:\\n\\n* `https://127.0.0.1:4443/math-service/MathServiceSoapHttpPort?WSDL`\\n\\n### Consuming the Service using HTTPS\\n\\n#### Generate and configure a client Keystore\\n\\nEvent if this is possible to use the same keystore for the server and the client, I will guide you in the steps to create a client\\ncertificate and import the certificate from the existing -server- one.\\n\\nHere the command *to create* a new keystore:\\n\\n```\\nkeytool -genkey -alias oracle-client -dname \\"CN=John Doe, OU=Blog O=MyDummyClient, S=CA, C=US\\" -keyalg RSA -keypass welcomeClient -storepass welcomeClient -keystore client.keystore\\n```\\n\\nThe next step is *to export* the certificate from the server keystore to be able to import it in the client:\\n\\n```\\nkeytool -keystore server.keystore -export -alias oracle-server -file server.cer\\n```\\n\\nYou can now *import the cerificate* in the client keystore:\\n\\n```\\nkeytool -keystore client.keystore -import -file server.cer\\n```\\n\\n### Generate the proxy\\n\\nYou have now the client certificate so you can use the Oracle Web\\nService Assembler to generate the proxy. The only specific thing you\\nhave to do is to specify which key store to use when running the tool.\\nThe command to use when generating the proxy is:\\n\\n```\\njava -Djavax.net.ssl.trustStore=/Users/tgrall/ssl/client.keystore\\n    -Djavax.net.ssl.keyStore=/Users/tgrall/ssl/client.keystore\\n    -Djavax.net.ssl.trustStorePassword=welcomeClient\\n    -Djavax.net.ssl.keyStorePassword=welcomeClient\\n    -jar $ORACLE_HOME/webservices/lib/wsa.jar\\n    -genProxy\\n    -wsdl https://127.0.0.1:4443/math-service/MathServiceSoapHttpPort?WSDL\\n\\n```\\n\\n### Calling the Service using secure endpoint\\n\\nConfigure the Java Environment to use the client store is made using the following System properties:\\n\\n*   javax.net.ssl.trustStore\\n*   javax.net.ssl.keyStore\\n*   javax.net.ssl.trustStorePassword\\n*   javax.net.ssl.keyStorePassword\\n\\nThis could be done using different approach, property file, -D command\\nline parameter or programmatically. To simply the example I am using\\nthe programmatic approach, the following code is part of the main\\nmethod of the Client class:\\n\\n``` java\\n...\\nSystem.setProperty(\\"javax.net.ssl.trustStore\\", \\"/Users/tgrall/ssl/client.keystore\\");\\nSystem.setProperty(\\"javax.net.ssl.keyStore\\", \\"/Users/tgrall/ssl/client.keystore\\");\\nSystem.setProperty(\\"javax.net.ssl.trustStorePassword\\", \\"welcomeClient\\");\\nSystem.setProperty(\\"javax.net.ssl.keyStorePassword\\", \\"welcomeClient\\");\\n...\\n// Adding Debug information\\nSystem.setProperty(\\"javax.net.debug\\", \\"ssl\\");\\n...\\n```\\n\\nIt is possible to change the Endpoint dynamically in the Proxy using the `setEndpoint` method.\\n\\n``` java\\n...\\ndemoclient.proxy.MathServiceSoapHttpPortClient myPort = new democlient.proxy.MathServiceSoapHttpPortClient();\\n...\\nString ep = \\"https://127.0.0.1:4443/math-service/MathServiceSoapHttpPort\\";\\nmyPort.setEndpoint(ep);\\nSystem.out.println(\\"Result of the operation is \\"+ myPort.add(2,2));\\n...\\n```\\n\\nYou should now be able to run the client and call the service using HTTPS. This would look like:\\n\\n![](http://static.flickr.com/86/268580831_c24ec07d89_o.png)"},{"id":"/2006/10/12/come-to-oracle-open-world-and-watch-mr-spring-and-mr-apache-speak","metadata":{"permalink":"/blog/2006/10/12/come-to-oracle-open-world-and-watch-mr-spring-and-mr-apache-speak","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-10-12-come-to-oracle-open-world-and-watch-mr-spring-and-mr-apache-speak.md","source":"@site/blog/2006-10-12-come-to-oracle-open-world-and-watch-mr-spring-and-mr-apache-speak.md","title":"Come to Oracle Open World and watch Mr Spring and Mr Apache speak","description":"Oracle Open World is getting very close... And I am very excited to go to lot of sessions, two of them looks very interesting in the Oracle Develop track:","date":"2006-10-12T00:00:00.000Z","formattedDate":"October 12, 2006","tags":[],"readingTime":0.78,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Come to Oracle Open World and watch Mr Spring and Mr Apache speak","categories":"conference oow oracle spring"},"prevItem":{"title":"Using HTTPS with Web Services","permalink":"/blog/2006/10/13/using-https-with-web-services"},"nextItem":{"title":"Accessing User Principal in a Web Service","permalink":"/blog/2006/09/21/accessing-user-principal-in-a-web-service"}},"content":"Oracle Open World is getting very close... And I am very excited to go to lot of sessions, two of them looks very interesting in the Oracle Develop track:\\n\\n*   Rod Johnson - Spring Update: What\'s New and Cool in Spring 2.0 (Monday 10/23/2006, 12:45 PM - 1:45 PM in the Hilton Hotel Grand Ballroom A)\\n*   Brian Behlendorf - Bringing Open Source Software Development Practices and Principles Into Your Company (Tuesday 10/24/2006, 1:15 PM - 2:15 PM in the Hilton Hotel Grand Ballroom A)\\n\\nThis is quite exciting to have Open Source gurus coming to present to the Oracle conference, and explain how to use the new Spring in their projects or leverage Open Source practices to improve development in house... Take a look to the full program of Oracle Develop.\\n\\nStart to use the Oracle OpenWorld Schedule Builder to organize your week in SF, if you have not registered yet for OOW click here."},{"id":"/2006/09/21/accessing-user-principal-in-a-web-service","metadata":{"permalink":"/blog/2006/09/21/accessing-user-principal-in-a-web-service","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-09-21-accessing-user-principal-in-a-web-service.md","source":"@site/blog/2006-09-21-accessing-user-principal-in-a-web-service.md","title":"Accessing User Principal in a Web Service","description":"WS-Security provides a standard way to secure Web Services. Since based on SOAP it is agnostic of the stack you are using. When using JAX-RPC implementation, you are running in a J2EE container. In this post I am giving a tip to access the Principal object.","date":"2006-09-21T00:00:00.000Z","formattedDate":"September 21, 2006","tags":[],"readingTime":1.19,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Accessing User Principal in a Web Service","categories":"ws java"},"prevItem":{"title":"Come to Oracle Open World and watch Mr Spring and Mr Apache speak","permalink":"/blog/2006/10/12/come-to-oracle-open-world-and-watch-mr-spring-and-mr-apache-speak"},"nextItem":{"title":"Choose a scripting language? Groovy or JRuby?","permalink":"/blog/2006/09/16/choose-a-scripting-language-groovy-or-jruby"}},"content":"WS-Security provides a standard way to secure Web Services. Since based on SOAP it is agnostic of the stack you are using. When using JAX-RPC implementation, you are running in a J2EE container. In this post I am giving a tip to access the Principal object.\\n\\nI have a service service, and I need to access some user information in its implementation class ( `org.tug.ws.sample.SimpleServiceImpl` ). This service is secure with WS-Security, with for example simple authentication, the following screenshot, is the configuration of inbound security in OracleAS 10gR3:\\n\\n![](http://static.flickr.com/92/248919057_ce2ba6a5f7_o.png Web Services Management )\\n\\nSo the service is secured, here the&nbsp;code that you have to add in your service implementation (or handlers) to access the Principal object.\\n\\n1.  Implement [`javax.xml.rpc.server.ServiceLifecycle`](http://java.sun.com/j2ee/1.4/docs/api/javax/xml/rpc/server/ServiceLifecycle.html)\\n2.  Implement the init(Object context) method to access\\nthe [`ServletEndpointContext`](http://java.sun.com/j2ee/1.4/docs/api/javax/xml/rpc/server/ServletEndpointContext.html),\\nthat you can for example put as a local member of your implementation\\nclass.\\n``` java\\npublic void init(Object context) {\\n  _servleContext = (ServletEndpointContext)context;\\n}\\n```\\n3.  Then you can access the principal object using the `getUserPrincipal()` method:\\n``` java\\n...\\nif (_servleContext.getUserPrincipal() != null ) {\\n  Principal userPrincipal = _servleContext.getUserPrincipal();\\n  ...\\n}\\n...\\n```\\n\\nYou can find more information about the Security in J2EE 1.4 Web Services in the [Designing Web Services with the J2EE 1.4 Platform](http://java.sun.com/blueprints/guidelines/designing_webservices/html/security.html#1045527) tutorial.\\n\\n\\n* * *\\n\\n*Update on Wednesday october 4th:* Frank Nimphius, has use this entry to create a more detail article about [End to End Security with Web Services Security](http://www.orablogs.com/fnimphius/archives/001815.html)."},{"id":"/2006/09/16/choose-a-scripting-language-groovy-or-jruby","metadata":{"permalink":"/blog/2006/09/16/choose-a-scripting-language-groovy-or-jruby","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-09-16-choose-a-scripting-language-groovy-or-jruby.md","source":"@site/blog/2006-09-16-choose-a-scripting-language-groovy-or-jruby.md","title":"Choose a scripting language? Groovy or JRuby?","description":"Last week I discussed dynamic languages with some consultants. This discussion was done in the context of integration of scripting technologies into Java EE environment. So the integration to the VM is important, I also think that the learning curve is a thing to consider.","date":"2006-09-16T00:00:00.000Z","formattedDate":"September 16, 2006","tags":[],"readingTime":0.955,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Choose a scripting language? Groovy or JRuby?","categories":"groovy scripting"},"prevItem":{"title":"Accessing User Principal in a Web Service","permalink":"/blog/2006/09/21/accessing-user-principal-in-a-web-service"},"nextItem":{"title":"More Groovy at Javaone !","permalink":"/blog/2006/05/11/more-groovy-at-javaone"}},"content":"Last week I discussed dynamic languages with some consultants. This discussion was done in the context of integration of scripting technologies into Java EE environment. So the integration to the VM is important, I also think that the learning curve is a thing to consider.\\n\\nIt is true that, like any developer Iike to learn things everyday, this is why I have done some development with PHP, with Ruby On Rails, and obviously with Groovy, Javascript and many other dynamic languages.\\n\\nThe discussion moved quickly to an argument about which language is the best... Hard to say, but I would expect that to be more productive in enterprise it is better to use a \\"Java Like\\" syntax that allows you to leverage the power of scripts. Based on this comment it is for me a no brainer to say that Groovy is more interesting to a core Java developer than JRuby (or other Jython, Jacl, ...). I do not even want to go in the details about VM integration, performances and so on...\\n\\nSo in this context, A. Sundararajan has posted [a very interesting comparison of Java, Groovy and JRuby syntaxes](http://blogs.sun.com/sundararajan/entry/java_groovy_and_j_ruby)."},{"id":"/2006/05/11/more-groovy-at-javaone","metadata":{"permalink":"/blog/2006/05/11/more-groovy-at-javaone","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-05-11-more-groovy-at-javaone.md","source":"@site/blog/2006-05-11-more-groovy-at-javaone.md","title":"More Groovy at Javaone !","description":"If like me you like scripting technologies and in particular Groovy and","date":"2006-05-11T00:00:00.000Z","formattedDate":"May 11, 2006","tags":[],"readingTime":1.725,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"More Groovy at Javaone !","categories":"conference javaone groovy"},"prevItem":{"title":"Choose a scripting language? Groovy or JRuby?","permalink":"/blog/2006/09/16/choose-a-scripting-language-groovy-or-jruby"},"nextItem":{"title":"Grails on Oracle (OracleAS and OracleXE)","permalink":"/blog/2006/05/03/grails-on-oracle-oracleas-and-oraclexe"}},"content":"![]( http://groovy.codehaus.org/images/groovy-logo.png )\\nIf like me you like scripting technologies and in particular Groovy and\\nGrails, JavaOne 2006 will be a very good moment to learn more about it.\\n\\nBeside the official sessions listed below, I would like to inform you\\nof various events interesting for the Groovy community:\\n\\n* Informal **\\"Groovy Community Meeting&rdquo\\"**, **Thursday night 5pm** around the Oracle Demobooth where Guillaume, Graeme and Dierk will give you an\\nopportunity to learn more about these projects\\n* Groovy presentations at the Oracle Booth in the pavilion\\n\\nThe official sessions about Groovy and Grails are\\n\\n<table align=\\"center\\">\\n   <tbody>\\n      <tr align=\\"center\\">\\n         <td align=\\"center\\" nowrap=\\"nowrap\\" width=\\"15%\\">BOF-0554 </td>\\n         <td align=\\"left\\" width=\\"45%\\">Dynamic Scripting Languages BOF </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            &nbsp;Tuesday\\n            05/16/2006\\n            10:30 PM - 11:20 PM\\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            Moscone Center Hall E 133 \\n         </td>\\n      </tr>\\n      <tr align=\\"center\\">\\n         <td align=\\"center\\" nowrap=\\"nowrap\\" width=\\"15%\\">\\n            BOF-2521 \\n         </td>\\n         <td align=\\"left\\" width=\\"45%\\">\\n            Rapid Web Application Development With Grails \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            &nbsp;Thursday\\n            05/18/2006\\n            08:30 PM - 09:20 PM \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n          Moscone Center Esplanade 307-310 \\n         </td>\\n      </tr>\\n      <tr align=\\"center\\">\\n         <td align=\\"center\\" nowrap=\\"nowrap\\" width=\\"15%\\">\\n            TS-1246 \\n         </td>\\n         <td align=\\"left\\" width=\\"45%\\">\\n            Simplify\\n            Enterprise Development With Scripting \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            &nbsp;Thursday\\n            05/18/2006\\n            11:00 AM - 12:00 PM \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n          Moscone Center\\n            Hall E 134 \\n         </td>\\n      </tr>\\n      <tr align=\\"center\\">\\n         <td align=\\"center\\" nowrap=\\"nowrap\\" width=\\"15%\\">\\n            TS-3273 \\n         </td>\\n         <td align=\\"left\\" width=\\"45%\\">\\n            Groovy\\n            = Java&trade; Technology+ Ruby + Python for the JVM&trade;\\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            &nbsp;Wednesday\\n            05/17/2006\\n            02:45 PM - 03:45 PM \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\"> \\n         Moscone Center\\n            Gateway 104 \\n         </td>\\n      </tr>\\n      <tr align=\\"center\\">\\n         <td align=\\"center\\" nowrap=\\"nowrap\\" width=\\"15%\\">\\n            TS-3714 \\n         </td>\\n         <td align=\\"left\\" width=\\"45%\\">\\n            Flash-Gridding\\n            with Java&trade; Technology: Using Project Glassfish<small><sup>SM</sup></small>,\\n            Jini&trade;/JavaSpaces&trade;, and Groovy as an Environment for\\n            an Open Source, Self-Assembling Supercomputer \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            &nbsp;Thursday\\n            05/18/2006\\n            02:45 PM - 03:45 PM \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n          Moscone Center Gateway 104 \\n         </td>\\n      </tr>\\n      <tr align=\\"center\\">\\n         <td align=\\"center\\" nowrap=\\"nowrap\\" width=\\"15%\\">\\n            TS-5386 \\n         </td>\\n         <td align=\\"left\\" width=\\"45%\\">\\n            Groovy\\n            Goes RFID with Smart Sensors for Real-World Control \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n            &nbsp;Tuesday\\n            05/16/2006\\n            05:45 PM - 06:45 PM \\n         </td>\\n         <td align=\\"center\\" width=\\"15%\\">\\n          Moscone Center Gateway 104 \\n         </td>\\n      </tr>\\n   </tbody>\\n</table>"},{"id":"/2006/05/03/grails-on-oracle-oracleas-and-oraclexe","metadata":{"permalink":"/blog/2006/05/03/grails-on-oracle-oracleas-and-oraclexe","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-05-03-grails-on-oracle-oracleas-and-oraclexe.md","source":"@site/blog/2006-05-03-grails-on-oracle-oracleas-and-oraclexe.md","title":"Grails on Oracle (OracleAS and OracleXE)","description":"OTN (Oracle Technology Network)","date":"2006-05-03T00:00:00.000Z","formattedDate":"May 3, 2006","tags":[],"readingTime":9.105,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Grails on Oracle (OracleAS and OracleXE)","categories":"grails groovy oracle"},"prevItem":{"title":"More Groovy at Javaone !","permalink":"/blog/2006/05/11/more-groovy-at-javaone"},"nextItem":{"title":"Oracle BPEL: Debugging \'internal\' SOAP Messages","permalink":"/blog/2006/04/23/oracle-bpel-debugging-internal-soap-messages"}},"content":"OTN (Oracle Technology Network)\\nReaders have noticed, that Oracle has\\npublished an article from [Richard Monson-Haefel](http://www.monson-haefel.com/) about [Ruby On Rails on Oracle](http://www.oracle.com/technology/pub/articles/haefel-oracle-ruby.html). This\\narticle introduces the Ruby On Rails framework and explains how to use\\nit to access an Oracle database. ([Oracle 10g Express Edition](http://www.oracle.com/technology/products/database/xe/index.html) to be exact)\\n\\nIf you are not familiar at all with Ruby On Rails, it is important to\\nnotice that it has nothing to do with Java, J2EE. It is a [Ruby](http://www.ruby-lang.org/)\\nbased framework. &nbsp;So yes Ruby On Rails is really interesting,\\npowerful and so on... but for me as a Java developer I would like to do\\nthe same using Java (or equivalent) leveraging the investment that I\\nhave done in J2EE; also important I want to be able to deploy and\\nmanage applications that are developed this way using my tools such as\\nOracle Enterprise Manager Application Server Control.\\n\\nThe paradigm \\"coding by convention\\" that is the driver of Ruby on Rails\\nhas been leveraged to developed a new framework: GRAILS. Grails uses\\nGroovy as the underlying language, so it runs on a JVM and can leverage\\nany existing Java API.\\n\\nIf you are a Java developer you will find very interesting to use this\\nframework to accelerate the development of Web applications. If you are\\nnot yet a Java developer but need to develop Web application faster,\\nand deploy the to your J2EE application server, Grails is also a very\\ngood tools.\\n\\nSince I have started with Richard\'s article I will use the same\\napplication/database schema to develop my first GRAILS application, and\\nalso use the same structure in my article...(*is it what we call lazy loading?*)\\n\\n### What is Groovy? What is Grails?\\n\\nGroovy is a dynamic language that leverage features from other\\nlanguages such as Ruby, Jython, and Smalltalk. Groovy is running at the\\ntop of a Java VM and makes available any existing Java objects (so all\\n  the API) to Groovy. Groovy is currently under standardization with the [JSR-24](http://www.jcp.org/en/jsr/detail?id=241)[1](http://www.jcp.org/en/jsr/detail?id=241).\\n  &nbsp;You can learn more about Groovy on the [Groovy site](http://groovy.codehaus.org/) and is [project leader\'s (Guillaume Laforge) blog](http://glaforge.free.fr/weblog/?catid=2).\\n\\nGRAILS is to Groovy what Ruby On Rails is to Ruby. Originally named \\"*Groovy On Rails*\\",\\n  this name has been dropped in favor of Grails to avoid\\n  confusion/competition. Like Ruby on Rails, Grails is designed to\\n  create [CRUD](http://en.wikipedia.org/wiki/CRUD_%28acronym%29)\\n  (*Create Read Update Delete*) Web applications. &nbsp;You can learn more\\n  about Grail on the [Grails site](http://grails.codehaus.org/) and is [project leader\'s (Graeme Rocher) blog](http://graemerocher.blogspot.com/).\\n\\nLet\'s now dive in the sample application, for this, as stated earlier I\\n  am using the sample application described in the OTN articles.\\n\\n### Example: The product Catalog\\n\\n#### Step 1: Set up the Oracle database\\n\\nIf you have not set up the schema and table from the article you just\\n  need to create the following objects:\\n\\n```\\nCREATE TABLE comics (\\n    id NUMBER(10) NOT NULL,\\n    title VARCHAR2(60),\\n    issue NUMBER(4),\\n    publisher VARCHAR2(60),\\n    PRIMARY KEY (id)\\n    );\\n    CREATE SEQUENCE comics_seq;\\n```  \\n\\nBased on the OTN article I have created this table in the <span class=\\"code\\">ruby</span> schema.\\n\\n#### Step 2: Install Grails\\n\\nGrails installation is straight forward and explained in the [Installation guide](http://grails.codehaus.org/Installation), basically:\\n\\n1. Download the binaries (I used [Grails 0.2](http://build.canoo.com/grails/artifacts//grails-bin-0.2-SNAPSHOT.zip))\\n2. Setup the different environment variable (<span class=\\"code\\">GRAILS_HOME, JAVA_HOME, PATH</span>), I used Java 5.\\n\\nYou are done !\\n\\n#### Step 3: create the Web Application\\n\\nNow we have installed the product, the next step is to create the application itself.\\n\\n**Create the application**\\n\\nThe <span class=\\"code\\">create-app</span> command is\\n  creating the full project, with the template\\n  with placeholder for the different components of your application such\\n  as configuration, MVC, and library and much more. To do it enter the\\n  following command, in your command line interface:\\n\\n```\\n> grails create-app\\n....\\n.....\\ncreate-app:\\n[input] Enter application name:\\ncomics_catalog\\n.....\\n```\\n\\nAs you will see, Grails uses Ant intensively, the <span class=\\"code\\">create-app</span>\\n  command will ask you for an application name, enter for example <span class=\\"code\\">comics_catalog</span>.\\n\\nThe created application contains now a list of directory allowing\\n  developer to start to build the application using Groovy, Grails and\\n  any Web components.\\n\\n***Add the Business Logic and Model:Domain Classes***\\n\\nOne of the biggest differences between Grails and RoR, is the fact that\\n  the main components of your application development is not the Table\\n  like you have in RubyOnRails but the \\"Domain Class\\". &nbsp;The\\n  domain class are the core of the business application, they contains\\n  the state and the behavior of your application.\\n\\nSo the next step is to create a Domain Class for the Comics, to do that\\n  you just need to go in the home directory of your project, eg cd <span class=\\"code\\">comics_catalog</span> and run the <span class=\\"code\\">create-domain-class</span>.\\n\\n```\\n> cd comics_catalog\\n> grails create-domain-class\\n....\\ncreate-domain-class:\\n[input] Enter domain class name:\\ncomics\\n....\\n```\\n\\nWhen the command ask you to enter the class name, enter <span class=\\"code\\">comics</span>. Grails, will not use the\\n  same naming convention that RoR has, so you need to use the same name\\n  for the class and the table you want to map your object on. The\\n  persistence layer is made using GROM (Grails Object Relational Mapping)\\n  that leverage hibernate.\\n\\nNote: In our case what we are doing is to leverage an existing database\\n  object and create the domain class at the top of it. Usually, Grails\\n  uses a different approach where everything is driven by the\\n  application, so you create the domain class first and then Grails will\\n  create the different database objects.\\n\\nThe Comics class does not have any information related to the mapping\\n  itself, so you have to create the different attributes in the domain\\n  class. This is where you you start to use Groovy, the domain class is\\n  located in the following location:\\n\\n* `./comics_catalog/grails-app/domain/Comics.groovy`\\n\\nNote hat by default Grails create the class with 2 attributes: id and\\n  version, keep them in place, and add title, issue and\\n  publisher.&nbsp;\\n\\n``` java\\nclass Comics {\\n  @Property Long id\\n  @Property Long version\\n\\n  // new properties for the Comics class\\n  @Property String title\\n  @Property Long issue\\n  @Property String publisher\\n\\n  String toString() { \\"${this.class.name} :  $id\\" }\\n}\\n```\\n\\n\\nWe are all set, we are ready to run the magic command that will create the different screens and flow.\\n\\n**Create the different screens from the domain class**\\n\\nYou can now run the `generate-all` command to create all the different screens.\\n\\n```\\n> grails generate-all\\n....\\ninput-domain-class:\\n[input] Enter domain class name:\\ncomics\\n....\\n```\\n\\nThis command creates the different Views and Controllers, you can take a look to the directories:\\n\\n* `./comics_catalog/grails-app/controllers`\\n* `./comics_catalog/grails-app/views`\\n\\n**Configure the database access**\\n\\nWhat we have to do is now to configure the application to use the Oracle database and schema.\\n\\nGrails uses a configuration file for data source:\\n\\n* `./comics_catalog/grails-app/conf/ApplicationDataSource.groovy`\\n\\nLet\'s edit this file to connect to our Oracle database.\\n\\n``` java\\n\\nclass ApplicationDataSource {\\n  @Property boolean pooled = true\\n  @Property String dbCreate = \\"update\\" // one of \'create\', \'create-drop\',\'update\'\\n  @Property String url = \\"jdbc:oracle:thin:@localhost:1521:XE\\"\\n  @Property String driverClassName = \\"oracle.jdbc.OracleDriver\\"\\n  @Property String username = \\"ruby\\"\\n  @Property String password = \\"ruby\\"\\n}\\n\\n```\\n\\n\\n\\nNothing special concerning the properties such as URL, DriverClassName,\\n  username and password.\\n\\nThe one that is interesting is the <span class=\\"code\\">dbCreate</span>,\\n  that allows you to configure the behavior on the schema to create or\\n  not objects.In our sample the table exists, so we want to reuse the\\n  object, but we want to be sure that we have all the mandatory objects,\\n  columns too, so I selected <span class=\\"code\\">update</span>.\\n\\nThe next thing to do is to add the Oracle JDBC driver to the\\n  application, to make it available. To make it available you just need\\n  to copy the JDBC driver into the lib directory of your application. In\\n  my case I am using Oracle XE so I copy the file from the following\\n  location:\\n\\n* `ORACLE_XE_HOME/app/oracle/product/10.2.0/server/jdbc/lib/ojdbc14.jar`\\nto\\n* `./comics_catalog/lib/`\\n\\n#### Step 4: Run the application\\n\\nGrails provide a way to run the application in stand alone mode, the command is <span class=\\"code\\">run-app</span>. This command starts an Web container (based on Jetty) with the application&nbsp;deployed.\\n\\n`grails run-app`\\n\\nNote: Jetty will start on port 8080, in order to start in on a different port like e.g. 9090 use:\\n\\n`grails -Dserver.port=9090 run-app`\\n\\nYou can now access the application using the following URL:\\n\\n`http://localhost:8080/comics_catalog/comics/`\\n\\nYour browser should show the list of comics from the Comics table.\\n\\n![](http://static.flickr.com/45/127519247_646a991d3a_o.png )\\n\\n*List of Comics*\\n\\nYou can create a new entry by clicking on the \\"New Comics\\" tab, and view/edit/delete existing record by clicking on the \\"Show\\" link.\\n\\n![](http://static.flickr.com/56/127519249_0e580ede98_o.png )\\n\\n*Edit/Create entry*\\n\\nAs you see the creation of an application is really easy. The next step is to deploy the application in your application server.\\n\\n#### Step 5: Deploy the application\\n\\nGrails provides a command to package the application as a WAR ready to\\n  be deployed, so in the root directory of your project you can run the\\n  following command:\\n\\n```\\ngrails war\\n```\\n\\nWhen you run this command you end with a WAR with the name of your\\n  application located in the root of your project, in our case: `comics_catalog.war`\\n\\nIf you take a look to this WAR you\'ll see that it is quite big ~10Mb,\\n  this is because all the libraries are included in the Lib directory of\\n  the web application. You can see the exact structure of the WAR in the\\n  ./tmp (`./comics_catalog/tmp/war`) directory of the application.\\n\\nYou can deploy the application as it is to Oracle Application Server 10_g_,\\n  but to avoid the issue with the class loader you should configure the\\n  Web application to load the local classes first. It can be done during\\n  deployment with the class loader configuration screen:\\n\\n![](http://static.flickr.com/52/127519251_8a522dac17_o.png )\\n\\nYou can also save this configuration in a deployment plan to facilitate later deployment.\\n\\nWhen the deployment is done you can access the application using the OracleAS host and port, something like:\\n\\n`http://localhost:8888/comics_catalog/comics/list`\\n\\nYou can now administer and monitor the application like any other J2EE application deployed in OracleAS 10g.\\n\\n*Better Deployment Options*\\n\\n* I personally do not like the idea of shipping all the Jar\\n  files in the WAR file, so instead you can use the OracleAS Shared\\n  Libraries to create a Grails library by uploading and configuring all\\n  the Jars. And package the War without all these libraries.\\n* Also you should be able to configure Hibernate/Spring to\\n  use a standard define Data source and use the JNDI name to lookup the\\n  connections.\\n\\n### Conclusion\\n\\nGRAILS like Ruby On Rails are really interesting frameworks allowing\\n  developers to create quickly Web application that access relational\\n  database and especially the Oracle Database.\\n\\nGrails is quite new (release 0.2), but the documentation is really nice\\n  and complete. I will encourage all developers that are interested by\\n  such framework to use it and provide feedback to the development team.\\n\\nI will try provide other post about deployment of Grails on OracleAS,\\n  but also related to other interesting features of this framework, for\\n  example Ajax support, Validations etc etc.\\n\\n### Resources\\n\\n* [Grails](http://grails.codehaus.org/)\\n* [Groovy](http://groovy.codehaus.org/)\\n* [Ruby on Rails](http://rubyonrails.org/)"},{"id":"/2006/04/23/oracle-bpel-debugging-internal-soap-messages","metadata":{"permalink":"/blog/2006/04/23/oracle-bpel-debugging-internal-soap-messages","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-04-23-oracle-bpel-debugging-internal-soap-messages.md","source":"@site/blog/2006-04-23-oracle-bpel-debugging-internal-soap-messages.md","title":"Oracle BPEL: Debugging \'internal\' SOAP Messages","description":"When you develop applications","date":"2006-04-23T00:00:00.000Z","formattedDate":"April 23, 2006","tags":[],"readingTime":3.975,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle BPEL: Debugging \'internal\' SOAP Messages","categories":"bpel soa"},"prevItem":{"title":"Grails on Oracle (OracleAS and OracleXE)","permalink":"/blog/2006/05/03/grails-on-oracle-oracleas-and-oraclexe"},"nextItem":{"title":"Google Data APIs","permalink":"/blog/2006/03/23/google-data-apis"}},"content":"When you develop applications\\nthat use SOAP based Web Services you very often use an HTTP proxy to\\ncapture the request and response that are exchanged between the clients\\nand servers. For this you can use the Oracle HTTP Analyzer that is part\\nof the toolset of Oracle JDeveloper, Axis TCP Monitor, or a packaged\\nversion of it that you have with Oracle BPEL Process Manager.\\n\\nBPELs are making extensive usage of SOAP messages, and it could be\\ninteresting to debug the different call to the partnerlinks. Oracle\\nBPEL PM, to avoid HTTP calls and make optimized SOAP message when the\\npartnerlink that you are invoking is deployed as a BPEL. So in the\\ndefault configuration you do not see the different calls. In this\\narticle I explain how you can configure the server to be able to do it.\\n\\nFor this I will be using:\\n\\n* Oracle BPEL Process Manager developer install running in an OC4J 10g Stand Alone (10.1.2.0.2)\\n* obtunnel, that is a package version of Axis TCP Monitor located in `&lt;BPEL_HOME&gt;\\\\bin\\\\obtunnel.bat`\\n* LoanFlow demo that you can install in 2 steps:\\n\\n#### Starting the Oralce BPEL Tunneling tool:\\n\\nJust run the command `&lt;BPEL_HOME&gt;\\\\bin\\\\obtunnel.bat` You will see the following application:\\n\\n![](http://static.flickr.com/46/133968437_aa0d5804f6_o.png )\\n\\n\\nBy default the TCPMonitor launched from BPEL listens on the port 1234\\nand proxies for the default Oracle BPEL port 9700. So in this context\\nyou will capture all the requests is you access the server on the port\\n1234.\\n\\nIt is not sufficiant here since the different partnerlinks endpoint are\\nnot dynamique and are set to the port 9700. So in this case you wont\'s\\nsee the request coming from the BPEL to a local partner link (and I am\\n  not talking about the SOAP Optimization yet).\\n\\nOne way that I use to work around this issue in development _--maybe we have more simple\\n  solutions, but this is the one that I use--_ is to change\\n  the port of the OC4J and make the proxy listening on the port 9700. In\\n  this case you will be able to capture the requests made from BPEL to\\n  its partnerlinks.\\n\\n#### Changing the Port of OC4J and the TCP Monitor\\n\\n*I. Change the HTTP port of OC4J used by BPEL*\\n\\n1.  Open `&lt;BPEL_HOME&gt;\\\\system\\\\appserver\\\\oc4j\\\\j2ee\\\\home\\\\config\\\\http-web-site.xml`\\n2.  Edit the `port` attribute of the root element `web-site` to enter a different value\\n\\neg: `&lt;web-site port=\\"9701\\"...`.  Stop your BPEL Process Manager\\n\\n*II. Start a new TCPMonitor on port 9700*\\n\\n1.  In the TCPMonitor sceen click on the Admin Tab\\n2.  Enter 9700 for the \\"Listen Port #\\" field (since we want to be sure the partnerlinks are called correctly)\\n3.  Enter 9701 (or the value you entered for the HTTP port) for \\"Target Port #\\".\\n4.  Click Add\\n5.  Click on the new tab \\"Port 9700\\". If you have an error message like \\"*java.net.BindException: Address Already in use: JVM_Bind*\\" this is simply because your BPEL process manager is not stopped. In this case stop the BPEL server, and start the TCPMonitor by clicking the Start button.\\n\\n\\n*III. Restart you BPEL Server*\\n\\nNothing special here you just need to start your server, and check that the BPEL PM is now listening on the HTTP port that you have entered, in my case 9701:\\n\\n* `http://localhost:9701/BPELConsole`\\n\\nYou can now go on the test page of the LoanFlow process (either on the port 9700 or 9701) and invoke the process. I am using 9701 since I want to capture the calls make by the Business Process to its partnerlinks.\\n\\nYou can see some HTTP activities in your TCPMonitor, but if you look in\\n        details you only see request to the different WSDLs used by the\\n        LoanFlow...\\n\\nI was like you expecting to be able to see the different SOAP requests\\n        and response, but BPEL does some optimization around local SOAP calls.\\n        So to be able to capture these requests you just need to turn of this\\n        optimization.\\n\\n#### Turning Off the SOAP Shortcut\\n\\n1.  In the BPEL console, click on the \\"*Manage BPEL Domain*\\" link (topright)\\n2.  You arrive in the configuration tab, look for the `optSoapShortcut` property and set it to `false`.\\n3.  Click `Apply` You can now go on the test page of the LoanFlow process (either on the port 9700 or 9701) and invoke the process. I am using 9701 since I want to capture the calls make by the Business Process to its partnerlinks.\\n\\nNow you can see all the SOAP requests and responses between the LoanFlow BPEL and its partnerlinks.\\n\\n![](http://static.flickr.com/46/133968438_d0dc761af0.jpg?v=0 )\\n\\n\\n![](http://static.flickr.com/46/133968438_d0dc761af0_o.png )\\n\\n\\n*Update on 08/02/2007*\\n\\nFor people that are currently using **Oracle BPEL 10.1.3.1** the *optSoapShortcut* is not visible anymore in the console, but it is still possible to configure this by adding it manually in the domain configuration file available at:\\n\\n`$BPEL_HOME/domains/<domain>/config/domain.xml`"},{"id":"/2006/03/23/google-data-apis","metadata":{"permalink":"/blog/2006/03/23/google-data-apis","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-03-23-google-data-apis.md","source":"@site/blog/2006-03-23-google-data-apis.md","title":"Google Data APIs","description":"Google has published a new set of API (and format) to update and access data. It looks really interesting. These services are published using the REST paradigm and XML.","date":"2006-03-23T00:00:00.000Z","formattedDate":"March 23, 2006","tags":[],"readingTime":0.865,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Google Data APIs","categories":null},"prevItem":{"title":"Oracle BPEL: Debugging \'internal\' SOAP Messages","permalink":"/blog/2006/04/23/oracle-bpel-debugging-internal-soap-messages"},"nextItem":{"title":"Oracle Industrial Telnet Server (ITS): The OracleAS Hidden Treasure..","permalink":"/blog/2006/03/13/oracle-industrial-telnet-server-its-the-oracleas-hidden-treasure-dot"}},"content":"![](http://www.google.com/logos/Logo*25wht.gif )[Google has published a new set of API](http://code.google.com/apis/gdata/overview.html) (and format) to update and access data. It looks really interesting. These services are published using the [REST](http://en.wikipedia.org/wiki/Representational*State*Transfer) paradigm and XML.\\n\\nBased on this protocol Google is now exposing its [Calendar](http://code.google.com/apis/gdata/calendar.html). I am currently playing around with it, really fun. A good opportunity for portlets and widgets developments...\\n\\n*Note for Oracle JDeveloper 10g (10.1.3) users:* I have define the GData and Calendar API as new library inside Oracle JDeveloper. But I had some issue compiling when I start to use any of the class coming from these library with the default configuration on Windows XP. I just switch from Oracle JVM to the standard JDK compiler in my project to avoid this \\"*Error: Internal compilation error, terminated with a fatal exception*\\".  To do this it is quite simple. Double click on your project, choose compiler in the left tree, and check the \\"Use Javac\\" option to force JDeveloper to use the standard javac command. *(I will have to find where this issue come from...)*"},{"id":"/2006/03/13/oracle-industrial-telnet-server-its-the-oracleas-hidden-treasure-dot","metadata":{"permalink":"/blog/2006/03/13/oracle-industrial-telnet-server-its-the-oracleas-hidden-treasure-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-03-13-oracle-industrial-telnet-server-its-the-oracleas-hidden-treasure-dot.md","source":"@site/blog/2006-03-13-oracle-industrial-telnet-server-its-the-oracleas-hidden-treasure-dot.md","title":"Oracle Industrial Telnet Server (ITS): The OracleAS Hidden Treasure..","description":"When I was working in Oracle Consulting I was surprised to see how many customers are using character mode applications, base on Oracle Forms. Lot of applications in wharehouses, harbour, ... are using telnet terminal, usually remote/mobile using RF networks.","date":"2006-03-13T00:00:00.000Z","formattedDate":"March 13, 2006","tags":[],"readingTime":0.92,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Industrial Telnet Server (ITS): The OracleAS Hidden Treasure..","categories":"oracle jdeveloper"},"prevItem":{"title":"Google Data APIs","permalink":"/blog/2006/03/23/google-data-apis"},"nextItem":{"title":"Oracle Fusion Middleware and Microsoft Interoperability - Developer\'s Guide","permalink":"/blog/2006/03/02/oracle-fusion-middleware-and-microsoft-interoperability-developers-guide"}},"content":"When I was working in Oracle Consulting I was surprised to see how many customers are using character mode applications, base on Oracle Forms. Lot of applications in wharehouses, harbour, ... are using telnet terminal, usually remote/mobile using RF networks.\\n\\nMoving to Java on the server was very hard for them because of the lack of support for easy character mode development based solutions.\\n\\nOracleAS 10_g_/ADFprovides such support with the _Industrial Telnet Server (ITS)_.  ITS is the telnet server running in a J2EE container as a J2CA adaptor, and uses JavaServer Faces to render the user interface. The advantage of using JSF for the UI, it allows developer to leverage automatically different renderers (HTML, Mobile and telnet) without changing the application.\\n\\nHere an example of the different renderer provided by Oracle ADF Faces (Instant Messaging, PDA, HTML and Telnet)\\n[![jsf-renderer.PNG](http://farm1.static.flickr.com/172/388762316_44c01f4260_o.png)](http://farm1.static.flickr.com/172/388762316_44c01f4260_o.png)\\n\\nIf you are looking for more information around Oracle ITS:\\n\\n*   [Oralce ADF Mobile](http://www.oracle.com/technology/products/iaswe/adfmb.html)\\n*   [How To Install, Configure, and Manage the Industrial Telnet Server](http://www.oracle.com/technology/products/jdev/101/howtos/telnet/index.html)\\n*   [How To Install the Industrial Telnet Server on Third-Party Containers](http://www.oracle.com/technology/products/iaswe/adfmb/mbl_pits3pinstall.html)\\n*   [Demonstration: Using JDeveloper to create Telnet Applications](http://www.oracle.com/technology/products/iaswe/adfmb/tiki-download_file.php152_telnet.swf)(flash)"},{"id":"/2006/03/02/oracle-fusion-middleware-and-microsoft-interoperability-developers-guide","metadata":{"permalink":"/blog/2006/03/02/oracle-fusion-middleware-and-microsoft-interoperability-developers-guide","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-03-02-oracle-fusion-middleware-and-microsoft-interoperability-developers-guide.md","source":"@site/blog/2006-03-02-oracle-fusion-middleware-and-microsoft-interoperability-developers-guide.md","title":"Oracle Fusion Middleware and Microsoft Interoperability - Developer\'s Guide","description":"Download the Beta version of the Oracle Application Server Developer\'s Guide for Microsoft Office Interoperability along with sample code (and other technical resources) from this new OTN page.","date":"2006-03-02T00:00:00.000Z","formattedDate":"March 2, 2006","tags":[],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Fusion Middleware and Microsoft Interoperability - Developer\'s Guide","categories":"oracle ws"},"prevItem":{"title":"Oracle Industrial Telnet Server (ITS): The OracleAS Hidden Treasure..","permalink":"/blog/2006/03/13/oracle-industrial-telnet-server-its-the-oracleas-hidden-treasure-dot"},"nextItem":{"title":"SourceLabs SASH certified on Oracle Fusion Middleware","permalink":"/blog/2006/02/10/sourcelabs-sash-certified-on-oracle-fusion-middleware"}},"content":"Download the Beta version of the Oracle Application Server Developer\'s Guide for Microsoft Office Interoperability along with sample code (and other technical resources) from [this new OTN page](http://www.oracle.com/technology/products/middleware/fusion-middleware-microsoft-interoperability.html).\\n\\n*   **Windows Platform**: Fusion Middleware is concurrently tested and delivered on Windows.\\n*   **.NET/Windows Server System Integratio**n: Fusion Middleware offers broad integration with Microsoft .NET and Windows Server System at multiple levels.\\n*   **Office Interoperability**: Fusion Middleware enables use of Office as the front-end for enterprise applications, as well as many ways to interact with enterprise information that can be read, parsed, and generated in  Office-formatted documents."},{"id":"/2006/02/10/sourcelabs-sash-certified-on-oracle-fusion-middleware","metadata":{"permalink":"/blog/2006/02/10/sourcelabs-sash-certified-on-oracle-fusion-middleware","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-02-10-sourcelabs-sash-certified-on-oracle-fusion-middleware.md","source":"@site/blog/2006-02-10-sourcelabs-sash-certified-on-oracle-fusion-middleware.md","title":"SourceLabs SASH certified on Oracle Fusion Middleware","description":"Oracle Fusion Middleware is now a certified platform for the SourceLabs \\"SASH\\" stack: Apache Struts, Apache Axis, the Spring Framework, and Hibernate. Visit this page to download free SASH software!","date":"2006-02-10T00:00:00.000Z","formattedDate":"February 10, 2006","tags":[{"label":"javaee","permalink":"/blog/tags/javaee"},{"label":"spring","permalink":"/blog/tags/spring"}],"readingTime":0.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"SourceLabs SASH certified on Oracle Fusion Middleware","tags":["javaee","spring"]},"prevItem":{"title":"Oracle Fusion Middleware and Microsoft Interoperability - Developer\'s Guide","permalink":"/blog/2006/03/02/oracle-fusion-middleware-and-microsoft-interoperability-developers-guide"},"nextItem":{"title":"Is OracleAS Portal used on the Internet????","permalink":"/blog/2006/01/27/is-oracleas-portal-used-on-the-internet"}},"content":"Oracle Fusion Middleware is now a certified platform for the SourceLabs \\"SASH\\" stack: Apache Struts, Apache Axis, the Spring Framework, and Hibernate. [Visit this page to download free SASH software](http://www.oracle.com/technology/tech/java/sash.html)!\\n\\nSASH is a distribution from SourceLabs that:\\n\\n1. Reconciles library versions and dependencies across the entire stack.\\n2. Includes dependability fixes for the baseline open source projects\\n3. Is rigorously tested according to the CERT7 method.\\n4. Is commercially supported."},{"id":"/2006/01/27/is-oracleas-portal-used-on-the-internet","metadata":{"permalink":"/blog/2006/01/27/is-oracleas-portal-used-on-the-internet","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-01-27-is-oracleas-portal-used-on-the-internet.md","source":"@site/blog/2006-01-27-is-oracleas-portal-used-on-the-internet.md","title":"Is OracleAS Portal used on the Internet????","description":"Just use google with a string used byOracleAS Portal URLs...","date":"2006-01-27T00:00:00.000Z","formattedDate":"January 27, 2006","tags":[{"label":"portal","permalink":"/blog/tags/portal"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Is OracleAS Portal used on the Internet????","tags":["portal"]},"prevItem":{"title":"SourceLabs SASH certified on Oracle Fusion Middleware","permalink":"/blog/2006/02/10/sourcelabs-sash-certified-on-oracle-fusion-middleware"},"nextItem":{"title":"Javaworld: Jason Hunter\'s \'New features added to Servlet 2.5\'","permalink":"/blog/2006/01/04/javaworld-jason-hunters-new-features-added-to-servlet-2-dot-5"}},"content":"Just use google with a string used byOracleAS Portal URLs...\\n\\n*   [servlet/page](http://www.google.com/search?hl=en&q=servlet%2Fpage&btnG=Google+Search&meta=)\\n*   [pls/portal](http://www.google.com/search?hl=en&lr=&q=pls%2Fportal&btnG=Search)\\n\\nA little tip, but you will see that people are very creative with Oracle Portal 10g!"},{"id":"/2006/01/04/javaworld-jason-hunters-new-features-added-to-servlet-2-dot-5","metadata":{"permalink":"/blog/2006/01/04/javaworld-jason-hunters-new-features-added-to-servlet-2-dot-5","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2006-01-04-javaworld-jason-hunters-new-features-added-to-servlet-2-dot-5.md","source":"@site/blog/2006-01-04-javaworld-jason-hunters-new-features-added-to-servlet-2-dot-5.md","title":"Javaworld: Jason Hunter\'s \'New features added to Servlet 2.5\'","description":"Jason Hunter has published an interesting article about new features added to servlet 2.5. The focus of this article is like most of the JEE 5 goals all about simplification, easy of use, this is done by leveraging Java5 annotations (resources refences, life cycle support,...), some improvements around the web.xml, and cross context sessions support.","date":"2006-01-04T00:00:00.000Z","formattedDate":"January 4, 2006","tags":[{"label":"java","permalink":"/blog/tags/java"},{"label":"javaee","permalink":"/blog/tags/javaee"}],"readingTime":0.275,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Javaworld: Jason Hunter\'s \'New features added to Servlet 2.5\'","tags":["java","javaee"]},"prevItem":{"title":"Is OracleAS Portal used on the Internet????","permalink":"/blog/2006/01/27/is-oracleas-portal-used-on-the-internet"},"nextItem":{"title":"SearchWebServices: Oracle\'s Steve Harris on Java and standing out in SOA","permalink":"/blog/2005/12/22/searchwebservices-oracles-steve-harris-on-java-and-standing-out-in-soa"}},"content":"Jason Hunter has published an interesting article about [new features added to servlet 2.5](http://www.javaworld.com/javaworld/jw-01-2006/jw-0102-servlet.html). The focus of this article is like most of the JEE 5 goals all about simplification, easy of use, this is done by leveraging Java5 annotations (resources refences, life cycle support,...), some improvements around the web.xml, and cross context sessions support."},{"id":"/2005/12/22/searchwebservices-oracles-steve-harris-on-java-and-standing-out-in-soa","metadata":{"permalink":"/blog/2005/12/22/searchwebservices-oracles-steve-harris-on-java-and-standing-out-in-soa","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-12-22-searchwebservices-oracles-steve-harris-on-java-and-standing-out-in-soa.md","source":"@site/blog/2005-12-22-searchwebservices-oracles-steve-harris-on-java-and-standing-out-in-soa.md","title":"SearchWebServices: Oracle\'s Steve Harris on Java and standing out in SOA","description":"Interested to know more about the Oracle Application Server 10g and the SOA/Java strategy?","date":"2005-12-22T00:00:00.000Z","formattedDate":"December 22, 2005","tags":[],"readingTime":0.26,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"SearchWebServices: Oracle\'s Steve Harris on Java and standing out in SOA"},"prevItem":{"title":"Javaworld: Jason Hunter\'s \'New features added to Servlet 2.5\'","permalink":"/blog/2006/01/04/javaworld-jason-hunters-new-features-added-to-servlet-2-dot-5"},"nextItem":{"title":"Oracle 10g: Oracle Open World Top 10 presentations","permalink":"/blog/2005/11/22/oracle-10g-oracle-open-world-top-10-presentations"}},"content":"Interested to know more about the Oracle Application Server 10g and the SOA/Java strategy?\\nTake some time to read the SearchWebServices.com interview with Steve Harris, VP of the Java Platform Group at Oracle.\\n\\n* [Oracle\'s Steve Harris on Java and standing out in SOA, part 1](http://searchwebservices.techtarget.com/qna/0,289202,sid26_gci1154040,00.html?track=NL-110&ad=537029)\\n\\nAddition on 30/12: follow [TheServerServide](http://www.theserverside.com/news/thread.tss?thread_id=38233) discussion"},{"id":"/2005/11/22/oracle-10g-oracle-open-world-top-10-presentations","metadata":{"permalink":"/blog/2005/11/22/oracle-10g-oracle-open-world-top-10-presentations","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-11-22-oracle-10g-oracle-open-world-top-10-presentations.md","source":"@site/blog/2005-11-22-oracle-10g-oracle-open-world-top-10-presentations.md","title":"Oracle 10g: Oracle Open World Top 10 presentations","description":"You can find online the top 10 presentations of Oracle Open World 2005:","date":"2005-11-22T00:00:00.000Z","formattedDate":"November 22, 2005","tags":[{"label":"conference","permalink":"/blog/tags/conference"},{"label":"oow","permalink":"/blog/tags/oow"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.62,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle 10g: Oracle Open World Top 10 presentations","tags":["conference","oow","oracle"]},"prevItem":{"title":"SearchWebServices: Oracle\'s Steve Harris on Java and standing out in SOA","permalink":"/blog/2005/12/22/searchwebservices-oracles-steve-harris-on-java-and-standing-out-in-soa"},"nextItem":{"title":"Using Eclipse WTP and OracleAS/OC4J","permalink":"/blog/2005/11/11/using-eclipse-wtp-and-oracleas-slash-oc4j"}},"content":"You can find online the [top 10 presentations of Oracle Open World 2005](http://www.oracle.com/technology/events/oow2005/ow2005_top10.html):\\n\\n1.   Optimizing the Optimizer: Essential SQL Tuning Tips and Techniques\\n2.  10 Things We Like About Oracle Database 10g Release 2\\n3.  PeopleSoft Enterprise, JD Edwards, and Oracle E-Business Suite Integration with Oracle Fusion Middleware\\n4.  The Future of Database and Information Technology\\n5.  Understanding Shared Pool Memory Structures: Tips on How to Optimize Usage and Avoid Errors\\n6.  Take the Guesswork Out of Database I/O Tuning\\n7.  Tuning Oracle SQL in the Real World\\n8.  What They Didn\'t Print in the Doc: High-Availability Best Practices from Oracle Maximum Availability\\n9.  Performance Diagnostics Demystified: Best Practices for Oracle Database 10g\\n10.  Best Practices for Oracle Database 10g Backup and Recovery\\n\\nEnjoy!"},{"id":"/2005/11/11/using-eclipse-wtp-and-oracleas-slash-oc4j","metadata":{"permalink":"/blog/2005/11/11/using-eclipse-wtp-and-oracleas-slash-oc4j","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-11-11-using-eclipse-wtp-and-oracleas-slash-oc4j.md","source":"@site/blog/2005-11-11-using-eclipse-wtp-and-oracleas-slash-oc4j.md","title":"Using Eclipse WTP and OracleAS/OC4J","description":"You have probably heard about Web Tools Platform (WTP), this project","date":"2005-11-11T00:00:00.000Z","formattedDate":"November 11, 2005","tags":[{"label":"oc4j","permalink":"/blog/tags/oc-4-j"},{"label":"oracle","permalink":"/blog/tags/oracle"},{"label":"eclipse","permalink":"/blog/tags/eclipse"}],"readingTime":1.395,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using Eclipse WTP and OracleAS/OC4J","tags":["oc4j","oracle","eclipse"]},"prevItem":{"title":"Oracle 10g: Oracle Open World Top 10 presentations","permalink":"/blog/2005/11/22/oracle-10g-oracle-open-world-top-10-presentations"},"nextItem":{"title":"Installing Blojsom on OracleAS 10g (10.1.3)","permalink":"/blog/2005/10/27/installing-blojsom-on-oracleas-10g-10-dot-1-3"}},"content":"You have probably heard about [Web Tools Platform (WTP)](http://www.eclipse.org/webtools/), this project\\nextends Eclipse with various tools for developping J2EE applications.\\nIt contains editors for HTML, JSP, XML and so more... In addition to\\nvarious wizards to create Web Services, EJB, Connector and more... WTP\\nallows developers to package the application using J2EE standard\\npackagind: EAR,WAR,RAR,JAR and deploy and run the application from the\\nIDE.\\n\\nThe challenge here is not how\\nyou build J2EE components using an IDE, but more how do you package,\\ndeploy and run your application easily from the IDE. Eclipse WTP allows\\nyou like Oracle JDeveloper does, to package the various J2EE components\\nin standard archives (JAR, WAR, EAR, RAR, ...) and deploy them to a\\nJ2EE container. WTP project has a very nice list of containers you\\ncan deploy to: Oracle, IBM, BEA, JBoss, Tomcat, ...\\n\\n[![](http://www.grallandco.com/blog/archives/wtp/wtp-server-thumb.PNG)](http://www.grallandco.com/blog/archives/wtp/wtp-server.html)\\n\\n\\nWith the latest build of Eclipse WTP (Release 1.0M8), Oracle\\nApplication Server 10g is now part of the default server list. So if\\nyour are an OracleAS user you can now use WTP and start to run and\\ndebut your application running in OracleAS from Eclipse.\\n\\nWhat do you need to start using WTP and OracleAS:\\n\\n1.  Download and install Eclipse WTP 1.0M8 from [Eclipse](http://www.eclipse.org/webtools/) site.\\n2.  Download and install OracleAS 10g (OC4J 10.1.3 Developer Preview 4) from [OTN](http://www.oracle.com/technology/tech/java/oc4j/1013/index.html)\\n3.  You must set a administrator password for OC4J using the start command\\n4.  Done! You can now use OC4J\\nand Eclipse.&nbsp;[I have a viewlet that shows the basic steps to create and run your first Web application](http://www.grallandco.com/blog/archives/wtp/wtp-oc4j.swf).\\n\\n[![](http://www.grallandco.com/blog/archives/wtp/wtp-server-conf-thumb.png)](http://www.grallandco.com/blog/archives/wtp/wtp-server-conf.html)\\n\\nOC4J configuration\\n\\n\\nUpdate _(Nov,18)_ : I forgot to mention the [Oracle Application  Server Adapter tutorial](http://www.eclipse.org/webtools/community/tutorials/OracleServerAdapter/OracleServerAdapter.html) from the Web Tools project."},{"id":"/2005/10/27/installing-blojsom-on-oracleas-10g-10-dot-1-3","metadata":{"permalink":"/blog/2005/10/27/installing-blojsom-on-oracleas-10g-10-dot-1-3","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-10-27-installing-blojsom-on-oracleas-10g-10-dot-1-3.md","source":"@site/blog/2005-10-27-installing-blojsom-on-oracleas-10g-10-dot-1-3.md","title":"Installing Blojsom on OracleAS 10g (10.1.3)","description":"I am using internally Blojsom for blogging about the product I am","date":"2005-10-27T00:00:00.000Z","formattedDate":"October 27, 2005","tags":[{"label":"java","permalink":"/blog/tags/java"},{"label":"oc4j","permalink":"/blog/tags/oc-4-j"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.895,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Installing Blojsom on OracleAS 10g (10.1.3)","tags":["java","oc4j","oracle"]},"prevItem":{"title":"Using Eclipse WTP and OracleAS/OC4J","permalink":"/blog/2005/11/11/using-eclipse-wtp-and-oracleas-slash-oc4j"},"nextItem":{"title":"PHP will beat Java? Not sure about it...","permalink":"/blog/2005/10/24/php-will-beat-java-not-sure-about-it-dot-dot-dot"}},"content":"I am using internally Blojsom for blogging about the product I am\\ntaking care of: OC4J and especially the Web Services stack. In this\\npost I am quickly documenting, as the title says, how to install\\nBlojsom on OracleAS 10g.\\n\\n##### 1-Download\\n\\nThe first thing to do is to download the product itself, just download the Blojsom quickstart from sourceforge.\\n\\n##### 2-Deploy\\n\\nSince, Blojsom is distributed as a WAR file you have nothing special to do, just deploy it. Anyway I have created a [viewlet &nbsp;that shows step by step deployment using Oracle Enterprise Manager](http://www.grallandco.com/blog/archives/viewlet/blojsom-install.swf).\\n\\nSo the application link will be: `http://localhost:8888/blojsom/`\\n\\nNote: if you want to use the command line utility (admin.jar) or the\\nAnt deployment task, you should package the Web archive in an EAR file.\\n\\n##### 3-Update the home page\\n\\nThe easiest way will be to just change the index.html page to redirect\\nto the default blog. The index.html page to be modified is in:\\n\\n*  `$ORACLE_HOME/j2ee/home/application/blojsom/blojsomXXXX/index.html`\\nwhere `blojsomXXXX` is the name of the Web application generated during deployment.\\n\\n##### 4- Start blogging...\\n\\nThis is it..."},{"id":"/2005/10/24/php-will-beat-java-not-sure-about-it-dot-dot-dot","metadata":{"permalink":"/blog/2005/10/24/php-will-beat-java-not-sure-about-it-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-10-24-php-will-beat-java-not-sure-about-it-dot-dot-dot.md","source":"@site/blog/2005-10-24-php-will-beat-java-not-sure-about-it-dot-dot-dot.md","title":"PHP will beat Java? Not sure about it...","description":"I would like react to last week marc andreessen\'s words (netscape founder during","date":"2005-10-24T00:00:00.000Z","formattedDate":"October 24, 2005","tags":[{"label":"php","permalink":"/blog/tags/php"},{"label":"java","permalink":"/blog/tags/java"}],"readingTime":2.775,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"PHP will beat Java? Not sure about it...","tags":["php","java"]},"prevItem":{"title":"Installing Blojsom on OracleAS 10g (10.1.3)","permalink":"/blog/2005/10/27/installing-blojsom-on-oracleas-10g-10-dot-1-3"},"nextItem":{"title":"Can I do SOA with Web Services today?","permalink":"/blog/2005/10/20/can-i-do-soa-with-web-services-today"}},"content":"I would like react to last week marc andreessen\'s words (netscape founder during\\n  the php/zend conference andreessen basically says that\\n  [PHP will beat Java](http://management.silicon.com/itpro/0,39024675,39153501,00.htm) and[ succeeds where Java is not](http://news.com.com/Andreessen+PHP+succeeding+where+Java+isnt/2100-1012_3-5903187.html). Most of the reasons that\\n  Andreesen used to justify this saying are because of PHP simplicity. Do you\\n  think he is talking about the language or the application server -yeah, let\'s\\n  call it this way... (J2EE...)..\\n\\nLet\'s talk about J2EE first, it is true that when you take a look to the\\n  learning curve of J2EE and PHP it is a no brainer.. PHP is effectively more simple.\\n  But we can start with the acronyms themselves: J2EE==Enterprise, where\\n  PHP==Personal Home Page ( since then renamed Hypertext Processor). So from the\\n  origins the 2 technologies were not here to achieve the same goals. But PHP has\\n  proven that it could be used for complex application/Web sites, and still keep\\n  it simplicity.\\n\\nSo PHP is simple this is true, but Java on the server could be also. I think\\n  that the complexity comes from the fact that J2EE its expert always see an\\n  application as a enterprise application, and what that means in term of life\\n  cycle, development process, packaging and so on....&nbsp;&nbsp;&nbsp; Why am I\\n  saying that it is coming from the expert? Just take a look to a PHP tutorial\\n  and a J2EE one? This is clear that you must be a very good and experienced\\n  developer to start with J2EE... But this is is not coming from the technologies,\\n  but mainly the way we talk about it. I think we should talk a little more about\\n  simple development with Java. For example focusing o JSP and JSTL for simple\\n  stuff. I agree that putting too much logic in JSP and reduce the number of\\n  layer is \'bad\' for complex applications, but it could be used for some. Why do\\n  we need to always package a WAR or EAR file? Yes you can technically deploy exploded\\n  archive and modify the information after the fact. This is not necessary the\\n  best practices but it could be useful for some applications (not necessary only\\n    in development environment.\\n\\nAbout the language itself, sure that Java is more complex, it is considerate as\\n    a System Level Language, where PHP is a scripting language. Do we still compare\\n    Shell and C? No we need both of them isn\'t?\\n\\nIn the same time, Java as a language must be simplified, or let be more precise...\\n    I think Java developers need the simplicity of scripting languages such as PHP\\n    but with the power of Java under the cover. And this is happening right now. JavaScript\\n    is now integrated to Java 6, Groovy provided a well integrated JVM with a very powerful\\n    and simple syntax. Grails -Groovy On Rails- provided a simple framework for\\n    CRUD applications.\\n\\nStill skeptical about the simplicity of PHP, try it...\\n\\nStill skeptical about Java being simple, try a scripting language... for example Groovy\\n\\nIn conclusion the platform of my dream should:\\n\\n* Keep the simple case simple\\n* Made the impossible possible\\n\\n\\nAnd I am sure that Java is the good language and platform for that; but\\nGroovy and other scripting languages will help. And I will also\\n    continue to develop using PHP too. And both worlds will coexists and be\\n    integrated using Web Services and or JSR-223."},{"id":"/2005/10/20/can-i-do-soa-with-web-services-today","metadata":{"permalink":"/blog/2005/10/20/can-i-do-soa-with-web-services-today","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-10-20-can-i-do-soa-with-web-services-today.md","source":"@site/blog/2005-10-20-can-i-do-soa-with-web-services-today.md","title":"Can I do SOA with Web Services today?","description":"I am just coming back for Toronto where I have been presenting Oracle","date":"2005-10-20T00:00:00.000Z","formattedDate":"October 20, 2005","tags":[{"label":"soa","permalink":"/blog/tags/soa"}],"readingTime":3.73,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Can I do SOA with Web Services today?","tags":["soa"]},"prevItem":{"title":"PHP will beat Java? Not sure about it...","permalink":"/blog/2005/10/24/php-will-beat-java-not-sure-about-it-dot-dot-dot"},"nextItem":{"title":"Publishing SQL and DML as Web Service","permalink":"/blog/2005/09/27/publishing-sql-and-dml-as-web-service"}},"content":"I am just coming back for Toronto where I have been presenting Oracle\\nDeveloper Day: SOA for J2EE Developers. As part of this exercise I have\\nintroduced Web Services and related technologies/standards.\\n\\nA person in the audience asked me:\\n\\n> But can I do SOA Today?\\n> Are Web Services are mature enough?\\n\\nI think that is a very interesting and valid question. The answer is\\ncertainly:&nbsp;\\n\\n* *Yes you can, and I would add, yes you should!!*\\n\\nWeb Services are &nbsp;definitively big actors in Service Oriented\\nArchitecture since by definition they are loosely coupled,\\nimplementation agnostic and facilitate reusability. In the same time\\nthe concern of this person are justified since some important pieces of\\ninfrastructure are yet available in vendors solutions. Or if they exist\\nthey are proprietary and won\'t allow interoperability of the service\\nwith other stack... Losing interoperability is a big deal when you talk\\nabout Web Services since most of the time, if the designers want to use\\nWeb Services it is to be able to reuse it in various applications,\\nindependently of the platform where this application is running.\\n\\n#### Why I have answered Yes?\\n\\n\x3c!-- truncate --\x3e\\n\\nWhy I am so positive when I answered this person? Just simply because\\nWeb Services are today mature enough. But like any development of\\napplications/systems the development should start with a clear\\ndefinition of the requirements. When the development team will start to\\nwrite down the requirement it will be clear of not what are the\\nimportant pieces in term of services. Beside the business requirements,\\nlots of requirements are technical/IT related such as security,\\nperformance, manageability, reliability, and transaction management,\\nperformance... So when you design your system do not forgot to clarify\\nwhat are the different infrastructure services you need....\\n\\nIt is then easy to match your infrastructure requirement list to the\\ndifferent quality of services supported by the platform you will\\nimplementing on, and as important, the platform(s) you will have to\\nintegrate with.&nbsp; For example today it is possible to easily\\ncreate SOA/Web Services application that are secured because\\nWS-Security is a standard supported by most of the vendors. At the\\nopposite it may be very challenging to create SOA/WS based application\\nthat involve a very complex transaction model with various applications\\nand system since no standard have been implemented in a real\\ninteroperable manner. But no worry the WS brains are here and work on\\nit, take some time to read more about WS-Transactions and other related\\nstandardization effort.\\n\\nBased on the previous example with security and transaction, it is also\\nimportant to keep in mind that the Web Services is a mature technology\\nbut it is still evolving -based on real life requirements-. So in the\\nsame time your application/system will evolve -it is one of your goal\\nwhen you do SOA, it is to build a more agile system that can react to\\nbusiness, or technological changes quicker-, do not forget that your\\nvendor are continuously working to not only define the standard but\\nalso implement them in their product. So you may put as a requirement\\nfrom the beginning a specific infrastructure services that are not\\navailable in today\'s product but the important question to ask you\\nbefore dropping SOA/Web Service is when I really need this feature?\\nWhen this feature will be available in my platform?\\n\\nThis is why also it is important to understand the strategy of your\\nvendor in term of flexibility and adaptability of their solution to the\\ndifferent standards, and how it can help you to take care of legacy\\nservices that need to integrate with your new applications. One nice\\nexample is the usage of Oracle Web Services Manager. Even if\\nWS-Security is one of the first standard around \\"Enterprise Web\\nServices\\" it has not been in the different stack for long so lot of\\nexisting WS do not support WS-Sec. Oracle Web Services Manager allow\\nyou using agents and/or gateway to add security in a standard way to\\nexisting services, and enrich you SOA with a better quality of\\nservices.&nbsp; So do not say no to SOA/SOA because a standard does\\nnot exist or exist but not implemented, it will come -take a look to\\nall the WS-* effort-, and this standards will be able to extend the\\nsystem that you are building as you need.... SOA is all about agility,\\nto be sure that it will be agile enough to provide you more services as\\nyou go!"},{"id":"/2005/09/27/publishing-sql-and-dml-as-web-service","metadata":{"permalink":"/blog/2005/09/27/publishing-sql-and-dml-as-web-service","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-27-publishing-sql-and-dml-as-web-service.md","source":"@site/blog/2005-09-27-publishing-sql-and-dml-as-web-service.md","title":"Publishing SQL and DML as Web Service","description":"This morning I have been talking about the Oracle Database and Web Services. If you are","date":"2005-09-27T00:00:00.000Z","formattedDate":"September 27, 2005","tags":[{"label":"ws","permalink":"/blog/tags/ws"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":3.705,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Publishing SQL and DML as Web Service","tags":["ws","oracle"]},"prevItem":{"title":"Can I do SOA with Web Services today?","permalink":"/blog/2005/10/20/can-i-do-soa-with-web-services-today"},"nextItem":{"title":"Oracle Developer Day: September/October Sessions","permalink":"/blog/2005/09/20/oracle-developer-day-september-slash-october-sessions"}},"content":"This morning I have been talking about the Oracle Database and Web Services. If you are\\nOracle 10g developer (Database or Application Server) you\\nprobably already know that you can publish PL/SQL stored procedure as\\nWeb Services... One of the new feature of our OracleAS 10.1.3 release\\nis the fact that now you can publish SQL and DML as well. In this post\\nI am quickly explaining the basic steps to publish a query as Web\\nService.\\n\\n\x3c!-- truncate --\x3e\\n\\n#### Oracle Web Service Assembler\\n\\nIf you are not familiar with the Oracle Web Services tools, or if you\\nare using Oracle JDeveloper to generate your services and client, I\\nwould like to use the Oracle WS command line utility to do the work\\nhere. The Oracle Web Services Assembler, aka WSA, allows you to\\ngenerate\\nclient and server using Oracle JAX-RPC implementation and extensions.\\n\\nWSA is a Java utility that can be used as command line or with Apache\\nAnt, basically it is a Jar file located in <span\\nclass=\\"code\\">$ORACLE_HOME/webservices/lib/wsa.jar</span>.\\nType the following command in a terminal to learn more about WSA.\\n\\n```\\njava -jar $ORACLE_HOME/webservices/lib/wsa.jar -help\\n  Usage:\\n  java -jar wsa.jar -<command> -debug -help\\n  where command can be one of:\\n    analyze\\n    annotationAssemble\\n    aqAssemble\\n    assemble\\n    corbaAssemble\\n    dbJavaAssemble\\n    ejbAssemble\\n    fetchWsdl\\n    genApplicationDescriptor\\n    genConcreteWsdl\\n    genDDs\\n    genGatewayService\\n    genInterface\\n    genProxy\\n    genQosWsdl\\n    genValueType\\n    genWsdl\\n    help\\n    jmsAssemble\\n    plsqlAssemble\\n    sqlAssemble\\n    topDownAssemble\\n    version\\n```\\n\\n\\nAs you can see with the different command options, Oracle WAS allowsyou to do \\"everything\\" Web Services related, for example creating a WS\\nfrom an EJB using `ejbAssemble`, from a stored procedure using `plsqlAssemble`... and I let you guess, we will be using `sqlAssemble` for this specific demo.\\n\\nTo have the detail of all the parameters of each command you can just\\nenter\\n\\n```\\njava -jar $ORACLE_HOME/webservices/lib/wsa.jar -[command] -help\\n```\\n\\n#### Generating the Web Service from a SQL\\n\\nSo you can easily generate a Web Service using WSA from multiple SQL\\nstatements, to do it, just use the following command:\\n\\n```\\njava -jar wsa.jar -sqlAssemble\\n    -appName my-soaql-application\\n    -dataSource jdbc/MyDBServices\\n    -sqlStatement \\"getAllEmp=select ename, sal from emp\\"\\n    -sqlStatement \\"getEmpByDept=select ename, sal from emp where DEPTNO = :{dept NUMBER}\\"\\n    -dbConnection jdbc:oracle:thin:@localhost:1521:orcl\\n    -dbUser scott/tiger\\n```\\n\\nSo WSA has created the different classes needed by the Web Service but\\nalso packaged in a EAR file that you can now deploy to your\\nOracleAS instance. The appName parameter is used to generate the\\ndifferent application name and files (EAR and WAR). The sqlStatement\\nused to specify the different queries you want to publish as operation.\\n\\n#### Deploying the application\\n\\nYou can either use Oracle EM Application Server Control to deploy the\\napplication or using the command line utility, `admin.jar`\\n\\n```\\njava -jar $ORACLE_HOME/j2ee/home/admin.jar ormi://hostName[:ormiPort] oc4jadmin password -deploy -file my-soaql-application -deploymentName my-soaql-application\\n\\njava -jar $ORACLE_HOME/j2ee/home/admin.jar ormi://hostName[:ormiPort] oc4jadmin password  -bindWebApp my-soaql-application my-soaql-application-web default-web-site /soaql\\n```\\n\\nAlso be sure that you have a datasource defined in your application server that match the parameters set when you ran the WSA command, in my case\\nit will be `jdbc/MyDBServices` that connection to my local database using the SCOTT schema.\\n\\nYou should now be able to access the service using the following URL:\\n\\n* `http://youserver:port/soaql/my-soaql-application`\\n\\n#### What is going on?\\n\\nWhen you are running the Web Service that is deployed inside OracleAS\\nthe flow is quite simple:\\n\\n1.  A client is sending a request to the server using SOAP. So it uses the different typed as defined in the payload\\n2.  The JAX-RPC Servlet processes the request and deserializes the message\\n3.  The generated classes use the OC4J DataSource to connect to the database and execute the statement using JDBC\\n4.  The database sends the data to the classes, and servlet that creates a SOAP response\\n\\nWhen you access the test page (\\n  http://youserver:port/soaql/my-soaql-application )  or when\\n  you are viewing the generated WSDL you probably notice that each query\\n  is published with 3 different operations. These operations return the\\n  same data but using different formats:\\n\\n  * &lt;operationName&gt;Bean : returns the data as serialize Javabean\\n  * &lt;operationName&gt;XML : returns the data in the SOAP body as Rowset/Row structure\\n  * &lt;operationName&gt;XMLRowSet : returns the data in the SOAP body using the WebRowset format ([JSR-114](http://jcp.org/en/jsr/detail?id=114))\\n\\n#### Summary\\n\\nIn this small article you have learnt how to pulish a SQL statement as Web Service. It is interesting to take a look closely to the WSA tools that give you several way of building Web Services, starting from SQL, Stored Procedure, Java, EJB, or using a contract based approach starting from the WSDL."},{"id":"/2005/09/20/oracle-developer-day-september-slash-october-sessions","metadata":{"permalink":"/blog/2005/09/20/oracle-developer-day-september-slash-october-sessions","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-20-oracle-developer-day-september-slash-october-sessions.md","source":"@site/blog/2005-09-20-oracle-developer-day-september-slash-october-sessions.md","title":"Oracle Developer Day: September/October Sessions","description":"Oracle Developer Day](http","date":"2005-09-20T00:00:00.000Z","formattedDate":"September 20, 2005","tags":[{"label":"conference","permalink":"/blog/tags/conference"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.64,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Developer Day: September/October Sessions","tags":["conference","oracle"]},"prevItem":{"title":"Publishing SQL and DML as Web Service","permalink":"/blog/2005/09/27/publishing-sql-and-dml-as-web-service"},"nextItem":{"title":"Oracle JDeveloper 10g (10.1.3) EA 1 available","permalink":"/blog/2005/09/20/oracle-jdeveloper-10g-10-dot-1-3-ea-1-available"}},"content":"Oracle Developer Day](http://www.oracle.com/webapps/events/EventsDetail.jsp?p_eventId=9691&src=1587968&src=1587968&Act=43) is a free, hands-on workshop that will give you insight into the latest technologies in SOA and Java. You can choose from one of two topics:\\n\\n* Track 1: Emerging SOA and J2EE technologies: EJB 3.0, JavaServer Faces and BPEL\\n* Track 2: Rapid J2EE Application Development for Forms and Designer Developers\\n\\nHere the list of dates and cities:\\n\\n* [New York,    NY \u2013 Thursday, September 29, 2005](http://www.oracle.com/go/?&amp;Src=4008297&amp;Act=15) \u2013 New York Metro Area Oracle User Group Day\\n* [Pasadena, CA \u2013 Tuesday, October 11, 2005](http://www.oracle.com/go/?&amp;Src=2286074&amp;Act=826)\\n* [Toronto, Ontario \u2013 Tuesday October 18, 2005](http://www.oracle.com/go/?&amp;Src=2286074&amp;Act=820)\\n* [Montreal, Quebec \u2013 Thursday, October 20, 2005](http://www.oracle.com/go/?&amp;Src=2286074&amp;Act=821)\\n* [Columbus, OH \u2013 Monday, October 24, 2005](http://www.oracle.com/go/?&amp;Src=2286074&amp;Act=827)\\n* [Minneapolis, MN \u2013 Thursday, October 27, 2005](http://www.oracle.com/go/?&amp;Src=2286074&amp;Act=828)\\n* Calgary, Alberta \u2013 Tuesday, November 1, 2005"},{"id":"/2005/09/20/oracle-jdeveloper-10g-10-dot-1-3-ea-1-available","metadata":{"permalink":"/blog/2005/09/20/oracle-jdeveloper-10g-10-dot-1-3-ea-1-available","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-20-oracle-jdeveloper-10g-10-dot-1-3-ea-1-available.md","source":"@site/blog/2005-09-20-oracle-jdeveloper-10g-10-dot-1-3-ea-1-available.md","title":"Oracle JDeveloper 10g (10.1.3) EA 1 available","description":"You can now downlaod the Early Access 1 version of Oracle JDeveloper 10g (10.1.3) from OTN.","date":"2005-09-20T00:00:00.000Z","formattedDate":"September 20, 2005","tags":[{"label":"java","permalink":"/blog/tags/java"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.08,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle JDeveloper 10g (10.1.3) EA 1 available","tags":["java","oracle"]},"prevItem":{"title":"Oracle Developer Day: September/October Sessions","permalink":"/blog/2005/09/20/oracle-developer-day-september-slash-october-sessions"},"nextItem":{"title":"Oracle Open World: Monday","permalink":"/blog/2005/09/19/oracle-open-world-monday"}},"content":"You can now downlaod the Early Access 1 version of Oracle JDeveloper 10g (10.1.3) from [OTN](http://www.oracle.com/technology/software/products/jdev/index.html)."},{"id":"/2005/09/19/oracle-open-world-monday","metadata":{"permalink":"/blog/2005/09/19/oracle-open-world-monday","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-19-oracle-open-world-monday.md","source":"@site/blog/2005-09-19-oracle-open-world-monday.md","title":"Oracle Open World: Monday","description":"Another nice day in San Francisco; I have been in booth duty most of the day and I have finished the day with a first presentation, with George Trujillo about J2EE Deployment strategy. This presentation I hope was interested for the quite big audience, the long Q&A session -1h, it is nice to be the last in the room- was very good since I talk with various Oracle customer about J2EE management in perspective of the database management. The benefits of Oracle Grid Control for Database and AppServer administrators... I would invite you if you are not familiar with Oracle Grid Control to take a look to OTN.","date":"2005-09-19T00:00:00.000Z","formattedDate":"September 19, 2005","tags":[{"label":"conference","permalink":"/blog/tags/conference"},{"label":"oow","permalink":"/blog/tags/oow"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.875,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Open World: Monday","tags":["conference","oow","oracle"]},"prevItem":{"title":"Oracle JDeveloper 10g (10.1.3) EA 1 available","permalink":"/blog/2005/09/20/oracle-jdeveloper-10g-10-dot-1-3-ea-1-available"},"nextItem":{"title":"Oracle Open World: Sunday","permalink":"/blog/2005/09/18/oracle-open-world-sunday"}},"content":"Another nice day in San Francisco; I have been in booth duty most of the day and I have finished the day with a first presentation, with George Trujillo about J2EE Deployment strategy. This presentation I hope was interested for the quite big audience, the long Q&A session -1h, it is nice to be the last in the room- was very good since I talk with various Oracle customer about J2EE management in perspective of the database management. The benefits of Oracle Grid Control for Database and AppServer administrators... I would invite you if you are not familiar with [Oracle Grid Control to take a look to OTN](http://www.oracle.com/technology/products/oem/index.html).\\n\\nSome of the interesting point is the announcement of the new [OracleAS 10g R3](http://biz.yahoo.com/prnews/050919/sfm087.html?.v=24), that enhanced the Fusion Middleware to deliver SOA, beside the support of J2EE 1.5, Java 5, EJB 30, JSF, I was very pleased to spend my day demonstrating our new UDDI V3 registry that continue to expend our Web Services stack by giving new services in addition to BPEL and Web Services Manager."},{"id":"/2005/09/18/oracle-open-world-sunday","metadata":{"permalink":"/blog/2005/09/18/oracle-open-world-sunday","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-18-oracle-open-world-sunday.md","source":"@site/blog/2005-09-18-oracle-open-world-sunday.md","title":"Oracle Open World: Sunday","description":"The first Oracle World Day for me has started with finishing the various demonstrations of the J2EE Core OC4J and Web Services demonstration Pods. Tomorrow Monday, we will be showing some very cool stuff around Web Services development and management, with some good news around Web Services Registry. Also on the J2EE Core, nice demonstrations of the different services but some of the new features of the OracleAS 10.1.3 such as the new clustering framework and the usage of scripting technologies, especially Groovy, to administer and monitor OracleAS, thanks to JMX!","date":"2005-09-18T00:00:00.000Z","formattedDate":"September 18, 2005","tags":[{"label":"conference","permalink":"/blog/tags/conference"},{"label":"oow","permalink":"/blog/tags/oow"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":1.955,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Open World: Sunday","tags":["conference","oow","oracle"]},"prevItem":{"title":"Oracle Open World: Monday","permalink":"/blog/2005/09/19/oracle-open-world-monday"},"nextItem":{"title":"Administer and Monitor OC4J using JConsole","permalink":"/blog/2005/09/16/administer-and-monitor-oc4j-using-jconsole"}},"content":"The first Oracle World Day for me has started with finishing the various demonstrations of the J2EE Core OC4J and Web Services demonstration Pods. Tomorrow Monday, we will be showing some very cool stuff around Web Services development and management, with some good news around Web Services Registry. Also on the J2EE Core, nice demonstrations of the different services but some of the new features of the OracleAS 10.1.3 such as the new clustering framework and the usage of scripting technologies, especially Groovy, to administer and monitor OracleAS, thanks to JMX!\\n\\nWhat about the rest of the day, I was invited to the Web Services Special Interest Group, WS-SIG, - no.. this is not a new acronym in the WS-* world-. This new SIG has been created in the beginning of the year and on the behalf of IOUG, please take a look to the [WSSIG web site](http://wssig.oaug.org/). ![IMG_6718.jpg](http://farm1.static.flickr.com/179/388749477_d89cc5e85d_o.jpg). One of the very interesting things of this SIG is, like the J2EE SIG (Organized by OTUG, IOUG, OAUG and Oracle), developers, architects, and administrators of Fusion Middleware, the Database and Oracle Fusion applications are talking together. This reunion of people is good since we all from development to operations have a different view of systems. So during this panel we have introduced quickly the concepts, and then the audience jumped in for the Q&A session, as you can guess, we add the classical questions around Web Services for the enterprise such as what about security? Management? Life Cycle ?... and so on. We answered obviously, but I would say if you are lucky enough to be at San Francisco this week, please come to the Demo Ground especially Web Services, Web Services Manager and BPEL where you will see the Oracle products in action and you will be able to find experts to help you to answer your questions.\\n\\nThe main auditorium was packed to see the first keynote/show of Oracle Open World 2005, where Oracle presented some of the numerous charity efforts that it does, and offer some fun to the attendees with a nice show from [Dana Carvey](http://www.danacarvey.net/)\\n\\n#### What about Oracle Blog dinner?\\n\\n[Mark Rittman](http://www.rittman.net/) is organizing a dinner for the different Oracle bloggers, it will be at the [Paragon Restaurant](http://www.paragonrestaurant.com/sfhome.htm) at Second Street on Townsend at 8.00pm on Tuesday 20th. ( I will be there for sure...)"},{"id":"/2005/09/16/administer-and-monitor-oc4j-using-jconsole","metadata":{"permalink":"/blog/2005/09/16/administer-and-monitor-oc4j-using-jconsole","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-16-administer-and-monitor-oc4j-using-jconsole.md","source":"@site/blog/2005-09-16-administer-and-monitor-oc4j-using-jconsole.md","title":"Administer and Monitor OC4J using JConsole","description":"OC4J 10g","date":"2005-09-16T00:00:00.000Z","formattedDate":"September 16, 2005","tags":[{"label":"oc4j","permalink":"/blog/tags/oc-4-j"},{"label":"jmx","permalink":"/blog/tags/jmx"}],"readingTime":1.16,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Administer and Monitor OC4J using JConsole","tags":["oc4j","jmx"]},"prevItem":{"title":"Oracle Open World: Sunday","permalink":"/blog/2005/09/18/oracle-open-world-sunday"},"nextItem":{"title":"Beta Version of Zend Core for Oracle available","permalink":"/blog/2005/09/02/beta-version-of-zend-core-for-oracle-available"}},"content":"OC4J 10*g*\\n(10.1.3) is a J2EE 1.4 containers, so as part of the specifications, it\\nsupports JMX for management and deployment. One of the benefits of JMX\\nis the fact that finally Java applications, and in our case the J2EE\\ncontainers, have a standard based interface to be administered. OC4J\\nexposes using its MBean Server system and application lever management\\nbeans (MBeans) that you can monitor and control from the Oracle\\nApplication Server Control (ASC) that is pre-deployed; but you can use\\nany JMX client application. Sun has included as part of Java 5 JMX but\\nalso provides a standard JMX client called JConsole. This post is\\nsimply explaining how you can use the Sun\'s JConsole with OC4J.\\n\\n1.  Set the environment:\\n\\n```\\n$ORACLE_HOME to the OC4J home\\n\\n$JAVA_HOME your JDK home\\n```\\n\\n2.  Start the OC4J with the following property set\\n\\n```\\n$JAVA_HOME/bin/java -Dcom.sun.management.jmxremote -jar oc4j.jar\\n\\n```\\nThe -Dcom.sun.management.jmxremote system property creates an RMI\\nconnector to the MBeanServer, we will use this RMI connector from the\\nconsole iself.\\n\\n3.  Start the JConsole, with the\\nfollowing command, adding the OC4J administration class to the\\nclasspath.\\n\\n```\\n$JAVA_HOME/bin/jconsole -J-Djava.class.path=$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar;$ORACLE_HOME/lib/adminclient.jar\\n\\n```\\n4.  The console will\\nautomatically ask you to connect to the OC4J process and you can start\\nto monitor and administer your OC4J instance\\n\\n![]( http://farm1.static.flickr.com/139/388744041_9cbbb6734f_o.png )\\n\\nConnection to the OC4J MBean Server\\n\\n![]( http://farm1.static.flickr.com/148/388744045_1bb6e9c149_o.png )\\n\\nJava 5 JConsole browsing the OC4J MBeans"},{"id":"/2005/09/02/beta-version-of-zend-core-for-oracle-available","metadata":{"permalink":"/blog/2005/09/02/beta-version-of-zend-core-for-oracle-available","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-09-02-beta-version-of-zend-core-for-oracle-available.md","source":"@site/blog/2005-09-02-beta-version-of-zend-core-for-oracle-available.md","title":"Beta Version of Zend Core for Oracle available","description":"Zend Core for Oracle, developed in partnership with Zend Technologies, supports businesses using PHP with Oracle Database for mission-critical Web applications. It provides a seamless out-of-the-box experience delivering a stable, high performance, easy-to-install and supported PHP development and production environment fully integrated with the Oracle Database.","date":"2005-09-02T00:00:00.000Z","formattedDate":"September 2, 2005","tags":[{"label":"php","permalink":"/blog/tags/php"}],"readingTime":0.45,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Beta Version of Zend Core for Oracle available","tags":["php"]},"prevItem":{"title":"Administer and Monitor OC4J using JConsole","permalink":"/blog/2005/09/16/administer-and-monitor-oc4j-using-jconsole"},"nextItem":{"title":"Online Groovy Beginners Tutorial","permalink":"/blog/2005/08/29/online-groovy-beginners-tutorial"}},"content":"![](http://www.oracle.com/technology/tech/php/zendcore/core4oracle_logo_m.gif)Zend Core for Oracle, developed in partnership with Zend Technologies, supports businesses using PHP with Oracle Database for mission-critical Web applications. It provides a seamless out-of-the-box experience delivering a stable, high performance, easy-to-install and supported PHP development and production environment fully integrated with the Oracle Database.\\n\\nZend Core for Oracle will be available as a free download from Zend in late 2005; it is currently available in Beta. Support and updates for Zend Core for Oracle will be made available directly from Zend.\\n\\n[Find more and download link from OTN](http://www.oracle.com/technology/tech/php/zendcore/index.html)."},{"id":"/2005/08/29/online-groovy-beginners-tutorial","metadata":{"permalink":"/blog/2005/08/29/online-groovy-beginners-tutorial","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-08-29-online-groovy-beginners-tutorial.md","source":"@site/blog/2005-08-29-online-groovy-beginners-tutorial.md","title":"Online Groovy Beginners Tutorial","description":"Interested by Groovy but never used it... You will be please to use this nice tutorial available on the Groovy site:","date":"2005-08-29T00:00:00.000Z","formattedDate":"August 29, 2005","tags":[{"label":"groovy","permalink":"/blog/tags/groovy"},{"label":"scripting","permalink":"/blog/tags/scripting"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Online Groovy Beginners Tutorial","tags":["groovy","scripting"]},"prevItem":{"title":"Beta Version of Zend Core for Oracle available","permalink":"/blog/2005/09/02/beta-version-of-zend-core-for-oracle-available"},"nextItem":{"title":"YAIM (Yet Another Instant Messenging): Google Talk","permalink":"/blog/2005/08/25/yaim-yet-another-instant-messenging-google-talk"}},"content":"Interested by Groovy but never used it... You will be please to use this nice tutorial available on the Groovy site:\\n* [Groovy Beginners Tutorial](http://groovy.codehaus.org/Beginners+Tutorial)\\n\\nThanks to Graham Miller\'s contribution..."},{"id":"/2005/08/25/yaim-yet-another-instant-messenging-google-talk","metadata":{"permalink":"/blog/2005/08/25/yaim-yet-another-instant-messenging-google-talk","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-08-25-yaim-yet-another-instant-messenging-google-talk.md","source":"@site/blog/2005-08-25-yaim-yet-another-instant-messenging-google-talk.md","title":"YAIM (Yet Another Instant Messenging): Google Talk","description":"I have been playing around with Google Talk It is quite nice, once again I like the look... But I do not see \'revolutionnary\' features...","date":"2005-08-25T00:00:00.000Z","formattedDate":"August 25, 2005","tags":[],"readingTime":0.58,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"YAIM (Yet Another Instant Messenging): Google Talk"},"prevItem":{"title":"Online Groovy Beginners Tutorial","permalink":"/blog/2005/08/29/online-groovy-beginners-tutorial"},"nextItem":{"title":"Oracle at FTP Java Pro Live","permalink":"/blog/2005/08/23/oracle-at-ftp-java-pro-live"}},"content":"I have been playing around with [Google Talk](http://www.google.com/talk/) It is quite nice, once again I like the look... But I do not see \'revolutionnary\' features...\\nLike Skype I was able to use it at work IM and Voice chat...\\nI like the fact that they [document](http://www.google.com/talk/otherclients.html) how to work around their lack of platform support... I would like to have a mac version... I am surprised that Mac users in Google did not yet implement it!\\n\\nFo developer perspective it is interesting to know that they use standard based protocol such as Jabber/XMPP, and they invite people to use the API (since open standard) and server... Take a look to the [developer section of their documentation](http://www.google.com/talk/developer.html)."},{"id":"/2005/08/23/oracle-at-ftp-java-pro-live","metadata":{"permalink":"/blog/2005/08/23/oracle-at-ftp-java-pro-live","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-08-23-oracle-at-ftp-java-pro-live.md","source":"@site/blog/2005-08-23-oracle-at-ftp-java-pro-live.md","title":"Oracle at FTP Java Pro Live","description":"FTP will organize the \\"Java Pro Live event September 12-14 in San Diego; Oracle will be presenting its vision and solution around Service Oriented Architecture and Java Development. Here some of the events:","date":"2005-08-23T00:00:00.000Z","formattedDate":"August 23, 2005","tags":[],"readingTime":0.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle at FTP Java Pro Live"},"prevItem":{"title":"YAIM (Yet Another Instant Messenging): Google Talk","permalink":"/blog/2005/08/25/yaim-yet-another-instant-messenging-google-talk"},"nextItem":{"title":"JavaOne Hangover","permalink":"/blog/2005/07/02/javaone-hangover"}},"content":"FTP will organize the \\"[Java Pro Live ](http://www.ftponline.com/conferences/javaprolive/)event September 12-14 in San Diego; Oracle will be presenting its vision and solution around Service Oriented Architecture and Java Development. Here some of the events:\\n\\n*   Ted Farell\'s Keynote\\n*   EJB 3.0: The Next Generation by [Mike Keith](http://jroller.com/page/mkeith)\\n*   [Exploring the Latest J2EE Technologies with Oracle JDeveloper 10.1.3](http://www.ftponline.com/conferences/javaprolive/workshops.aspx#ws3)"},{"id":"/2005/07/02/javaone-hangover","metadata":{"permalink":"/blog/2005/07/02/javaone-hangover","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-07-02-javaone-hangover.md","source":"@site/blog/2005-07-02-javaone-hangover.md","title":"JavaOne Hangover","description":"This last 2 weeks have been kind of crazy for me, and for Oracle.. Last week we were present at the Oracle Developer Tools User Group, and immediately jump in for the 10th edition of JavaOne.","date":"2005-07-02T00:00:00.000Z","formattedDate":"July 2, 2005","tags":[{"label":"javaone","permalink":"/blog/tags/javaone"},{"label":"conference","permalink":"/blog/tags/conference"}],"readingTime":1.66,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaOne Hangover","tags":["javaone","conference"]},"prevItem":{"title":"Oracle at FTP Java Pro Live","permalink":"/blog/2005/08/23/oracle-at-ftp-java-pro-live"},"nextItem":{"title":"JavaOne Day 1: What do to today?","permalink":"/blog/2005/06/27/javaone-day-1-what-do-to-today"}},"content":"This last 2 weeks have been kind of crazy for me, and for Oracle.. Last week we were present at the Oracle Developer Tools User Group, and immediately jump in for the 10th edition of JavaOne.\\n\\nI was not able to attend as much sessions as I wanted, but I had the pleasure to work a lot on the Oracle Booth in the JavaOne Pavillion. I  worked on the J2EE, JSF, BPEL and Web Services ones. It was really great, with lot of technical questions around our products; I have to say that this year we had lot of interest around JavaServer Faces and EJB.\\n\\nAnd this has been even better after Thomas Kurian\'s keynote on Tuesday with the following annoucements:\\n\\n* Oracle is participating to [MyFaces](http://myfaces.apache.org/) project. You can find more information on [JavaServer Faces on OTN](http://www.oracle.com/technology/tech/java/jsf.html).\\n\\n* Oracle will lead the development of the JSF extension for Eclipse, as you may know Oracle JDeveloper, now free, has one of the most appealing JSF Designer tool. So we are currently porting this design tools inside Eclipse, to facilitate the adoption of Faces by the developers.\\n\\n* Mike Keith, Toplink Architect, will be now co-specification leader of the persistence specification. Also Oracle will develop the Reference Implementation of the persistence engine of the JEE 5 platform. As you  may know you can already start to develop EJB 3.0 within the early implementation available on [OTN](http://otn.oracle.com/ejb3). Note that the next release of Oracle Application Server 10g (10.1.3), EJB 3.0 will be supported.\\n\\n* As you may have seen some weeks ago, Oracle is also the lead in the Eclipse project to implement the EJB 3 tool.\\n\\n* Also still around Eclipse, Oracle, most successful player around BPEL, will continue the development of the BPEL deigner inside Eclipse and put that officially in the Eclipse Project. If you are not familiar with BPEL, I will invite you [to jump to the OTN Web Site](http://otn.oracle.com/bpel).\\n\\n* Last, but not least, [Oracle JDeveloper is now free](http://otn.oracle.com/products/jdev)..."},{"id":"/2005/06/27/javaone-day-1-what-do-to-today","metadata":{"permalink":"/blog/2005/06/27/javaone-day-1-what-do-to-today","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-27-javaone-day-1-what-do-to-today.md","source":"@site/blog/2005-06-27-javaone-day-1-what-do-to-today.md","title":"JavaOne Day 1: What do to today?","description":"Here we go, the first day of Java One is about to start; and here some of the cool stuff to do:","date":"2005-06-27T00:00:00.000Z","formattedDate":"June 27, 2005","tags":[{"label":"javaone","permalink":"/blog/tags/javaone"},{"label":"conference","permalink":"/blog/tags/conference"}],"readingTime":0.695,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaOne Day 1: What do to today?","tags":["javaone","conference"]},"prevItem":{"title":"JavaOne Hangover","permalink":"/blog/2005/07/02/javaone-hangover"},"nextItem":{"title":"JavaOne: Oracle activities","permalink":"/blog/2005/06/26/javaone-oracle-activities"}},"content":"Here we go, the first day of Java One is about to start; and here some of the cool stuff to do:\\n\\n* Next-Generation Web Services in the Java\u2122 Platform (TS-7230 / 2:15pm)\\n* Groovy = Java\u2122 Technology+ Ruby + Python for the JVM\u2122 (TS-3402 / 3:30pm)\\n* Web Services in the Real World (TS-3999 / 4:45pm)\\n* Java Web Services Development Using Annotations (TS-7964 / 6:00pm)\\n* Writing Performant WSDLs to Build Enterprise Web Services Applications (BOF-9213 / 7:30)\\n* _OC4J: Meet the Developers_ (BOF-9024 / 9:30pm)\\n*  Ensuring 100% Application Portability with J2EE (BOF-9030 / 10:30pm)\\n\\nAlso I think the opening Sun\'s Keynote from 8:30 to 11:00 am will be I am sure a good starting point for this week !\\n\\nBusy day isn\'t? (I am on duty the morning, will try to go to afternoon sessions...)"},{"id":"/2005/06/26/javaone-oracle-activities","metadata":{"permalink":"/blog/2005/06/26/javaone-oracle-activities","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-26-javaone-oracle-activities.md","source":"@site/blog/2005-06-26-javaone-oracle-activities.md","title":"JavaOne: Oracle activities","description":"The 2005 edition of JavaOne will be a very good one, at least based of what Oracle is planning to do.","date":"2005-06-26T00:00:00.000Z","formattedDate":"June 26, 2005","tags":[{"label":"conference","permalink":"/blog/tags/conference"},{"label":"javaone","permalink":"/blog/tags/javaone"}],"readingTime":1.62,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaOne: Oracle activities","tags":["conference","javaone"]},"prevItem":{"title":"JavaOne Day 1: What do to today?","permalink":"/blog/2005/06/27/javaone-day-1-what-do-to-today"},"nextItem":{"title":"Google Code: AJAXSLT","permalink":"/blog/2005/06/24/google-code-ajaxslt"}},"content":"The 2005 edition of JavaOne will be a very good one, at least based of what Oracle is planning to do.\\n\\n### Oracle Booth\\n\\nI invite you to come on the Oracle booth (_booth 625_) where Oracle Developers and Product Managers will be pleased to talk, and demonstrate the latest technologies and their integration in our product (J2EE 1.4, EJB 3.0, JSF, BPEL, Web Services, RFID and more...). Like last year we provide you a chance to *win lot of cool stuff*. First of all enter a raffle for a chance to win a Sony PlayStation 2, Star Wars PC game packs, and Atari joysticks.\\n\\nAlso the first 500 people to submit an entry survey with a stamp from an Oracle demo pod will receive a ticket to see [Star Wars III: Revenge of the Sith](http://www.starwars.com/episode-iii/) the evening of Tuesday, June 28.\\n\\nI will be present on the J2EE Demo pod on Monday morning and I will be please to stamp you to offer you a ticket for the Stars Wars... but remember you can get a ticket for the space too -yes you can go to the space-, by participating to the [Oracle Space Sweepstakes](http://oracle.promotionexpert.com/SpaceSweepstakes/en/index.jsp?&Src=3559501&Act=20).\\n\\n### Meet the Gurus\\n\\nThe Oracle booth also is a place where you can meet several gurus for a presentation and discussion. Here some of the subjects that will be presented:\\n\\n* J2EE and EJB 3.0\\n* JSF, and advanced HTML/AJaX features\\n* SOA, BPEL\\n* Apache Maven\\n\\nYou can find the [schedule on OTN](http://www.oracle.com/technology/events/javaone05/gurus.html).\\n\\n### Sessions and BOF\\n\\nOracle is also proud to have 16 technical sessions and BOF, you can find the list of them [here](http://www.oracle.com/technology/events/javaone05/sessions.html).  One of them is my favorite:\\n\\n* \\"_Oracle Application Server Containers for J2EE (OC4J): Meet the Developers_\\". It will be Monday 27th at 9:30pm, this is the chance for you to meet, and interact with the J2EE and Web Services developers from Oracle.\\n\\nSee you there and enjoy JavaOne 2005!"},{"id":"/2005/06/24/google-code-ajaxslt","metadata":{"permalink":"/blog/2005/06/24/google-code-ajaxslt","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-24-google-code-ajaxslt.md","source":"@site/blog/2005-06-24-google-code-ajaxslt.md","title":"Google Code: AJAXSLT","description":"Google has released via its Google Code project an implementation of XSLT in javascript... Quite interesting to improve the development of AJaX based applications, I think this is the only reason to put the term AJaX in the project... But it is true that everytime you put AJaX in a title of something you are getting people interested...","date":"2005-06-24T00:00:00.000Z","formattedDate":"June 24, 2005","tags":[{"label":"ajax","permalink":"/blog/tags/ajax"}],"readingTime":0.385,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Google Code: AJAXSLT","tags":["ajax"]},"prevItem":{"title":"JavaOne: Oracle activities","permalink":"/blog/2005/06/26/javaone-oracle-activities"},"nextItem":{"title":"Thanks to MozBackup...","permalink":"/blog/2005/06/14/thanks-to-mozbackup-dot-dot-dot"}},"content":"Google has released via its [Google Code](http://code.google.com/) project an implementation of XSLT in javascript... Quite interesting to improve the development of AJaX based applications, I think this is the only reason to put the term AJaX in the project... But it is true that everytime you put AJaX in a title of something you are getting people interested...\\n\\nThe source code is available in [sourceforge](http://sourceforge.net/projects/goog-ajaxslt/).\\n\\nFeel free to post comments into the [Google group](http://groups-beta.google.com/group/google-ajax/) created for this"},{"id":"/2005/06/14/thanks-to-mozbackup-dot-dot-dot","metadata":{"permalink":"/blog/2005/06/14/thanks-to-mozbackup-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-14-thanks-to-mozbackup-dot-dot-dot.md","source":"@site/blog/2005-06-14-thanks-to-mozbackup-dot-dot-dot.md","title":"Thanks to MozBackup...","description":"My Windows laptop has been infected by some viruses and adware -why I am not only using my Mac and Linux computers?-, so I just reinstalled the full OS.....","date":"2005-06-14T00:00:00.000Z","formattedDate":"June 14, 2005","tags":[],"readingTime":0.29,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Thanks to MozBackup..."},"prevItem":{"title":"Google Code: AJAXSLT","permalink":"/blog/2005/06/24/google-code-ajaxslt"},"nextItem":{"title":"Pushlets are adopting AJaX","permalink":"/blog/2005/06/13/pushlets-are-adopting-ajax"}},"content":"My Windows laptop has been infected by some viruses and adware -why I am not only using my Mac and Linux computers?-, so I just reinstalled the full OS.....\\n\\nI just want to use my blog to thanks the small utility [MozBackup](http://mozbackup.jasnapaka.com/) that allowed to quickly backup and recover my Firefox and Thunderbird configuration and data!\\n\\nThanks again..."},{"id":"/2005/06/13/pushlets-are-adopting-ajax","metadata":{"permalink":"/blog/2005/06/13/pushlets-are-adopting-ajax","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-13-pushlets-are-adopting-ajax.md","source":"@site/blog/2005-06-13-pushlets-are-adopting-ajax.md","title":"Pushlets are adopting AJaX","description":"Back in consulting I did some small projects using the Pushlets framework. A nice and simple solution to create easily Web eventing applications...","date":"2005-06-13T00:00:00.000Z","formattedDate":"June 13, 2005","tags":[{"label":"ajax","permalink":"/blog/tags/ajax"}],"readingTime":0.26,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Pushlets are adopting AJaX","tags":["ajax"]},"prevItem":{"title":"Thanks to MozBackup...","permalink":"/blog/2005/06/14/thanks-to-mozbackup-dot-dot-dot"},"nextItem":{"title":"JSR 198 Early Draft Open For Review","permalink":"/blog/2005/06/08/jsr-198-early-draft-open-for-review"}},"content":"![](http://www.pushlets.com/assets/pushlet-logo.gif)Back in consulting I did some small projects using the [Pushlets framework](http://www.pushlets.com). A nice and simple solution to create easily Web eventing applications...\\n\\nThe Pushlet team has created a small demonstration of integration of Pushlet and XMLHttpRequest together...\\n\\nIf you never used Pushlets, take a look to it it is really nice!"},{"id":"/2005/06/08/jsr-198-early-draft-open-for-review","metadata":{"permalink":"/blog/2005/06/08/jsr-198-early-draft-open-for-review","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-08-jsr-198-early-draft-open-for-review.md","source":"@site/blog/2005-06-08-jsr-198-early-draft-open-for-review.md","title":"JSR 198 Early Draft Open For Review","description":"The JSR-198:A Standard Extension API for Integrated Development Environment expert group has made the early draft available for review.","date":"2005-06-08T00:00:00.000Z","formattedDate":"June 8, 2005","tags":[],"readingTime":0.635,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JSR 198 Early Draft Open For Review"},"prevItem":{"title":"Pushlets are adopting AJaX","permalink":"/blog/2005/06/13/pushlets-are-adopting-ajax"},"nextItem":{"title":"Summary of Ajax frameworks","permalink":"/blog/2005/06/08/summary-of-ajax-frameworks"}},"content":"The [JSR-198:A Standard Extension API for Integrated Development Environment](http://http://jcp.org/en/jsr/detail?id=198) expert group has made the early draft available for [review](http://jcp.org/aboutJava/communityprocess/edr/jsr198/index.html).\\n\\nI really love this JSR and I am very happy that is now making progress because I played around in the past with Oracle JDeveloper Plugin APIs and I was disappointed that my work could not be used in other IDE...\\n\\n[Oracle JDeveloper 10g (10.1.3)](http://www.oracle.com/technology/products/jdev/index.html) API will be based on the JSR 198. Take a look the [Oracle JDeveloper Extension page](http://http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/index.html).\\n\\nOn my side I have to dig out the old extensions that I wrote for 9.0.3 and migrate them to 10.1.3, I will start with the small [Lejos](http://lejos.sourceforge.net/) extension, if I can find it... It has been so long since I did not play with my legos..."},{"id":"/2005/06/08/summary-of-ajax-frameworks","metadata":{"permalink":"/blog/2005/06/08/summary-of-ajax-frameworks","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-08-summary-of-ajax-frameworks.md","source":"@site/blog/2005-06-08-summary-of-ajax-frameworks.md","title":"Summary of Ajax frameworks","description":"Michael Maheomff has published on AjaxPatterns Web site an article that list existing Ajax frameworks. This AJAXFrameworks article talks about server side solutions (Java, PHP, .Net, Lisp, Ruby, ....) and client side solutions that provided helpers around XMLHttpRequests and HTML programming.","date":"2005-06-08T00:00:00.000Z","formattedDate":"June 8, 2005","tags":[],"readingTime":0.315,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Summary of Ajax frameworks"},"prevItem":{"title":"JSR 198 Early Draft Open For Review","permalink":"/blog/2005/06/08/jsr-198-early-draft-open-for-review"},"nextItem":{"title":"Steve Jobs\'  Keynote...","permalink":"/blog/2005/06/06/steve-jobs-keynote-dot-dot-dot"}},"content":"Michael Maheomff has published on [AjaxPatterns](http://www.ajaxpatterns.org) Web site an article that list existing Ajax frameworks. This [AJAXFrameworks article](http://www.ajaxpatterns.org/AJAXFrameworks) talks about server side solutions (Java, PHP, .Net, Lisp, Ruby, ....) and client side solutions that provided helpers around XMLHttpRequests and HTML programming.\\n\\nFeel free to let some comments to Michael on its wiki if he has forgotten some... for example nothing about JavaServer Faces..."},{"id":"/2005/06/06/steve-jobs-keynote-dot-dot-dot","metadata":{"permalink":"/blog/2005/06/06/steve-jobs-keynote-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-06-steve-jobs-keynote-dot-dot-dot.md","source":"@site/blog/2005-06-06-steve-jobs-keynote-dot-dot-dot.md","title":"Steve Jobs\'  Keynote...","description":"Today is an important day. We\u2019ve got some great stuff for you today.","date":"2005-06-06T00:00:00.000Z","formattedDate":"June 6, 2005","tags":[{"label":"apple","permalink":"/blog/tags/apple"}],"readingTime":0.7,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Steve Jobs\'  Keynote...","comments":true,"tags":["apple"]},"prevItem":{"title":"Summary of Ajax frameworks","permalink":"/blog/2005/06/08/summary-of-ajax-frameworks"},"nextItem":{"title":"Apple on Intel.. Xbox on PowerPC is the world getting crazy","permalink":"/blog/2005/06/05/apple-on-intel-dot-xbox-on-powerpc-is-the-world-getting-crazy"}},"content":"> Today is an important day. We\u2019ve got some great stuff for you today.\\n\\nHere we go, WWDC 2005 has started, and as usual it is hot!  You can have a summary of [Steve Jobs\' keynote on Macworld Web site](http://www.macworld.com/news/2005/06/06/liveupdate/index.php).\\n\\nI think the most breaking news, beside the good numbers is the confirmation of the fact that \\"[Apple drops IBM PowerPC line for Intel chips](http://www.macworld.com/news/2005/06/06/powerpcintel/index.php)\\"... What do you think? I am personally happy; this will stop the \'useless\' discussion about the speed of the chip of PC versus Mac... and, I am optimistic on the fact that Macs will be faster ;-) For me the chip is not important since the only code that I write on Mac is Java based, and little bit of AppleScript...\\n\\nI am impatient to see the new stuff that Apple will put in Leopard..."},{"id":"/2005/06/05/apple-on-intel-dot-xbox-on-powerpc-is-the-world-getting-crazy","metadata":{"permalink":"/blog/2005/06/05/apple-on-intel-dot-xbox-on-powerpc-is-the-world-getting-crazy","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-05-apple-on-intel-dot-xbox-on-powerpc-is-the-world-getting-crazy.md","source":"@site/blog/2005-06-05-apple-on-intel-dot-xbox-on-powerpc-is-the-world-getting-crazy.md","title":"Apple on Intel.. Xbox on PowerPC is the world getting crazy","description":"Once again a rumor of  Apple switching to Intel is making some noise on the net....","date":"2005-06-05T00:00:00.000Z","formattedDate":"June 5, 2005","tags":[{"label":"osx","permalink":"/blog/tags/osx"},{"label":"apple","permalink":"/blog/tags/apple"}],"readingTime":0.615,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Apple on Intel.. Xbox on PowerPC is the world getting crazy","tags":["osx","apple"]},"prevItem":{"title":"Steve Jobs\'  Keynote...","permalink":"/blog/2005/06/06/steve-jobs-keynote-dot-dot-dot"},"nextItem":{"title":"\'Scripting: Higher Level Programming for the 21st Century\' paper from  John K. Ousterhout","permalink":"/blog/2005/06/03/scripting-higher-level-programming-for-the-21st-century-paper-from-john-k-ousterhout"}},"content":"Once again a rumor of [ Apple switching to Intel](http://www.forbes.com/feeds/ap/2005/06/05/ap2077202.html) is making some noise on the net....\\n\\nWe won\'t have to wait for long since the news talks about Steve Job announcing that during his keynote at WWDC. I am currently planning to buy a new powerbook 12\\" (in addition to the 15\\" and iMac)... who knows, it may be an \\"Intel Inside\\" one ;-)\\n\\nThe other fun part is the deal that IBM is trying to obtain with game platform.. such as Sony and... MSFT...\\n\\nWe\'ll see tomorrow... Talking about WWDC, Oracle has a booth and some presentations for this event... I will be personally working on the pod presenting Oracle 10g stack on Mac OS X... See you this week !"},{"id":"/2005/06/03/scripting-higher-level-programming-for-the-21st-century-paper-from-john-k-ousterhout","metadata":{"permalink":"/blog/2005/06/03/scripting-higher-level-programming-for-the-21st-century-paper-from-john-k-ousterhout","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-06-03-scripting-higher-level-programming-for-the-21st-century-paper-from-john-k-ousterhout.md","source":"@site/blog/2005-06-03-scripting-higher-level-programming-for-the-21st-century-paper-from-john-k-ousterhout.md","title":"\'Scripting: Higher Level Programming for the 21st Century\' paper from  John K. Ousterhout","description":"With all the Java Scripting language around (Groovy, Rhino, Ruby, Jython... and so on) it is always interesting to understand where this come from... but also be able to talk with people about it, especially when they do not understand why scripting is interesting...","date":"2005-06-03T00:00:00.000Z","formattedDate":"June 3, 2005","tags":[{"label":"groovy","permalink":"/blog/tags/groovy"},{"label":"php","permalink":"/blog/tags/php"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"scripting","permalink":"/blog/tags/scripting"}],"readingTime":1.05,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"\'Scripting: Higher Level Programming for the 21st Century\' paper from  John K. Ousterhout","tags":["groovy","php","javascript","scripting"]},"prevItem":{"title":"Apple on Intel.. Xbox on PowerPC is the world getting crazy","permalink":"/blog/2005/06/05/apple-on-intel-dot-xbox-on-powerpc-is-the-world-getting-crazy"},"nextItem":{"title":"Good Bye JAX-RPC, weclome to JAX-WS","permalink":"/blog/2005/05/29/good-bye-jax-rpc-weclome-to-jax-ws"}},"content":"With all the Java Scripting language around (Groovy, Rhino, Ruby, Jython... and so on) it is always interesting to understand where this come from... but also be able to talk with people about it, especially when they do not understand why scripting is interesting...\\n\\nGoogling around looking for some information on scripting benefits for a presentation I found this very intersting article from John Ousterhout, (he wrote the article in 1998) but I am sure you will still appreciate it...\\n\\n[Scripting: Higher Level Programming for the 21st Century](http://home.pacbell.net/ouster/scripting.html).\\n\\nOne of the most interesting part in this context is the paragraph 4:\\n> A scripting language is not a replacement for a system programming language or vice versa. Each is suited to a different set of tasks. For gluing and system integration, applications can be developed 5-10x faster with a scripting language; system programming languages will require large amounts of boilerplate and conversion code to connect the pieces, whereas this can be done directly with a scripting language. For complex algorithms and data structures, the strong typing of a system programming language makes programs easier to manage. Where execution speed is key, a system programming language can often run 10-20x faster than a scripting language because it makes fewer run-time checks."},{"id":"/2005/05/29/good-bye-jax-rpc-weclome-to-jax-ws","metadata":{"permalink":"/blog/2005/05/29/good-bye-jax-rpc-weclome-to-jax-ws","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-05-29-good-bye-jax-rpc-weclome-to-jax-ws.md","source":"@site/blog/2005-05-29-good-bye-jax-rpc-weclome-to-jax-ws.md","title":"Good Bye JAX-RPC, weclome to JAX-WS","description":"Finally the Java specification about Web Service will  have a name that makes sense... moving away from JAX-RPC to JAX-WS (JAX Web Services)...","date":"2005-05-29T00:00:00.000Z","formattedDate":"May 29, 2005","tags":[{"label":"webservices","permalink":"/blog/tags/webservices"},{"label":"soap","permalink":"/blog/tags/soap"},{"label":"java","permalink":"/blog/tags/java"},{"label":"javaee","permalink":"/blog/tags/javaee"}],"readingTime":0.435,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Good Bye JAX-RPC, weclome to JAX-WS","tags":["webservices","soap","java","javaee"]},"prevItem":{"title":"\'Scripting: Higher Level Programming for the 21st Century\' paper from  John K. Ousterhout","permalink":"/blog/2005/06/03/scripting-higher-level-programming-for-the-21st-century-paper-from-john-k-ousterhout"},"nextItem":{"title":"Leaving for India...","permalink":"/blog/2005/05/15/leaving-for-india-dot-dot-dot"}},"content":"Finally the Java specification about Web Service will  have a name that makes sense... moving away from JAX-RPC to JAX-WS (JAX Web Services)...\\n\\n> The JAX-RPC name, which stands for Java API for XML-based RPC, is misleading because developers assume it is only about RPC, according to Doug Kohlert, a Sun staff engineer, in his blog this week. \u201cBy renaming JAX-RPC to JAX-WS, we can eliminate this confusion,\u201d Kohlert wrote. JAX-WS stands for Java API for XML Web Services.\\n\\n\\n[This infoworld article will give you more details](http://www.infoworld.com/article/05/05/25/HNjaxrpc_1.html)."},{"id":"/2005/05/15/leaving-for-india-dot-dot-dot","metadata":{"permalink":"/blog/2005/05/15/leaving-for-india-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-05-15-leaving-for-india-dot-dot-dot.md","source":"@site/blog/2005-05-15-leaving-for-india-dot-dot-dot.md","title":"Leaving for India...","description":"Tomorrow, I am leaving for India where I will present Oracle Application Server 10g and SOA to some of our partners. (J2EE, Web Services, BPEL and all put together!)","date":"2005-05-15T00:00:00.000Z","formattedDate":"May 15, 2005","tags":[],"readingTime":0.275,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Leaving for India..."},"prevItem":{"title":"Good Bye JAX-RPC, weclome to JAX-WS","permalink":"/blog/2005/05/29/good-bye-jax-rpc-weclome-to-jax-ws"},"nextItem":{"title":"Oracle SQL Developer Express","permalink":"/blog/2005/05/12/oracle-sql-developer-express"}},"content":"Tomorrow, I am leaving for India where I will present Oracle Application Server 10*g* and SOA to some of our partners. (J2EE, Web Services, BPEL and all put together!)\\n\\nI will be in Bangalore and Calcutta, for 2 weeks, so if you have good place to go in this cities let me know tugdual[at]grallandco[dot]com !"},{"id":"/2005/05/12/oracle-sql-developer-express","metadata":{"permalink":"/blog/2005/05/12/oracle-sql-developer-express","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-05-12-oracle-sql-developer-express.md","source":"@site/blog/2005-05-12-oracle-sql-developer-express.md","title":"Oracle SQL Developer Express","description":"You are a database developer, and want to develop, test and debug PL/SQL routines. You are a Java, PHP or .Net developer and you want to browse your schemas... Oracle has the tool for you: Oracle SQL Developer Express is a free","date":"2005-05-12T00:00:00.000Z","formattedDate":"May 12, 2005","tags":[{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle SQL Developer Express","tags":["oracle"]},"prevItem":{"title":"Leaving for India...","permalink":"/blog/2005/05/15/leaving-for-india-dot-dot-dot"},"nextItem":{"title":"Web UI: What\'s behind AJAX? Simple use cases using XMLHttpRequest","permalink":"/blog/2005/05/09/web-ui-whats-behind-ajax-simple-use-cases-using-xmlhttprequest"}},"content":"You are a database developer, and want to develop, test and debug PL/SQL routines. You are a Java, PHP or .Net developer and you want to browse your schemas... Oracle has the tool for you: Oracle SQL Developer Express is a free"},{"id":"/2005/05/09/web-ui-whats-behind-ajax-simple-use-cases-using-xmlhttprequest","metadata":{"permalink":"/blog/2005/05/09/web-ui-whats-behind-ajax-simple-use-cases-using-xmlhttprequest","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-05-09-web-ui-whats-behind-ajax-simple-use-cases-using-xmlhttprequest.md","source":"@site/blog/2005-05-09-web-ui-whats-behind-ajax-simple-use-cases-using-xmlhttprequest.md","title":"Web UI: What\'s behind AJAX? Simple use cases using XMLHttpRequest","description":"I am sure you already heard about AJAX, the \\"new\\" way of building Web","date":"2005-05-09T00:00:00.000Z","formattedDate":"May 9, 2005","tags":[{"label":"ajax","permalink":"/blog/tags/ajax"}],"readingTime":8.68,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Web UI: What\'s behind AJAX? Simple use cases using XMLHttpRequest","tags":["ajax"]},"prevItem":{"title":"Oracle SQL Developer Express","permalink":"/blog/2005/05/12/oracle-sql-developer-express"},"nextItem":{"title":"Which kind of developer are you? A software terrorist?","permalink":"/blog/2005/05/03/which-kind-of-developer-are-you-a-software-terrorist"}},"content":"I am sure you already heard about AJAX, the \\"new\\" way of building Web\\napplication where using HTML, Javascript, DOM, and XML you can\\n&nbsp;build very interactive Web applications, some examples of\\nthis interactivity are Google (suggest, local, mail), MSN, Amazon, .....\\n\\nIf the term &nbsp;Ajax is\\nrelatively new, the technologies used are quite old now. And you do not\\nneed anything new to be able to leverage this approach in your current\\ndevelopment, and since it is mainly a client (browser) based technology\\nthis can be used whether you develop your application in Java, PHP, or\\nor even PL/SQL for the Web. In the same time, you do not want to have\\nto develop too much of the client side to handle user interaction, this\\nis where new frameworks that provide support of this technologies,\\nlet\'s say AJAX based, are very interesting. It is now possible to\\ncreate very interactive Web application without passing 80% of your\\ntime in the client side development - that could be generated from your\\nserver obviously.\\n\\nIn the same time, before adopting such framework, it is important to\\nunderstand what is happening. This is why I am creating this quick\\nintroduction based on 2 simple use cases:\\n\\n* [loading message](http://www.grallandco.com/blog/archives/ajax/demo-2.html):\\n&nbsp;you often want to provide some\\nfeedback to your user when your browser is processing some data\\n* [dependent lists](http://www.grallandco.com/blog/archives/ajax/demo-3.html):\\nhow you can easily create dependent lists, in lot of application you\\nwant to drive a selection list from another\\n\\n\x3c!-- truncate --\x3e\\n\\nUpdate May 30:\\n\\nIf you have tested my sample on Safari, you can see that the first request works, but following subsequent requests do not, on work around is to modify the HTTP header adding the following value, has to be done before the send, just send a old date...\\n\\n``` javascript\\nhttpRequest.setRequestHeader(\'If-Modified-Since\',\'Sun,3 Jun 1973 00:00:00 GMT\');\\n```\\n\\n### What do you need to know?\\n\\nThe first interesting part is the object XMLHttpRequest. This object is\\ncurrently supported by most of the browsers. As usual with HTML and\\nJavascript the cross browser/platform testing is one of the most\\nimportant part of your project. XMLHttpRequest is not new since\\nMicrosoft has implemented it in Internet Explorer 5 as an ActiveX\\nobject. Also it has been later on integrate as a native component into\\nMozzila and Netscape (7.0), and into Apple Safari. If originally this\\nobject as been created to load data, as XML in the\\nbackground, you can using it to load any data.\\n\\n#### Creating XMLHttpRequest Object\\n\\nAs usual with Javascript, it begins with interoperability issue ;-),\\nsince in the MSFT world the XMLHttpRequest is implemented as an ActiveX\\nand in the other world(s) it is an native object, you have different\\nways to create the object. The idea is too test if the current browser\\nsupports or not the object and depending of the result create the\\nobject.\\n\\n``` javascript\\n\\nif (window.ActiveXObject)\\n{\\n  // Microsoft Way\\n  httpRequest = new ActiveXObject(\\"Microsoft.XMLHTTP\\");\\n}\\nelse if (window.XMLHttpRequest)\\n{\\n  // Others...\\n  httpRequest = new XMLHttpRequest();\\n}\\n\\n```\\n\\nNote: another way of choosing which code path to use depending of the\\nversion of the browser is to use [conditional compilation](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/script56/html/js56jsconconditionalcompilation.asp)\\nfor the Javascript engine.\\n\\nAlso I would add that it is probably good to provide a degraded version\\nof your application if you can not control which browser will be used\\nby your client, or if you want to provide that to mobile people. This\\nis for example what Google Mail is doing with its [Basic HTML](http://gmail.google.com/support/bin/answer.py?ctx=gmail&amp;hl=en&amp;answer=15049)\\nview.\\n\\n#### Retrieving the data\\nasynchronously\\n\\nOne of the most interesting features of the XMLHttpRequestObject is the\\nfact that you get data from a URL in the background of the user\\nactivity. This is how you can give lot of interactivity to your\\napplications.\\n\\n``` javascript\\nhttpRequest.open(\\"GET\\", \\"./getEmployeeList?deptno=10\\", true);\\nhttpRequest.onreadystatechange= function () {processRequest(); } ;\\nhttpRequest.send(null);\\n```\\n\\nThe [open](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmmthopenixmlhttprequest.asp)\\nmethod sets the different values that will be used by your request,\\nsuch as the type of request, the URL, if it is synchronous or not.\\n\\nThe \\"magic\\" comes from the&nbsp;[onreadystatechange](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmproonreadystatechangehttprequest.asp)&nbsp;property.\\nThis propery set an handler that will be called when the property [readyState](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmproreadystaterequest.asp)\\nchanges, when the request is sent, loading, or completed... Meaning,\\nthis is in this event handler, the processRequest\\nmethod in the sample, that you test if the response has been completed\\ncorrectly, and process the data.\\n\\nNote: even if it is possible, I do not think that you should use the\\nXMLHttpRequest in a synchronous mode since it will freeze the browser\\nof the user, for the duration of the request.\\n\\n#### Processing the response\\n\\nAs I said above the magic happens in the event handler associated with\\nthe state of the request. Let\'s see how you manipulate the request and\\nresponse:\\n\\n\\n``` javascript\\nfunction processRequest()\\n{\\n  if (httpRequest.readyState == 4)\\n  {\\n    if(httpRequest.status == 200)\\n    {\\n      // process data as XML\\n      httpRequest.responseXML;\\n\\n      // or Text\\n      alert(httpRequest.responseText);\\n    }\\n    else\\n    {\\n      alert(\\"Error loading page\\\\n\\"+ httpRequest.status +\\":\\"+ httpRequest.statusText);\\n    }\\n  }\\n}\\n\\n```\\n\\n\\nThe [readyState](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmproreadystaterequest.asp)\\npropery is used to know if all the data of the response have been\\nreceived or not, the different value for the state are:\\n\\n* 0 : uninitialized\\n* 1 : loading\\n* 2 :loaded\\n* 3 : interactive\\n* 4 : completed\\nMost of the time, you will just test the status 4. In my example I wait\\nfor status==4 and then I check the status of the response. The [status](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmprostatusixmlhttprequest.asp)\\nis the status returned by the server, for example 200 for OK, 404 for\\nnot found.\\n\\nThen you can treat the response as XML using the [responseXML](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmproresponsexmlixmlhttprequest.asp),\\nin this case you can manipulate the XML using the client DOM API. But\\nyou can also directly take the content as a string using the [responseText](http://msdn.microsoft.com/library/default.asp?url=/library/en-us/xmlsdk/html/xmproresponsetextixmlhttprequest.asp)\\nproperty.\\n\\n#### Manipulating the Data\\n\\nYou now have the data back from the request, as Text or XML, you just\\nneed using Javascript manipulate them and copy then into some part of\\nyour page. So all is based on the client side. Let\'s do a very simple\\nexample before moving to the use cases:\\n\\n* [Simple XMLHttpRequest sample](http://www.grallandco.com/blog/archives/demo-1.html) *What is happening?*\\n\\nSince I have explained the call above, here is the simple code that is\\nused to copy the content from the XMLHttpRequest object to the page:\\n\\n``` javascript\\n.....\\nif(httpRequest.status == 200)\\n{\\n  var contentViewer = document.getElementById(\\"contentViewer\\");\\n  contentViewer.innerHTML = httpRequest.responseText;\\n}\\n....\\n```\\n\\n\\nMost of the time, if you do &nbsp;not use XML you just have to copy\\nthe content of the result into a section of your page, so to do it you\\nuse:\\n\\n* create a DIV tag in yourpage with a specific id. (or other...)\\n* get the object you want to copy into using the `document.getElementById()` method, that is the most simple way...&nbsp;\\n* then from the object just use the `innerHTML`\\nproperty that represent the content of this DIV object. (You can also\\n  use the DOM API, to do the same thing, I am sure puris will say that\\n  this is the only way to do it correctly... )\\n  In most of the frameworks the data are exchanged in XML and some\\n  generic methods are calls on the client side to propulate data in the\\n  different components of the page.\\n\\n### Usecase 1: Loading Message\\n\\nOne common use case is to be able to have some feedback during\\n  processing of a request. Using the same approach you can print a\\n  message until the response is completely done.\\n\\n* [Online Demonstration](http://www.grallandco.com/blog/archives/ajax/demo-2.html) *What is happening?*\\n\\nThe tip here is just to take a zone where you want to print the status\\n  (\\"Loading...\\" for example). When the XMLHttpRequest status is different\\n  than 4 (completed) &nbsp;just print the status in this section\\n  using for example the innerHTML property. When the request is\\n  completed, just move the content of this zone to blank, or in our case\\n  the result of the request is printed into it.\\n\\n``` javascript\\n....\\nif (httpRequest.readyState == 4)\\n{\\n  if(httpRequest.status == 200)\\n  {\\n    // print the content of the page\\n    var contentViewer = document.getElementById(\\"contentViewer\\");\\n    contentViewer.innerHTML = httpRequest.responseText;\\n  }\\n}\\nelse\\n{\\n  // Print the loading message....\\n  var contentViewer = document.getElementById(\\"contentViewer\\");\\n  contentViewer.innerHTML = \\"<b style=\'color:red;\'>Loading....<b>\\";\\n}\\n...\\n```\\n\\n### Usecase 2: Dependent Lists\\n\\nIn lot of applications, you have submission form when one drop down list is controlled by another one. You have several way to support that depending of the size of your data set. For example you can download\\n  all the records (master and detail) and use some client side\\n  programming to refresh the content of the detail list depending of the\\n  value selected in the master one; or &nbsp;another solution is to\\n  just reload the page.\\n\\nUsing XMLHttpRequest, you can now just call the server to populate the\\n  values of the detail list when the user select another value in the\\n  master list. (and you can implement a client side cache if you want to\\n  avoid a second call with the same parameters, but this is another\\n  story...)\\n\\n* [Online Demonstration](http://www.grallandco.com/blog/archives/ajax/demo-3.html) *What is happening?*\\n\\nNothing more complex than in the previous examples, on the `onChange`\\n  of the master list (dept in the example) , you call a Javascript method\\n  that create the XMLHttRequest, with the correct parameter.\\n\\nThen in the event handler of this request, you populate the data as XML\\n  and add them to the detail list (emp in the example).\\n\\nIn this specific example I used XML, but if you want for simpler code\\n  you can write all in HTML and use the innerHTML property, but this is\\n  just a choice of implementation isn\'t?\\n\\n### Summary\\n\\n\\nFirst of all, you have seen that what we have behind AJAX is not just\\n  new technologies. The technologies exists now for a while, they have\\n  been exposed to the masse by Google, MSN, and other Web sites. The term\\n  itself is definitevely a recent one...\\n\\nAlso, I have showed you simple examples that can help you for your\\n  current developments, but you need to write some code on the client.\\n  The real future of AJAX, is when this is bundle into a framework, into\\n  the Faces components your are using. So before starting to implement\\n  such code in your application you should take a look around to see if\\n  any solution exists that can answer your need...\\n\\n  You can download the source of the demonstrations [here](http://www.grallandco.com/blog/archives/ajax/xmlhttprequest-demos.zip). I tried to keep the code as simple as possible by providing all the source in each single html file."},{"id":"/2005/05/03/which-kind-of-developer-are-you-a-software-terrorist","metadata":{"permalink":"/blog/2005/05/03/which-kind-of-developer-are-you-a-software-terrorist","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-05-03-which-kind-of-developer-are-you-a-software-terrorist.md","source":"@site/blog/2005-05-03-which-kind-of-developer-are-you-a-software-terrorist.md","title":"Which kind of developer are you? A software terrorist?","description":"Allen Holub just published on SDTime an editorial named \\"The Terror of Code in the Wrong Hands\\". I like the description of the \\"software terrorist\\": the guy who stays up all night, unwittingly but systematically destroying the entire team\u2019s last month\u2019s work while \u201cimproving\u201d the code. He doesn\u2019t tell anybody what he\u2019s done, and he never tests. He\u2019s created a ticking time bomb that won\u2019t be discovered for six months.","date":"2005-05-03T00:00:00.000Z","formattedDate":"May 3, 2005","tags":[],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Which kind of developer are you? A software terrorist?"},"prevItem":{"title":"Web UI: What\'s behind AJAX? Simple use cases using XMLHttpRequest","permalink":"/blog/2005/05/09/web-ui-whats-behind-ajax-simple-use-cases-using-xmlhttprequest"},"nextItem":{"title":"J2EE application development for Forms and Designer developers","permalink":"/blog/2005/04/28/j2ee-application-development-for-forms-and-designer-developers"}},"content":"[Allen Holub](http://www.holub.com/) just published on SDTime an editorial named \\"[The Terror of Code in the Wrong Hands](http://68.236.189.240/fullcolumn/column-20050501-01.html)\\". I like the description of the \\"_software terrorist_\\": the guy who stays up all night, unwittingly but systematically destroying the entire team\u2019s last month\u2019s work while \u201cimproving\u201d the code. He doesn\u2019t tell anybody what he\u2019s done, and he never tests. He\u2019s created a ticking time bomb that won\u2019t be discovered for six months.\\n\\nI am sure we all have a small list of co-worker that we put in this category... Hoping that I am not in yours :-)"},{"id":"/2005/04/28/j2ee-application-development-for-forms-and-designer-developers","metadata":{"permalink":"/blog/2005/04/28/j2ee-application-development-for-forms-and-designer-developers","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-28-j2ee-application-development-for-forms-and-designer-developers.md","source":"@site/blog/2005-04-28-j2ee-application-development-for-forms-and-designer-developers.md","title":"J2EE application development for Forms and Designer developers","description":"Back in France, and in Oracle consulting, I was very often called to present, introduce J2EE to Oracle Forms developers, since we all know that J2EE is the way to go!","date":"2005-04-28T00:00:00.000Z","formattedDate":"April 28, 2005","tags":[],"readingTime":0.885,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"J2EE application development for Forms and Designer developers","categories":["java","javaee"]},"prevItem":{"title":"Which kind of developer are you? A software terrorist?","permalink":"/blog/2005/05/03/which-kind-of-developer-are-you-a-software-terrorist"},"nextItem":{"title":"Web Services and SOA Web Site","permalink":"/blog/2005/04/28/web-services-and-soa-web-site"}},"content":"Back in France, and in Oracle consulting, I was very often called to present, introduce J2EE to Oracle Forms developers, since we all know that J2EE is the way to go!\\n\\nSue Harper and Grant Ronald have made this task easier by creating a specific OTN page for [Oracle Forms and Designer Developers](http://otn.oracle.com/formsdesignerj2ee) that are moving to J2EE.\\n\\nThis is also a subject that will be often discussed in the [J2EE SIG](http://www.odtug.com/2005_j2EE.htm) that the different Oracle user groups\\n[ODTUG](http://www.odtug.com), [IOUG](http://www.ioug.org),[OAUG](http://www.oaug.com) and [Oracle](http://otn.oracle.com) have created.\\n\\nI am sure that will be a very succesful subject during the next [ODTUG (Oracle Developer Tools User Group) Conference](http://www.odtug.com/2005_conference_home.asp) mid june in New Orleans. During this event I am presenting 3 papers:\\n* J2EE 1.4 Overview\\n* Oracle Application Server 10g: Best Application Server for the Oracle Database\\n* J2EE Persistence Using OracleAS TopLink ( will probably let our guru Doug Clarke doing it if he is coming...)\\n\\nIt is still time to register for this event, [take a look to the agenda](http://www.odtug.com/2005_conference_agenda.htm), this year will be a very good one !"},{"id":"/2005/04/28/web-services-and-soa-web-site","metadata":{"permalink":"/blog/2005/04/28/web-services-and-soa-web-site","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-28-web-services-and-soa-web-site.md","source":"@site/blog/2005-04-28-web-services-and-soa-web-site.md","title":"Web Services and SOA Web Site","description":"I was yesterday talking to a customer about Web Services and Services Oriented Architecture, and he was looking for more information. So I shared with him one of my bookmark.","date":"2005-04-28T00:00:00.000Z","formattedDate":"April 28, 2005","tags":[],"readingTime":0.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Web Services and SOA Web Site"},"prevItem":{"title":"J2EE application development for Forms and Designer developers","permalink":"/blog/2005/04/28/j2ee-application-development-for-forms-and-designer-developers"},"nextItem":{"title":"A very good HTML Editor... and open source","permalink":"/blog/2005/04/26/a-very-good-html-editor-dot-dot-dot-and-open-source"}},"content":"I was yesterday talking to a customer about Web Services and Services Oriented Architecture, and he was looking for more information. So I shared with him one of my bookmark.\\n\\n[http://www.service-architecture.com/](http://www.service-architecture.com/) is a Web site dedicated to WS and SOA with [lof of articles](http://www.service-architecture.com/articles/index.html) that defines for example most of the WS-* acronyms.\\n\\n* [Web Services Articles](http://www.service-architecture.com/web-services/articles/)\\n* [XML Articles](http://www.service-architecture.com/xml/articles/)\\n* [Application Server Article](http://www.service-architecture.com/application-servers/articles/index.html)"},{"id":"/2005/04/26/a-very-good-html-editor-dot-dot-dot-and-open-source","metadata":{"permalink":"/blog/2005/04/26/a-very-good-html-editor-dot-dot-dot-and-open-source","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-26-a-very-good-html-editor-dot-dot-dot-and-open-source.md","source":"@site/blog/2005-04-26-a-very-good-html-editor-dot-dot-dot-and-open-source.md","title":"A very good HTML Editor... and open source","description":"I have started to use on my Mac and PC the free HTML editor NVU, it is really a great product!","date":"2005-04-26T00:00:00.000Z","formattedDate":"April 26, 2005","tags":[],"readingTime":0.545,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"A very good HTML Editor... and open source"},"prevItem":{"title":"Web Services and SOA Web Site","permalink":"/blog/2005/04/28/web-services-and-soa-web-site"},"nextItem":{"title":"Mike Keith Blog: The EJB 3.0 Hibernate Fallacy","permalink":"/blog/2005/04/21/mike-keith-blog-the-ejb-3-dot-0-hibernate-fallacy"}},"content":"I have started to use on my Mac and PC the free HTML editor [NVU](http://nvu.com), it is really a great product!\\n\\nKey Features:\\n\\n* Multi Plaform: MacOS X, Linux, Windows... the project has been started by [LinSpire](http://www.linspire.com/)... so not a big surprise\\n* FTP Site manager\\n* Good CSS Editor and Integration to the HTML Editor\\n* Form Editing\\n* Based on Gecko\\n\\nNothing revolutionnary compare with DreamWeaver or FrontPage.. except that it is still very powerfull and free...\\n\\nI hope that some Open Source developers will take time to develop Dav Site management, Source Control integration, JSP Development (at least same as the PHP stuff it currently has...)"},{"id":"/2005/04/21/mike-keith-blog-the-ejb-3-dot-0-hibernate-fallacy","metadata":{"permalink":"/blog/2005/04/21/mike-keith-blog-the-ejb-3-dot-0-hibernate-fallacy","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-21-mike-keith-blog-the-ejb-3-dot-0-hibernate-fallacy.md","source":"@site/blog/2005-04-21-mike-keith-blog-the-ejb-3-dot-0-hibernate-fallacy.md","title":"Mike Keith Blog: The EJB 3.0 Hibernate Fallacy","description":"Mike Keith, Oracle Toplink architect, persistence guru, and member of the EJB 3.0 Expert Group, gives an, in his last entry an overview of EJB 3.0, in the context of the comparison with Hibernate and especially explaining why the statement that we sometimes hear \\"EJB 3.0 is Hibernate\\" is  wrong.","date":"2005-04-21T00:00:00.000Z","formattedDate":"April 21, 2005","tags":[{"label":"java","permalink":"/blog/tags/java"},{"label":"javaEE","permalink":"/blog/tags/java-ee"},{"label":"jpa","permalink":"/blog/tags/jpa"},{"label":"ejb","permalink":"/blog/tags/ejb"}],"readingTime":0.555,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Mike Keith Blog: The EJB 3.0 Hibernate Fallacy","tags":["java","javaEE","jpa","ejb"]},"prevItem":{"title":"A very good HTML Editor... and open source","permalink":"/blog/2005/04/26/a-very-good-html-editor-dot-dot-dot-and-open-source"},"nextItem":{"title":"Free Subversion Online Book","permalink":"/blog/2005/04/19/free-subversion-online-book"}},"content":"Mike Keith, Oracle Toplink architect, persistence guru, and member of the EJB 3.0 Expert Group, gives an, [in his last entry](http://www.jroller.com/comments/mkeith/Weblog/the_ejb_3_0_hibernate) an overview of EJB 3.0, in the context of the comparison with Hibernate and especially explaining why the statement that we sometimes hear \\"EJB 3.0 _is_ Hibernate\\" is  wrong.\\n\\nThis blog entry as been selected as a news by The Server Side, titled \\"[EJB 3.0 **is not** Hibernate](http://theserverside.com/news/thread.tss?thread_id=33450)\\" and is very active in term of comments/reaction, feel free to add yours there.\\n\\nIn the same time if you want to learn more about EJB 3.0 and start developing with it you can find infromation and EJB 3.0 container on [OTN](http://otn.oracle.com/ejb3)."},{"id":"/2005/04/19/free-subversion-online-book","metadata":{"permalink":"/blog/2005/04/19/free-subversion-online-book","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-19-free-subversion-online-book.md","source":"@site/blog/2005-04-19-free-subversion-online-book.md","title":"Free Subversion Online Book","description":"You can find a online version of the book Version Control with Subversion. Thanks to the authors Ben Collins-Sussman, Brian W. Fitzpatrick,C. Michael Pilato, and their publisher O\'Reilly.","date":"2005-04-19T00:00:00.000Z","formattedDate":"April 19, 2005","tags":[],"readingTime":0.14,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Free Subversion Online Book"},"prevItem":{"title":"Mike Keith Blog: The EJB 3.0 Hibernate Fallacy","permalink":"/blog/2005/04/21/mike-keith-blog-the-ejb-3-dot-0-hibernate-fallacy"},"nextItem":{"title":"Infoworld about OracleAS 10gR2: Write once","permalink":"/blog/2005/04/12/infoworld-about-oracleas-10gr2-write-once"}},"content":"You can find a online version of the book [Version Control with Subversion](http://svnbook.red-bean.com/). Thanks to the authors Ben Collins-Sussman, Brian W. Fitzpatrick,C. Michael Pilato, and their publisher [O\'Reilly](http://www.oreilly.com/catalog/0596004486/)."},{"id":"/2005/04/12/infoworld-about-oracleas-10gr2-write-once","metadata":{"permalink":"/blog/2005/04/12/infoworld-about-oracleas-10gr2-write-once","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-12-infoworld-about-oracleas-10gr2-write-once.md","source":"@site/blog/2005-04-12-infoworld-about-oracleas-10gr2-write-once.md","title":"Infoworld about OracleAS 10gR2: Write once","description":"As you can guess, when you work on a product it is always a pleasure to see good feedback, from end users, or from the media...","date":"2005-04-12T00:00:00.000Z","formattedDate":"April 12, 2005","tags":[],"readingTime":1.295,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Infoworld about OracleAS 10gR2: Write once"},"prevItem":{"title":"Free Subversion Online Book","permalink":"/blog/2005/04/19/free-subversion-online-book"},"nextItem":{"title":"JavaPolis Survey: Which Persistence...","permalink":"/blog/2005/04/08/javapolis-survey-which-persistence-dot-dot-dot"}},"content":"As you can guess, when you work on a product it is always a pleasure to see good feedback, from end users, or from the media...\\n\\nTom Yager, from InfoWorld has published an [article about Oracle Application Server 10g R2](http://www.infoworld.com/article/05/04/11/15TCoracle_1.html).\\n\\nHere some of the comments:\\n> \u201cI\u2019m extremely impressed with AS 10g Release 2 as a commercial product. Oracle has designed a valuable feature set built on top of J2EE 1.3, optimizing it for SOAs (service-oriented architectures), BPEL (Business Process Execution Language), BPM, pervasive support of XML, and real-time business intelligence, among many, many other things.\u201d\\n\\n\\n> \u201cBut I\u2019m just as impressed with the clear emphasis Oracle has placed on complying not only with the letter of Java server standards, but with their \u201cwrite once, run anywhere\u201d spirit. You get to choose whether you want to bind yourself to Oracle\u2019s extras. AS 10g Release 2 permits, and even automates, deployment to and management of non-Oracle Java app servers that adhere to the J2EE 1.3 standard. And if you do this, there are no subtle glitches meant to prod you toward paying for Oracle\u2019s server.\u201d\\n\\n\\n> \u201cAfter several weeks of living with AS 10g Release 2, I found that Oracle\u2019s added value is nothing short of spectacular for enterprise applications and well worth paying for.\u201d\\n\\n\\n> \u201cThe myth of \u201cwrite once, run anywhere\u201d has been turned on its head. With AS 10g Release 2, Oracle has delivered on that age-old promise as well as a software giant can.\u201d\\n\\n\\nFeel free to drop me comments about your experience with OracleAS 10g..."},{"id":"/2005/04/08/javapolis-survey-which-persistence-dot-dot-dot","metadata":{"permalink":"/blog/2005/04/08/javapolis-survey-which-persistence-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-08-javapolis-survey-which-persistence-dot-dot-dot.md","source":"@site/blog/2005-04-08-javapolis-survey-which-persistence-dot-dot-dot.md","title":"JavaPolis Survey: Which Persistence...","description":"JavaPolis is offering a survey about Persistence Technology, if you have not voted yet go on JavaPolis Web site.","date":"2005-04-08T00:00:00.000Z","formattedDate":"April 8, 2005","tags":[],"readingTime":0.095,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaPolis Survey: Which Persistence..."},"prevItem":{"title":"Infoworld about OracleAS 10gR2: Write once","permalink":"/blog/2005/04/12/infoworld-about-oracleas-10gr2-write-once"},"nextItem":{"title":"yagoohoogle : the best of both world.. in a simple way","permalink":"/blog/2005/04/06/yagoohoogle-the-best-of-both-world-dot-in-a-simple-way"}},"content":"JavaPolis is offering a survey about Persistence Technology, if you have not voted yet go on [JavaPolis Web site](http://www.javapolis.com)."},{"id":"/2005/04/06/yagoohoogle-the-best-of-both-world-dot-in-a-simple-way","metadata":{"permalink":"/blog/2005/04/06/yagoohoogle-the-best-of-both-world-dot-in-a-simple-way","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-04-06-yagoohoogle-the-best-of-both-world-dot-in-a-simple-way.md","source":"@site/blog/2005-04-06-yagoohoogle-the-best-of-both-world-dot-in-a-simple-way.md","title":"yagoohoogle : the best of both world.. in a simple way","description":"A friend of mine send me this funny and useful link http://www.yagoohoogle.com/...","date":"2005-04-06T00:00:00.000Z","formattedDate":"April 6, 2005","tags":[],"readingTime":0.08,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"yagoohoogle : the best of both world.. in a simple way"},"prevItem":{"title":"JavaPolis Survey: Which Persistence...","permalink":"/blog/2005/04/08/javapolis-survey-which-persistence-dot-dot-dot"},"nextItem":{"title":"Simplified Development using EJB 3.0: Raghu Kodali tested it for us","permalink":"/blog/2005/03/31/simplified-development-using-ejb-3-dot-0-raghu-kodali-tested-it-for-us"}},"content":"A friend of mine send me this funny and useful link [http://www.yagoohoogle.com/](http://www.yagoohoogle.com/)...\\n\\nJust simple and neat."},{"id":"/2005/03/31/simplified-development-using-ejb-3-dot-0-raghu-kodali-tested-it-for-us","metadata":{"permalink":"/blog/2005/03/31/simplified-development-using-ejb-3-dot-0-raghu-kodali-tested-it-for-us","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-31-simplified-development-using-ejb-3-dot-0-raghu-kodali-tested-it-for-us.md","source":"@site/blog/2005-03-31-simplified-development-using-ejb-3-dot-0-raghu-kodali-tested-it-for-us.md","title":"Simplified Development using EJB 3.0: Raghu Kodali tested it for us","description":"Raghu just posted an interesting article asking the question \\"Does EJB 3.0 really make application development easy?\\".","date":"2005-03-31T00:00:00.000Z","formattedDate":"March 31, 2005","tags":[{"label":"java","permalink":"/blog/tags/java"},{"label":"javaEE","permalink":"/blog/tags/java-ee"},{"label":"ejb","permalink":"/blog/tags/ejb"},{"label":"jpa","permalink":"/blog/tags/jpa"}],"readingTime":0.685,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Simplified Development using EJB 3.0: Raghu Kodali tested it for us","tags":["java","javaEE","ejb","jpa"]},"prevItem":{"title":"yagoohoogle : the best of both world.. in a simple way","permalink":"/blog/2005/04/06/yagoohoogle-the-best-of-both-world-dot-in-a-simple-way"},"nextItem":{"title":"What to think about all the Ajax noise?","permalink":"/blog/2005/03/28/what-to-think-about-all-the-ajax-noise"}},"content":"Raghu just posted an interesting article asking the question \\"[Does EJB 3.0 really make application development easy?](http://www.jroller.com/comments/raghukodali/Weblog/does_ejb_3_0_really)\\".\\n\\nSo as you will see EJB 3.0 does simplify the development by reducing the number of source and descriptor files that you have to manipulate. (I won\'t go in the detail of the number of lines of Java or XML). What I really like about EJB 3.0 is not only the fact that Entity bean are POJOs, but also that now you can read the code and understand how the application is build, thanks to the annotations! I am sure that if you got the EJB 2.x genes you do not find them complex, but I did not have this mutation...\\n\\nI am inviting you to download the [Oracle EJB 3.0 Developer Preview](http://otn.oracle.com/ejb3) to be more familiar with this technology."},{"id":"/2005/03/28/what-to-think-about-all-the-ajax-noise","metadata":{"permalink":"/blog/2005/03/28/what-to-think-about-all-the-ajax-noise","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-28-what-to-think-about-all-the-ajax-noise.md","source":"@site/blog/2005-03-28-what-to-think-about-all-the-ajax-noise.md","title":"What to think about all the Ajax noise?","description":"In the last 2 months the blogosphere has been very verbal about Ajax technology that describe a way of doing Rich UI using DHTML, Javascript and XML over HTTP.  If you are not already familiar with the concept the article \\"Ajax: A New Approach to Web Applications\\" by Jesse James Garrett is a very good introduction.","date":"2005-03-28T00:00:00.000Z","formattedDate":"March 28, 2005","tags":[],"readingTime":2.215,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What to think about all the Ajax noise?","categories":["ajax"]},"prevItem":{"title":"Simplified Development using EJB 3.0: Raghu Kodali tested it for us","permalink":"/blog/2005/03/31/simplified-development-using-ejb-3-dot-0-raghu-kodali-tested-it-for-us"},"nextItem":{"title":"New Oracle Products on OTN","permalink":"/blog/2005/03/27/new-oracle-products-on-otn"}},"content":"In the last 2 months the blogosphere has been very verbal about Ajax technology that describe a way of doing Rich UI using DHTML, Javascript and XML over HTTP.  If you are not already familiar with the concept the article \\"[Ajax: A New Approach to Web Applications](http://www.adaptivepath.com/publications/essays/archives/000385.php)\\" by Jesse James Garrett is a very good introduction.\\n\\nFor me, it is the first time that finally we have something really new in term of UI that is compliant with most of the modern architectures that are server side Java based. You can argue that Flex, OpenLazlo, ..., are very good and powerful, but first of all they use a proprietary plugin and really integrated with the development approach that most of the Java developers are currently using (MVC based on Struts, JSF or others). What get me very excited about Ajax is the fact that finally we can see Web Development giving an easy and standard/open way of creating rich client.\\n\\nIf you search around Ajax your will see that more and more framework are providing integration of Ajax technologies into their solution. What I see as the biggest move is the adoption of Ajax by JavaServer Faces, and simply because it is the J2EE standard for Java Web Development. I am sure that lot of you will say that Tapestry,ROR and other existing frameworks are providing the integration now, so why bother with JSF... Hmmm, I will say that all the current solution as still geek oriented and really focus on the core developers... What I see with JSF/Ajax integration is finally enterprise developers, that are used to 4GL development tools, (where the most important part is to develop business logic not a nice framework or a new set of libraries) will be able to develop Web Applications based on components that are smart enough to give the usability of a desktop application.\\n\\nTo give an idea of what I mean, you can take a look to [Oracle ADF Faces components](http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/jsf/index.html) some of them are really powerful and provide rich interactivity to the user.  One example is the [Oracle Table Component](http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/jsf/doc/tagdoc/core/table.html) that support pagination (next/previous) without refreshing the whole page. As a developer you drop the component on your page, set the properties, and done! You do not have to care about any HTML or Javascript coding. And this using a standard based faces components that you can use in any Web Container and even IDE. I know that the ADF Faces components are not using the sames stack as Ajax (eg: XML over HTTP) but this is just an example of what will be the experience for a developer and a user."},{"id":"/2005/03/27/new-oracle-products-on-otn","metadata":{"permalink":"/blog/2005/03/27/new-oracle-products-on-otn","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-27-new-oracle-products-on-otn.md","source":"@site/blog/2005-03-27-new-oracle-products-on-otn.md","title":"New Oracle Products on OTN","description":"Oracle BPEL Process Manager 10.1.2 Beta-3 Preview","date":"2005-03-27T00:00:00.000Z","formattedDate":"March 27, 2005","tags":[],"readingTime":0.73,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New Oracle Products on OTN"},"prevItem":{"title":"What to think about all the Ajax noise?","permalink":"/blog/2005/03/28/what-to-think-about-all-the-ajax-noise"},"nextItem":{"title":"PHP on OC4J??? Yes with JSR 223 RI","permalink":"/blog/2005/03/26/php-on-oc4j-yes-with-jsr-223-ri"}},"content":"**Oracle BPEL Process Manager 10.1.2 Beta-3 Preview **\\n\\nBPEL PM 10.1.2 Beta 3 is [now available on OTN](http://www.oracle.com/technology/software/products/ias/preview.html). One of the biggest new, is the fact that the BPEL Designer is now based on Oracle JDeveloper and provide much more functionnalities than previous release.\\n\\n**Oracle Drive Preview Release**\\n[Oracle Drive Preview Release](http://www.oracle.com/technology/products/ias/portal/content_management_10gr2.html) is a powerful WebDAV client that allows Portal content management and publishing directly from your Windows Desktop; Key Highlights:\\n\\n*   Mount the Portal Repository as a Windows Drive\\n*   Get Portal specific menu options to set properties, ACL information, preview content and pages, etc\\n*   Access the Portal repository with a command line utility\\n*   Search from your Windows Explorer\\n*   Perform Virus check on the Portal Repository\\n*   Work with offline content and synchronize when online\\n*   Edit content with any available Editor\\n\\nYou can see a [demonstration of Oracle Drive](http://www.oracle.com/technology/products/ias/portal/viewlets/ORACLEDRIVEVIEWLET.swf) on OTN."},{"id":"/2005/03/26/php-on-oc4j-yes-with-jsr-223-ri","metadata":{"permalink":"/blog/2005/03/26/php-on-oc4j-yes-with-jsr-223-ri","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-26-php-on-oc4j-yes-with-jsr-223-ri.md","source":"@site/blog/2005-03-26-php-on-oc4j-yes-with-jsr-223-ri.md","title":"PHP on OC4J??? Yes with JSR 223 RI","description":"In this entry I am explaining how to use the JSR-223 Reference Implementation into Oracle Containers for J2EE (OC4J).","date":"2005-03-26T00:00:00.000Z","formattedDate":"March 26, 2005","tags":[],"readingTime":1.625,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"PHP on OC4J??? Yes with JSR 223 RI","categories":["oc4j","oracle","php","java"]},"prevItem":{"title":"New Oracle Products on OTN","permalink":"/blog/2005/03/27/new-oracle-products-on-otn"},"nextItem":{"title":"Blog Beginner: check out w.bloggar and other desktop tools","permalink":"/blog/2005/03/16/blog-beginner-check-out-w-dot-bloggar-and-other-desktop-tools"}},"content":"In this entry I am explaining how to use the JSR-223 Reference Implementation into Oracle Containers for J2EE (OC4J).\\n\\nNote: I have done this only on Windows XP Platform, I did not test on Unix/Linux yet.\\n\\n1-The first thing to do is to download the [JSR-223 Reference Implementation from the JCP Web site](http://jcp.org/aboutJava/communityprocess/pr/jsr223/index.html).\\n\\n2- Unzip the _sjp-1_0-ea-windows-i586.zip_ in a directory let say _d:\\\\java\\\\jsr223-ri_.\\n\\n3- You can choose here to do a proper installation (requires perl) or not. The *proper* installation preconfigure Apache Tomcat and the JSR-223 command line demonstration. Since what we are trying to achieve is to install the RI inside OC4J you do not need to do the installation.\\n\\n4- Download\\n\\n[this zip file](http://www.grallandco.com/blog/archives/ant-jsr223.zip)\\nthat contains a Ant task and various files to configure and deploy the JSR223 sample into OC4J.\\n\\n&nbsp;&nbsp;&nbsp;4.1 To execute the Ant script you must start to set the following environman variables:\\n*   JAVA_HOME that points to the JDK home\\n*   ORACLE_HOME that points to the OC4J home (eg: d:\\\\oracle\\\\oc4j)\\n*   JSR223_HOME that points to directory where you have unzipped the JSR223 RI (eg d:\\\\java\\\\jsr223)\\n\\n&nbsp;&nbsp;&nbsp;4.2 Create a startup script for OC4J using the command\\n\\n```\\nant\\n```\\n\\nThe default target create a script names _start-jsr223.bat_ in the %ORACLE_HOME%\\\\bin directory. This script set various environment variable and start OC4J with the correct system parameters to load correctly the PHP scrip engine from Java.\\n\\n&nbsp;&nbsp;&nbsp;4.3 Start OC4J using the created script\\n\\n```\\n%ORACLE_HOME%\\\\bin\\\\start-jsr223.bat\\n```\\n\\n&nbsp;&nbsp;&nbsp;4.4 Package and Deploy the Sample Application\\n\\n```\\nant deploy\\n```\\n\\nThis target packacge the Web samples from the reference implementation in a ear/war file, modifying the _web.xml_ to define the different servlet mapping needed by JSR223.\\n\\n5 Use the sample application\\n\\n`http://localhost:8888/jsr-223`\\n\\nYou can start to use PHP in OC4J ;-)  Take some time to read the sample code and specifications. JSR-223 is not only for Web container but also for JS2E, it is so interesting that the JSR-223 will probably be part of Mustang..."},{"id":"/2005/03/16/blog-beginner-check-out-w-dot-bloggar-and-other-desktop-tools","metadata":{"permalink":"/blog/2005/03/16/blog-beginner-check-out-w-dot-bloggar-and-other-desktop-tools","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-16-blog-beginner-check-out-w-dot-bloggar-and-other-desktop-tools.md","source":"@site/blog/2005-03-16-blog-beginner-check-out-w-dot-bloggar-and-other-desktop-tools.md","title":"Blog Beginner: check out w.bloggar and other desktop tools","description":"A friend a mine just ask me:","date":"2005-03-16T00:00:00.000Z","formattedDate":"March 16, 2005","tags":[],"readingTime":1.145,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Blog Beginner: check out w.bloggar and other desktop tools"},"prevItem":{"title":"PHP on OC4J??? Yes with JSR 223 RI","permalink":"/blog/2005/03/26/php-on-oc4j-yes-with-jsr-223-ri"},"nextItem":{"title":"JavaWorld Article:  Choosing a Java scripting language","permalink":"/blog/2005/03/15/javaworld-article-choosing-a-java-scripting-language"}},"content":"A friend a mine just ask me:\\n> What blog software do you recommend if somebody is starting newly, should have the ability to create categories, easy image uploading?\\n\\nI did not understand immediately his question, but he wanted to know if some Desktop application exists to easily publish on a blog...\\n\\nI am sure that you can find a lot on the Web, I personnaly use [w.bloggar](http://www.wbloggar.com/). As you may know most of the Blog (server) provides XML-RPC, it is how this tools communicate with your server\\n\\nI will have to test these tools on my Mac: [BlogApp](http://www.objectivelabs.com/blogapp.php) and [iBlog](http://homepage.mac.com/soapdog/iblog/). If you are Linux user take a look at: [BlogniX](http://blognix.sourceforge.net/)\\n\\n\x3c!-- truncate --\x3e\\n\\nHere the list of the supported online services and blog supported by w.bloggar:\\n\\n**Blog Services **\\n\\n* [Blogger ](http://www.blogger.com)\\n* [TypePad ](http://www.typepad.com)\\n* [BigBlogTool ](http://www.bigblogtool.com)\\n* [Blogalia ](http://www.blogalia.com)\\n* [TheBlog ](http://www.theblog.com.br)\\n* [Blog-City ](http://www.blog-city.com/)\\n* [EraBlog.NET ](http://EraBlog.NET)\\n* [Upsaid ](http://www.upsaid.com)\\n* [UBlog ](http://www.u-blog.net)\\n* [SquareSpace ](http://www.squarespace.com)\\n* [BlogWare ](http://www.blogware.com)\\n* [DearDiary.Net ](http://www.deardiary.net/)\\n\\n**Blog Tools **\\n\\n* [MovableType ](http://www.movabletype.org)\\n* [Nucleus ](http://www.nucleuscms.org/)\\n* [b2 ](http://www.cafelog.com)\\n* [WordPress ](http://www.wordpress.org)\\n* [b2 evolution ](http://www.b2evolution.net)\\n* [Blog:CMS ](http://www.blogcms.com)\\n* [.Text ](http://workspaces.gotdotnet.com/dottext)\\n* [BlogWorks XML ](http://hypothecate.co.uk/blogworksXML/)\\n* [Drupal ](http://www.drupal.org)\\n* [LiveJournal ](http://www.livejournal.com)\\n* [pMachine ](http://www.pmachine.com/)\\n* [Xoops ](http://www.xoops.org)\\n* [E-Xoops ](http://www.e-xoops.com)\\n* [PostNuke ](http://www.postnuke.com)\\n* [blojsom ](http://blojsom.sf.net)\\n* [Roller Weblogger ](http://www.rollerweblogger.org)\\n* [Domino ](http://www.codestore.net/dbapi)\\n* [YACS ](http://www.yetanothercommunitysystem.com)\\n* [Xaraya ](http://xaraya.com)"},{"id":"/2005/03/15/javaworld-article-choosing-a-java-scripting-language","metadata":{"permalink":"/blog/2005/03/15/javaworld-article-choosing-a-java-scripting-language","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-15-javaworld-article-choosing-a-java-scripting-language.md","source":"@site/blog/2005-03-15-javaworld-article-choosing-a-java-scripting-language.md","title":"JavaWorld Article:  Choosing a Java scripting language","description":"This new Java World article compares the different Java scripting languages (Groovy, JudoScript, Pnuts, JRuby, Jacl, Jython, Rhino, and BeanShell), and list the issues that you have to select the good one...","date":"2005-03-15T00:00:00.000Z","formattedDate":"March 15, 2005","tags":[{"label":"groovy","permalink":"/blog/tags/groovy"},{"label":"scripting","permalink":"/blog/tags/scripting"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"java","permalink":"/blog/tags/java"}],"readingTime":0.425,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaWorld Article:  Choosing a Java scripting language","tags":["groovy","scripting","javascript","java"]},"prevItem":{"title":"Blog Beginner: check out w.bloggar and other desktop tools","permalink":"/blog/2005/03/16/blog-beginner-check-out-w-dot-bloggar-and-other-desktop-tools"},"nextItem":{"title":"Great News ! OracleAS EJB 3.0 Preview is available","permalink":"/blog/2005/03/04/great-news-oracleas-ejb-3-dot-0-preview-is-available"}},"content":"This new [Java World article](http://www.javaworld.com/javaworld/jw-03-2005/jw-0314-scripting_p.html) compares the different Java scripting languages (Groovy, JudoScript, Pnuts, JRuby, Jacl, Jython, Rhino, and BeanShell), and list the issues that you have to select the good one...\\n\\nIn this article that I like I would like to read a little about the JSR-223 that should help with the integration part.\\n\\nI will be very interested to know how you use Java Scripting language in your projects, so feel free to drop me a comment or mail ( tugdual at gmail)"},{"id":"/2005/03/04/great-news-oracleas-ejb-3-dot-0-preview-is-available","metadata":{"permalink":"/blog/2005/03/04/great-news-oracleas-ejb-3-dot-0-preview-is-available","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-03-04-great-news-oracleas-ejb-3-dot-0-preview-is-available.md","source":"@site/blog/2005-03-04-great-news-oracleas-ejb-3-dot-0-preview-is-available.md","title":"Great News ! OracleAS EJB 3.0 Preview is available","description":"Looking forward to play around with EJB 3.0?","date":"2005-03-04T00:00:00.000Z","formattedDate":"March 4, 2005","tags":[{"label":"oc4j","permalink":"/blog/tags/oc-4-j"},{"label":"javaEE","permalink":"/blog/tags/java-ee"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Great News ! OracleAS EJB 3.0 Preview is available","tags":["oc4j","javaEE","oracle"]},"prevItem":{"title":"JavaWorld Article:  Choosing a Java scripting language","permalink":"/blog/2005/03/15/javaworld-article-choosing-a-java-scripting-language"},"nextItem":{"title":"Oracle and PHP","permalink":"/blog/2005/02/24/oracle-and-php"}},"content":"Looking forward to play around with EJB 3.0?\\n\\nYou can now download the OracleAS 3.0 Preview from OTN:\\n\\"[Oracle Application Server EJB 3.0 Preview](http://www.oracle.com/technology/tech/java/ejb30.html)\\".\\n\\nIn addition to the container you can find documentation, demonstration and technical papers.\\n\\nI am sure you will enjoy it, discover the new way of deal with persistence based on Java annotations..."},{"id":"/2005/02/24/oracle-and-php","metadata":{"permalink":"/blog/2005/02/24/oracle-and-php","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-02-24-oracle-and-php.md","source":"@site/blog/2005-02-24-oracle-and-php.md","title":"Oracle and PHP","description":"A friend of mine was asking me:","date":"2005-02-24T00:00:00.000Z","formattedDate":"February 24, 2005","tags":[],"readingTime":0.795,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle and PHP"},"prevItem":{"title":"Great News ! OracleAS EJB 3.0 Preview is available","permalink":"/blog/2005/03/04/great-news-oracleas-ejb-3-dot-0-preview-is-available"},"nextItem":{"title":"BPEL: Edwin answer to Dutch\'Rant post","permalink":"/blog/2005/02/17/bpel-edwin-answer-to-dutchrant-post"}},"content":"A friend of mine was asking me:\\n\\n_can I use PHP with Oracle?_\\n\\nSure you can!\\n\\nAnd a very good source of information about PHP and Oracle is the [OTN](http://otn.oracle.com) Web site itself, and especially the [OpenSource page](http://www.oracle.com/technology/tech/opensource/index.html). Here a list of my favorites:\\n\\n* [Get Started with Oracle and PHP](http://www.oracle.com/technology/oramag/webcolumns/2003/techarticles/hull_php.html)\\n* [Installing Oracle, PHP, and Apache on Linux](http://www.oracle.com/technology/tech/opensource/php/apache/inst_php_apache_linux.html)\\n* [An overview on globalizing Oracle PHP Applications (PDF)](http://www.oracle.com/technology/tech/opensource/php/globalizing_oracle_php_applications.pdf)\\n* [Generic Oracle and PHP FAQ](http://www.oracle.com/technology/tech/opensource/php_faq.html)\\n* [Oracle PHP Troubleshooting FAQ](http://www.oracle.com/technology/tech/opensource/php/php_troubleshooting_faq.html)\\n* [PHP OTN Forum](http://otn.oracle.com/forums/php.html)\\n\\n_Can I run my PHP Applications inside Oracle Application Server?_\\n\\nSure! Not only you can access the Oracle database from a PHP Application, but also if you are running Oracle Application 10_g_ the usage of mod_php is supported with Oracle HTTP Server see:\\n\\n* [Statement of Direction: PHP Support on OHS](http://www.oracle.com/technology/tech/opensource/php/php_ohs_sod.html)\\n* [PHP Instructions for OHS](http://www.oracle.com/technology/products/ias/ohs/htdocs/php_ohs.htm)\\n\\n_And what about development?_\\n\\nDevelopment could be done inside Oracle JDeveloper 10g using the The [Oracle JDeveloper 10g PHP Extension](http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/php/index.html)"},{"id":"/2005/02/17/bpel-edwin-answer-to-dutchrant-post","metadata":{"permalink":"/blog/2005/02/17/bpel-edwin-answer-to-dutchrant-post","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-02-17-bpel-edwin-answer-to-dutchrant-post.md","source":"@site/blog/2005-02-17-bpel-edwin-answer-to-dutchrant-post.md","title":"BPEL: Edwin answer to Dutch\'Rant post","description":"If you are interested by BPEL you can take a look to the blog entry \'BPEL: Yes or No?\' and the answer/comments from Edwin (VP of Development of Oracle BPEL Process Manager).","date":"2005-02-17T00:00:00.000Z","formattedDate":"February 17, 2005","tags":[],"readingTime":0.16,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"BPEL: Edwin answer to Dutch\'Rant post"},"prevItem":{"title":"Oracle and PHP","permalink":"/blog/2005/02/24/oracle-and-php"},"nextItem":{"title":"JSR 223: Public Review of the Specification","permalink":"/blog/2005/02/08/jsr-223-public-review-of-the-specification"}},"content":"If you are interested by BPEL you can take a look to [the blog entry \'BPEL: Yes or No?\'](http://blog.arendsen.net/index.php?p=13) and the answer/comments from Edwin (VP of Development of Oracle BPEL Process Manager)."},{"id":"/2005/02/08/jsr-223-public-review-of-the-specification","metadata":{"permalink":"/blog/2005/02/08/jsr-223-public-review-of-the-specification","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-02-08-jsr-223-public-review-of-the-specification.md","source":"@site/blog/2005-02-08-jsr-223-public-review-of-the-specification.md","title":"JSR 223: Public Review of the Specification","description":"The JSR 223: Scripting for the Java Platform is now open for public review until March 28th.","date":"2005-02-08T00:00:00.000Z","formattedDate":"February 8, 2005","tags":[{"label":"jsr","permalink":"/blog/tags/jsr"},{"label":"groovy","permalink":"/blog/tags/groovy"},{"label":"java","permalink":"/blog/tags/java"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JSR 223: Public Review of the Specification","tags":["jsr","groovy","java"]},"prevItem":{"title":"BPEL: Edwin answer to Dutch\'Rant post","permalink":"/blog/2005/02/17/bpel-edwin-answer-to-dutchrant-post"},"nextItem":{"title":"JavaWorld Article: Event-driven services in SOA","permalink":"/blog/2005/02/01/javaworld-article-event-driven-services-in-soa"}},"content":"The JSR 223: Scripting for the Java Platform is now open for public review until March 28th.\\n\\nIf you are not familiar with it, the JSR 223 main goal is to define how the Java platform should integrate with scripting languages. It defines a standard API similar to [Bean Scripting Framework](http://jakarta.apache.org/bsf/).\\n\\nYou can dowlnoad the spec from the [JCP Web Site](http://jcp.org/en/jsr/detail?id=223)."},{"id":"/2005/02/01/javaworld-article-event-driven-services-in-soa","metadata":{"permalink":"/blog/2005/02/01/javaworld-article-event-driven-services-in-soa","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-02-01-javaworld-article-event-driven-services-in-soa.md","source":"@site/blog/2005-02-01-javaworld-article-event-driven-services-in-soa.md","title":"JavaWorld Article: Event-driven services in SOA","description":"Interesting article about Event Driven Architecture (EDA) and Service Oriented Architecture based on Mule an open source ESB.","date":"2005-02-01T00:00:00.000Z","formattedDate":"February 1, 2005","tags":[],"readingTime":0.09,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaWorld Article: Event-driven services in SOA"},"prevItem":{"title":"JSR 223: Public Review of the Specification","permalink":"/blog/2005/02/08/jsr-223-public-review-of-the-specification"},"nextItem":{"title":"uConnect my best Blue Tooth experience.. so far...","permalink":"/blog/2005/01/23/uconnect-my-best-blue-tooth-experience-dot-so-far-dot-dot-dot"}},"content":"[Interesting article](http://www.javaworld.com/javaworld/jw-01-2005/jw-0131-soa_p.html) about Event Driven Architecture (EDA) and Service Oriented Architecture based on Mule an open source ESB."},{"id":"/2005/01/23/uconnect-my-best-blue-tooth-experience-dot-so-far-dot-dot-dot","metadata":{"permalink":"/blog/2005/01/23/uconnect-my-best-blue-tooth-experience-dot-so-far-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-01-23-uconnect-my-best-blue-tooth-experience-dot-so-far-dot-dot-dot.md","source":"@site/blog/2005-01-23-uconnect-my-best-blue-tooth-experience-dot-so-far-dot-dot-dot.md","title":"uConnect my best Blue Tooth experience.. so far...","description":"A friend a mine just bought the top of the line of the Dodge Grand Caravan... Really over-loaded...","date":"2005-01-23T00:00:00.000Z","formattedDate":"January 23, 2005","tags":[],"readingTime":0.605,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"uConnect my best Blue Tooth experience.. so far..."},"prevItem":{"title":"JavaWorld Article: Event-driven services in SOA","permalink":"/blog/2005/02/01/javaworld-article-event-driven-services-in-soa"},"nextItem":{"title":"First 2005 resolution: make a donation for South Asia","permalink":"/blog/2005/01/03/first-2005-resolution-make-a-donation-for-south-asia"}},"content":"A friend a mine just bought the top of the line of the Dodge Grand Caravan... Really over-loaded...\\n\\nMy favorite feature is [uConnect](http://www.dodge.com/crossbrand/uconnect/dodge/interface.html), that provide hand-free voice system based on Bluetooth...\\n\\nI just turned on blue tooth on my phone, and five minutes later I was able to talk to my friends and family and share that with all people in the car. I have several bluetooth stuff (Macs, PDA, Phone) and this is the first time that using uConnect that I find a really cool usage of it...\\n\\nThe missing feature: I would like to see an integration between my address book on the PDA to the GPS system allowing the system to direclty set the destination from an address...."},{"id":"/2005/01/03/first-2005-resolution-make-a-donation-for-south-asia","metadata":{"permalink":"/blog/2005/01/03/first-2005-resolution-make-a-donation-for-south-asia","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2005-01-03-first-2005-resolution-make-a-donation-for-south-asia.md","source":"@site/blog/2005-01-03-first-2005-resolution-make-a-donation-for-south-asia.md","title":"First 2005 resolution: make a donation for South Asia","description":"My first 2005 action was to give money for south Asia. I made a donation to the Unicef Organization and my employer Oracle is matching my amount.","date":"2005-01-03T00:00:00.000Z","formattedDate":"January 3, 2005","tags":[],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"First 2005 resolution: make a donation for South Asia"},"prevItem":{"title":"uConnect my best Blue Tooth experience.. so far...","permalink":"/blog/2005/01/23/uconnect-my-best-blue-tooth-experience-dot-so-far-dot-dot-dot"},"nextItem":{"title":"Eclipse and OC4J using the Lomboz Plugin","permalink":"/blog/2004/12/20/eclipse-and-oc4j-using-the-lomboz-plugin"}},"content":"My first 2005 action was to give money for south Asia. I made a donation to the Unicef Organization and my employer Oracle is matching my amount.\\n\\nSo think about it and take five minutes of your time to make a donation to one of this organization:\\n\\n*   **American Red Cross** [International Response Fund](https://www.redcross.org/donate/donation-form.asp)\\n*   **AmeriCares** [South Asia Earthquake Relief Fund](https://www.americaresfoundation.net/donate/default.aspx?id=South%20Asia%20Earthquake%20Relief%20Fund)\\n*   **Direct Relief International** [International Assistance Fund](https://www.directrelief.org/sections/support_us/d_donate_now.html)\\n*   **M\xe9decins Sans Fronti\xe8res International** [Tsunami Emergency Appeal](http://www.msf.org/donations/index.cfm)\\n*   **Oxfam** [Asian Earthquake &amp; Tsunami Fund](https://secure.ga3.org/02/asia_earthquake04)\\n*   **Sarvodaya** [Relief Fund for Tsunami Tragedy](http://www.sarvodaya.org/)\\n*   **Save the Children** [Asia Earthquake/Tsunami Relief Fund](http://savethechildren.org/radio_asia_earthquake.asp)\\n*   **UNICEF** [South Asia Tsunami Relief Efforts](http://www.unicefusa.org/tsunami)"},{"id":"/2004/12/20/eclipse-and-oc4j-using-the-lomboz-plugin","metadata":{"permalink":"/blog/2004/12/20/eclipse-and-oc4j-using-the-lomboz-plugin","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-12-20-eclipse-and-oc4j-using-the-lomboz-plugin.md","source":"@site/blog/2004-12-20-eclipse-and-oc4j-using-the-lomboz-plugin.md","title":"Eclipse and OC4J using the Lomboz Plugin","description":"You can find a new How-To Document that explains how to use OC4J and Eclipse together using the Lomboz plugin.","date":"2004-12-20T00:00:00.000Z","formattedDate":"December 20, 2004","tags":[{"label":"eclipse","permalink":"/blog/tags/eclipse"},{"label":"java","permalink":"/blog/tags/java"},{"label":"javaEE","permalink":"/blog/tags/java-ee"},{"label":"oc4j","permalink":"/blog/tags/oc-4-j"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.215,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Eclipse and OC4J using the Lomboz Plugin","tags":["eclipse","java","javaEE","oc4j","oracle"]},"prevItem":{"title":"First 2005 resolution: make a donation for South Asia","permalink":"/blog/2005/01/03/first-2005-resolution-make-a-donation-for-south-asia"},"nextItem":{"title":"What\'s new in OC4J 10g (10.1.3) Developer Preview","permalink":"/blog/2004/12/15/whats-new-in-oc4j-10g-10-dot-1-3-developer-preview"}},"content":"You can find a new [How-To Document](http://www.oracle.com/technology/tech/java/oc4j/1013/howtos/how-to-configure-lomboz/doc/lomboz-howto-part-1.html) that explains how to use OC4J and Eclipse together using the Lomboz plugin.\\n\\nYou can download the server definition files for the Production release of OC4J 10g (9.0.4) or for the J2EE 1.4 Developer Preview 10.1.3."},{"id":"/2004/12/15/whats-new-in-oc4j-10g-10-dot-1-3-developer-preview","metadata":{"permalink":"/blog/2004/12/15/whats-new-in-oc4j-10g-10-dot-1-3-developer-preview","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-12-15-whats-new-in-oc4j-10g-10-dot-1-3-developer-preview.md","source":"@site/blog/2004-12-15-whats-new-in-oc4j-10g-10-dot-1-3-developer-preview.md","title":"What\'s new in OC4J 10g (10.1.3) Developer Preview","description":"Application Server Control","date":"2004-12-15T00:00:00.000Z","formattedDate":"December 15, 2004","tags":[],"readingTime":6.38,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"What\'s new in OC4J 10g (10.1.3) Developer Preview"},"prevItem":{"title":"Eclipse and OC4J using the Lomboz Plugin","permalink":"/blog/2004/12/20/eclipse-and-oc4j-using-the-lomboz-plugin"},"nextItem":{"title":"Oracle Toplink and Spring framework integration","permalink":"/blog/2004/12/10/oracle-toplink-and-spring-framework-integration"}},"content":"###Application Server Control\\n\\nManagement and monitoring support is provided by a new version of Enterprise Manager Application Server Control which is included directly within the OC4J 10g (10.1.3) Developer Preview 3 release.\\n\\n\x3c!-- truncate --\x3e\\n\\nApplication Server Control is is fully JMX based and provides management and monitoring capabilities for this J2EE 1.4 compliant version of OC4J. It features a JSR-88 based deployment client with a powerful deployment plan editor, as well as a generic\\nJMX MBean browser that is JSR-77 aware. Application (user-defined) MBeans are also supported to the same extent as system MBeans.\\n\\nOther new areas of support include Web services management, TopLink session management, a JNDI browser, among many other new features.\\n\\n* Provides a generic JMX MBean browser that gives users a full view into all system MBeans. The new MBean browser provides features such as:\\n  * Hierarchical view of all system MBeans based on JSR-77\\nnaming hierarchy\\n  * Comprehensive search capabilities across MBean, attribute\\nand operation names, as well as support for searches using\\nthe JMX query syntax\\n  * Ability to view all MBean properties, such as attributes,\\noperations, statistics, notifications\\n  * Ability to invoke operations. Users will be able to invoke operations that require complex types as input parameters based on String based constructors for those complex types where applicable\\n  * Ability to change attribute values where applicable\\n  * Ability to subscribe to JMX notifications\\n* Application (user-defined) MBeans are supported to the same extent as system MBeans (see above) and are accessible via a link from the individual application home page\\n* JMX Notifications that the user chooses to subscribe to will be received and listed on the Received Notifications page\\n* Deployment follows JSR-88 and provides the following new features:\\n  * A generic and powerful JSR-88 deployment plan editor.\\n  * Comprehensive deployment progress messages during application deployments.\\n* New Web service management capabilities providing features such as:\\n  * Enable/Disable\\n  * Performance\\n  * Logging\\n  * Auditing\\n  * Security\\n  * Reliability\\n* TopLink session monitoring and management support\\n* Improved log viewing and searching capabilities\\n* A JNDI browser let users view the overall JNDI namespace, as well as application context namespaces\\n* Many other new features in areas such as JMS, JTA, JDBC, etc.\\n\\n\\n###Configuration, Administration and Deployment\\n\\n* Provides full support for JMX 1.2 and JMX Remote Access API (JSR-160)\\n* Implements Java2 Management Specification (JSR-77) to provide JMX MBean controls for configuration and monitoring of the server and deployed applications\\n* Implements Java2 Deployment API (JSR-88) to support standard deployment operations,uses a separate deployment plan to capture OC4J specific deployment details in a non intrusive manner. Deployment plans can be presented to server at deployment time to provide server with OC4J configuration set\\n* Fine grained security controls to facilitate administration at the system and application only levels\\n* A set of Ant tasks which utilize JSR-88 are provided to support deployment related operations from Ant scripts\\n* Flexible classloading implementation which allows for the deployment of shared-libraries which can be consumed by deployed applications. Using the shared-library mechanism, applications have complete control over which class libraries are loaded, enabling the use of different XML parsers and Oracle JDBC driver versions than what are provided by default by OC4J\\n\\n###EJB\\n\\n* Toplink is now fully integrated as the default persistence manager for performing Container Managed Persistence (CMP) with Entity EJBs\\n* Support for incremental EJB deployment, replacing individual class files instead of an\\nentire EJB module\\n\\n###JMS\\n\\n* JMS 1.1 compatible with OJMS and OracleAS JMS* A generic JMS JCA 1.5 Resource Adapter\\n* Complete integration of third party JMS providers into OC4J\\n* JMX based dynamic configuration, management and monitoring of JMS infrastructure\\n\\n### Web Services\\n\\n* Ant tasks for developing Web services including:\\n  * assemble - generate a Web service from a Java class\\n  * topdownAssemble - generate a Web service from a WSDL\\n  * annotationAssemble - generate a Web service from JSR 181 annotations\\n  * ejbAssemble - generate a Web service from an EJB 2.1\\n  * jmsAssemble - generate a Web service from a JMS queue or topic\\n  * plsqlAssemble - generate a Web service from a PLSQL package\\n  * sqlAssemble - generate a Web service from a sql statement\\n  * dbJavaAssemble - generate a Web service from a Java class located in the Oracle Database\\n  * genGatewayService - generate a gateway service for third party Web service WSDLs\\n  * genProxy - generate a client proxy from a WSDL to invoke a Web service\\n\\nInformation on these tasks and more is available in the OC4J 10.1.3\\nDeveloper Preview 3 [Documentation Library](http://download.oracle.com/otn/java/oc4j/1013/doc/).\\n\\n* An extended command line WebServicesAssembler tool providing the same functionality as the Ant tasks\\n* A Web services management framework enabling users to SOAP auditing, content based logging, security and reliability. The framework enables administrators to enable and disable services as well as enable and disable management\\ncharacteristics applied to those services. This framework is used by Application Server Control to provide Web services management configurability to system administrators and by JDeveloper to enable Web services management configuration during development\\n* Support for the OASIS standard WS-Reliability\\n* Support for SOAP 1.1 and SOAP 1.2\\n* Support for a SOAP over JMS binding in addition to the existing SOAP over HTTP binding\\n* Support for the OASIS standard WS-Security including authentication tokens, XML encryption and digital signatures. More information is available in the Security Release Notes.\\n* This release of OC4J has been tested both as a consumer of OracleAS BPEL Process Manager business\\nprocess WSDLs as well as producer of Web services that can be used in OracleAS BPEL Process Manager BPEL processes.\\n\\n###JCA\\n\\n* Compliant to JCA 1.5, also supports JCA 1.0 for backwards compatibility\\n* Tested with Oracle and third-party Adapters\\n* Management - JMX support (both standard and extensions) and Application Server Control for deployment, configuration, administration and metrics monitoring operations.\\n* Persistence for JCA using Object-XML mapping in the Toplink component\\n* Deployment enhancements:deployment for oc4j-ra.xml\\n* 2PC recovery support including JCA\\n\\n###Security -- JAAS/JAZN\\n\\n* Implementation of Web Services security (OASIS WSS 1.0 specification)\\n* Ability to integrate Oracle JAZN with 3rd party LDAP providers such as Sun One or Microsoft Active Directory. Please refer to Oracle Application Server Containers for J2EE documentation for detailed instructions.\\n\\n###Job Scheduler\\n\\nThe OracleAS Job Scheduler provides asynchronous scheduling services\\nfor J2EE applications with the following features:\\n\\n* API for submitting and controlling jobs\\n* Temporal- and trigger-based job execution\\n* Event listeners for monitoring job execution and status\\n* iCalendar recurrence expression support\\n* API-level JTA support for job submission and control\\n* Automatic retry of failed jobs\\n* Job blackout windows\\n* Configurable persistence for job definitions and configuration\\n* JMX monitoring and administration\\n\\nFor the latest documentation and sample applications see the Scheduler How-To\'s on OTN.\\n\\n###Application Clustering\\n\\n* Application clustering can be enabled on a specific application basis, enabling an OC4J instance to concurrently host both clustered and non clustered applications.\\n* Support has been added for additional replication protocols. The protocols provided are multicast and peer-to-peer for in memory based state replication. The peer replication protocol supports direct TCP based connections between the members of a cluster group. A database replication protocol is also provided which stores and retrieved\\nsession state to and from a specified database instance.\\n* The policies which determine when replication takes place have been extended in this release. Support is now provided\\nfor onCall, onChange, and onShutdown events. Web applications now default to using the onCall policy which queues up changes made to the HttpSession object within a method call and then send the change set when the method completes."},{"id":"/2004/12/10/oracle-toplink-and-spring-framework-integration","metadata":{"permalink":"/blog/2004/12/10/oracle-toplink-and-spring-framework-integration","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-12-10-oracle-toplink-and-spring-framework-integration.md","source":"@site/blog/2004-12-10-oracle-toplink-and-spring-framework-integration.md","title":"Oracle Toplink and Spring framework integration","description":"You can now find a preview of the Oracle Toplink &amp; framework integration. This allows you to easily use Toplink features in your Spring application.","date":"2004-12-10T00:00:00.000Z","formattedDate":"December 10, 2004","tags":[{"label":"oracle","permalink":"/blog/tags/oracle"},{"label":"toplink","permalink":"/blog/tags/toplink"},{"label":"java","permalink":"/blog/tags/java"},{"label":"spring","permalink":"/blog/tags/spring"}],"readingTime":0.165,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Toplink and Spring framework integration","tags":["oracle","toplink","java","spring"]},"prevItem":{"title":"What\'s new in OC4J 10g (10.1.3) Developer Preview","permalink":"/blog/2004/12/15/whats-new-in-oc4j-10g-10-dot-1-3-developer-preview"},"nextItem":{"title":"At Oracle Open World all the week","permalink":"/blog/2004/12/07/at-oracle-open-world-all-the-week"}},"content":"You can now find a preview of the Oracle Toplink &amp; framework integration. This allows you to easily use Toplink features in your Spring application.\\n\\nYou can download it from [OTN Web site](http://www.oracle.com/technology/products/ias/toplink/preview/spring/index.html)."},{"id":"/2004/12/07/at-oracle-open-world-all-the-week","metadata":{"permalink":"/blog/2004/12/07/at-oracle-open-world-all-the-week","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-12-07-at-oracle-open-world-all-the-week.md","source":"@site/blog/2004-12-07-at-oracle-open-world-all-the-week.md","title":"At Oracle Open World all the week","description":"As you may know this is Oracle Open World this week in San Francisco... I am working there all week mainly on the OC4J and JDeveloper demo pods. Come to see the new version that help you to develop J2EE and Web Services. Also I am presenting Thursday at 4pm a paper about Service Oriented Architecture (SOA) and Event Driven Architecture (EDA)","date":"2004-12-07T00:00:00.000Z","formattedDate":"December 7, 2004","tags":[{"label":"oow","permalink":"/blog/tags/oow"},{"label":"conference","permalink":"/blog/tags/conference"},{"label":"oracle","permalink":"/blog/tags/oracle"}],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"At Oracle Open World all the week","tags":["oow","conference","oracle"]},"prevItem":{"title":"Oracle Toplink and Spring framework integration","permalink":"/blog/2004/12/10/oracle-toplink-and-spring-framework-integration"},"nextItem":{"title":"New OC4J and JDeveloper Release Available","permalink":"/blog/2004/12/04/new-oc4j-and-jdeveloper-release-available"}},"content":"As you may know this is Oracle Open World this week in San Francisco... I am working there all week mainly on the OC4J and JDeveloper demo pods. Come to see the new version that help you to develop J2EE and Web Services. Also I am presenting Thursday at 4pm a paper about Service Oriented Architecture (SOA) and Event Driven Architecture (EDA)\\n\\nI invite you to join this feast... if you can not take a look to the [OTN at Oracle World blog](http://www.orablogs.com/otn_at_openworld/), that highlights important keynotes, session and demoground activities...\\n\\nYou can also take a look to the official [Oracle Open World Web site](http://www.oracle.com/openworld/online/index.html)"},{"id":"/2004/12/04/new-oc4j-and-jdeveloper-release-available","metadata":{"permalink":"/blog/2004/12/04/new-oc4j-and-jdeveloper-release-available","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-12-04-new-oc4j-and-jdeveloper-release-available.md","source":"@site/blog/2004-12-04-new-oc4j-and-jdeveloper-release-available.md","title":"New OC4J and JDeveloper Release Available","description":"Oracle Containers for J2EE 10g (10.1.3) Developer Preview","date":"2004-12-04T00:00:00.000Z","formattedDate":"December 4, 2004","tags":[],"readingTime":1.045,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New OC4J and JDeveloper Release Available","categories":["javaEE","oc4j","oracle"]},"prevItem":{"title":"At Oracle Open World all the week","permalink":"/blog/2004/12/07/at-oracle-open-world-all-the-week"},"nextItem":{"title":"Article: Transaction Processing in Distributed Service-Oriented Applications","permalink":"/blog/2004/11/29/article-transaction-processing-in-distributed-service-oriented-applications"}},"content":"**Oracle Containers for J2EE 10g (10.1.3) Developer Preview **\\n\\nThis fully J2EE 1.4 compatible release of OC4J offers improvements in classloading, data-source consolidation and configuration, TopLink integration and many other areas. This release also delivers a host of new capabilities in the area of Web Services, including management capabilities to enable and disable services and to configure message reliability, logging and security options. The brand new browser-based Enterprise Manager Application Server Control management console provides deployment, management and monitoring of all types of J2EE applications using the new standard JMX (JSR-77) and Deployment (JSR-88) APIs.\\n\\nDownload it now and experience OC4J for yourself: [http://www.oracle.com/technology/tech/java/oc4j/1013](http://www.oracle.com/technology/tech/java/oc4j/1013)\\n\\n**Oracle JDeveloper 10g (10.1.3) Developer Preview**\\n\\nThe focus of this release has been to increase developer productivity, improve the user interface, and reduce the footprint. This Developer Preview release is intended to give you a sneak peek at the developer goodness to come and solicit your feedback. This release is the most substantial and ground-breaking we\'ve had in years, adding many new features -- including a new look and feel, greatly improved coding environment, extensive refactoring options, J2EE 1.4/J2SE 5.0 support, and visual JSF development. We encourage you to take it for a spin.\\n\\nDownload the product and check out the related resources at [http://www.oracle.com/technology/products/jdev/101/](http://www.oracle.com/technology/products/jdev/101/)"},{"id":"/2004/11/29/article-transaction-processing-in-distributed-service-oriented-applications","metadata":{"permalink":"/blog/2004/11/29/article-transaction-processing-in-distributed-service-oriented-applications","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-11-29-article-transaction-processing-in-distributed-service-oriented-applications.md","source":"@site/blog/2004-11-29-article-transaction-processing-in-distributed-service-oriented-applications.md","title":"Article: Transaction Processing in Distributed Service-Oriented Applications","description":"Jon Maron from Oracle has just published an article about transaction processing in distributed SOA.","date":"2004-11-29T00:00:00.000Z","formattedDate":"November 29, 2004","tags":[],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Article: Transaction Processing in Distributed Service-Oriented Applications","categories":["soa"]},"prevItem":{"title":"New OC4J and JDeveloper Release Available","permalink":"/blog/2004/12/04/new-oc4j-and-jdeveloper-release-available"},"nextItem":{"title":"Some cool stuff about JSP 2.0","permalink":"/blog/2004/11/26/some-cool-stuff-about-jsp-2-dot-0"}},"content":"Jon Maron from Oracle has just published an article about [transaction processing in distributed SOA](http://www.informit.com/articles/article.asp?p=351607).\\n\\nYou will learn in this article the challenges that you have when you want to manage transactions in a distributed Services Oriented Architecture running using Web Services."},{"id":"/2004/11/26/some-cool-stuff-about-jsp-2-dot-0","metadata":{"permalink":"/blog/2004/11/26/some-cool-stuff-about-jsp-2-dot-0","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-11-26-some-cool-stuff-about-jsp-2-dot-0.md","source":"@site/blog/2004-11-26-some-cool-stuff-about-jsp-2-dot-0.md","title":"Some cool stuff about JSP 2.0","description":"J2EE 1.4 introduces a major release of JSP: 2.0.","date":"2004-11-26T00:00:00.000Z","formattedDate":"November 26, 2004","tags":[],"readingTime":2.305,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Some cool stuff about JSP 2.0","categories":["javaEE","jsp"]},"prevItem":{"title":"Article: Transaction Processing in Distributed Service-Oriented Applications","permalink":"/blog/2004/11/29/article-transaction-processing-in-distributed-service-oriented-applications"},"nextItem":{"title":"New Oracle White Paper: Accelerate Development and Deployment of Service-Oriented Applications","permalink":"/blog/2004/11/19/new-oracle-white-paper-accelerate-development-and-deployment-of-service-oriented-applications"}},"content":"J2EE 1.4 introduces a major release of JSP: 2.0.\\n\\nHere some of the cool new features:\\n\\n\x3c!-- truncate --\x3e\\n\\n##### Direct usage of Expression Language (EL) in your JSP\\nYou do not need to put the EL in any tag now, just use it as needed:\\n\\n``` html\\n<html>\\n<head><title>JSP 2.0 new features</title></head>\\n<body>\\nHello ${param.name}\\n</body>\\n</html>\\n```\\nFind more about JSP 2.0 Expression Language in [the J2EE 1.4 tutorial](http://java.sun.com/j2ee/1.4/docs/tutorial/doc/JSPIntro7.html).\\n\\n##### Easy tags creation with .tag files\\nIt is now easier to create your own tags.\\nYou just need to create a new .tag file (or .tagx if you want to use XML syntax) in the WEB-INF/tags directory of your Web application; or META-INF/tags if you want to package the Tags in Jar file. So creating a .tag file is easy, using the attribute directive. The following example is a new tag named mytag.tag that prints a title set using the attribute title, in the color specified in the attribute textColor.\\n\\n``` html\\n<%@ attribute name=\\"title\\" required=\\"true\\" description=\\"Title of the document\\"%>\\n<%@ attribute name=\\"textColor\\" required=\\"true\\" description=\\"Color of the Title\\"%>\\n<h1 style=\\"color:${textColor}\\">${title}</h1>\\n```\\n\\nHere is the JSP that uses this new Tag:\\n\\n``` html\\n<%@ taglib tagdir=\\"/WEB-INF/tags/\\" prefix=\\"tags\\"%>\\n<html>\\n<head>\\n</head>\\n<body>\\n<p>\\n<tags:mytag title=\\"My new JSP\\" textColor=\\"blue\\"/>\\n</p>\\n<p>\\nHello World${param.name}\\n</p>\\n</body>\\n</html>\\n```\\n\\n##### Easy header and footer template using the prelude and coda includes\\n\\nIn most of the Web application that I have built, I started by creating template for my HTML pages; most of them to handle header and footer. Oracle JSP implementation provides this for a while using the [Global Include feature](http://download-west.oracle.com/docs/cd/B10464_01/web.904/b10320/trandepl.htm#1005780). JSP 2.0 introduces a standard way of doing that using prelude and coda includes. I *hate* the choice made by the spec to call that prelude and coda. May be good Java developer are necessary musicians, since this is commonly used there? Why not simply header/footer or using a prefix like pre.../post.... Anyway, that is not the point.\\n\\nThe way you can set a prelude and/or coda include to your JSPs is done with the new Web Descriptor tag: `<jsp-property-group>`. This new tag allows you to configure a set of JSP that matches a specific URL. Part of the subtags of `<jsp-property-group>` are:\\n\\n* `<include-coda>` : the path to JSP fragment (.jspf) to include in the beginning all the JSP that matched the URL.\\n* `<include-prelude>`:the path to the JSP fragment to include in the end all the JSP that matched the URL.\\n\\nAn example configuration:\\n\\n``` html\\n<jsp-config>\\n<jsp-property-group>\\n<url-pattern>*.jsp</url-pattern>\\n<include-prelude>/WEB-INF/includes/prelude.jsp</include-prelude>\\n<include-coda>/WEB-INF/includes/coda.jsp</include-coda>\\n</jsp-property-group>\\n</jsp-config>\\n```\\n\\nThis 3 new features of JSP 2.0 are just a very small list of the features introduced in JSP and Servlet in J2EE 1.4, but are my favorites. They are very easy to test and to adopt."},{"id":"/2004/11/19/new-oracle-white-paper-accelerate-development-and-deployment-of-service-oriented-applications","metadata":{"permalink":"/blog/2004/11/19/new-oracle-white-paper-accelerate-development-and-deployment-of-service-oriented-applications","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-11-19-new-oracle-white-paper-accelerate-development-and-deployment-of-service-oriented-applications.md","source":"@site/blog/2004-11-19-new-oracle-white-paper-accelerate-development-and-deployment-of-service-oriented-applications.md","title":"New Oracle White Paper: Accelerate Development and Deployment of Service-Oriented Applications","description":"Oracle has published a new white paper about Service Oriented Architecture, and how Oracle Application Server 10g accelerates SOA development and deployment.","date":"2004-11-19T00:00:00.000Z","formattedDate":"November 19, 2004","tags":[],"readingTime":0.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New Oracle White Paper: Accelerate Development and Deployment of Service-Oriented Applications","comments":true,"categories":["portal"]},"prevItem":{"title":"Some cool stuff about JSP 2.0","permalink":"/blog/2004/11/26/some-cool-stuff-about-jsp-2-dot-0"},"nextItem":{"title":"Oracle TopLink: Happy 10th Anniversary","permalink":"/blog/2004/11/17/oracle-toplink-happy-10th-anniversary"}},"content":"Oracle has published a new white paper about Service Oriented Architecture, and how Oracle Application Server 10g accelerates SOA development and deployment.\\n\\n[Read the paper on the Oracle Web Site](http://www.oracle.com/solutions/integration/oracleas10g_javaws_bwp.pdf)."},{"id":"/2004/11/17/oracle-toplink-happy-10th-anniversary","metadata":{"permalink":"/blog/2004/11/17/oracle-toplink-happy-10th-anniversary","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-11-17-oracle-toplink-happy-10th-anniversary.md","source":"@site/blog/2004-11-17-oracle-toplink-happy-10th-anniversary.md","title":"Oracle TopLink: Happy 10th Anniversary","description":"In the context of the 10th anniversary of Oracle TopLink, The Server Side published an interview of Mike Keith and Doug Clark, architect and product manager of TopLink. They talk about persistence in J2EE using EJB 3.0, but also the differences between TopLink and other persistence solutions...","date":"2004-11-17T00:00:00.000Z","formattedDate":"November 17, 2004","tags":[],"readingTime":0.245,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle TopLink: Happy 10th Anniversary","categories":["java","javaEE","toplink","orm"]},"prevItem":{"title":"New Oracle White Paper: Accelerate Development and Deployment of Service-Oriented Applications","permalink":"/blog/2004/11/19/new-oracle-white-paper-accelerate-development-and-deployment-of-service-oriented-applications"},"nextItem":{"title":"Windows User finally you will be able to use Konfabulator","permalink":"/blog/2004/11/15/windows-user-finally-you-will-be-able-to-use-konfabulator"}},"content":"In the context of the 10th anniversary of Oracle TopLink, <a href=\\"http://theserverside.com/news/thread.tss?thread_id=30017\\" target=\\"_tugSite\\">The Server Side published an interview of Mike Keith and Doug Clark</a>, architect and product manager of TopLink. They talk about persistence in J2EE using EJB 3.0, but also the differences between TopLink and other persistence solutions..."},{"id":"/2004/11/15/windows-user-finally-you-will-be-able-to-use-konfabulator","metadata":{"permalink":"/blog/2004/11/15/windows-user-finally-you-will-be-able-to-use-konfabulator","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-11-15-windows-user-finally-you-will-be-able-to-use-konfabulator.md","source":"@site/blog/2004-11-15-windows-user-finally-you-will-be-able-to-use-konfabulator.md","title":"Windows User finally you will be able to use Konfabulator","description":"One of my favorites tools on my Mac is Konfabulator that allows you to create -or use prebuilt- application called Widgets. This Widgets are for example battery level, weather, see the Widget Gallery.","date":"2004-11-15T00:00:00.000Z","formattedDate":"November 15, 2004","tags":[],"readingTime":0.38,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows User finally you will be able to use Konfabulator"},"prevItem":{"title":"Oracle TopLink: Happy 10th Anniversary","permalink":"/blog/2004/11/17/oracle-toplink-happy-10th-anniversary"},"nextItem":{"title":"HTML/Javascript tip: Refreshing an image not the full page","permalink":"/blog/2004/11/08/html-javascript-tip-refreshing-an-image-not-the-full-page"}},"content":"One of my favorites tools on my Mac is [Konfabulator](http://www.konfabulator.com/) that allows you to create -or use prebuilt- application called Widgets. This Widgets are for example battery level, weather, see the [Widget Gallery](http://www.widgetgallery.com/).\\n\\nThe reason why I love it is not necessary the list of existing one, but more because you can very easily develop yours using Javascript. Dowload it and start to develop your widgets, developer guide, javascript reference are available in the [Konfabulator Workshop](http://www.konfabulator.com/workshop/)"},{"id":"/2004/11/08/html-javascript-tip-refreshing-an-image-not-the-full-page","metadata":{"permalink":"/blog/2004/11/08/html-javascript-tip-refreshing-an-image-not-the-full-page","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-11-08-html-javascript-tip-refreshing-an-image-not-the-full-page.md","source":"@site/blog/2004-11-08-html-javascript-tip-refreshing-an-image-not-the-full-page.md","title":"HTML/Javascript tip: Refreshing an image not the full page","description":"I was discussing with a friend about the creation of a monitoring dashboard in HTML. As any monitoring tool you want to be able to see the information in real-time, so you need to refresh the content....","date":"2004-11-08T00:00:00.000Z","formattedDate":"November 8, 2004","tags":[],"readingTime":0.66,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"HTML/Javascript tip: Refreshing an image not the full page"},"prevItem":{"title":"Windows User finally you will be able to use Konfabulator","permalink":"/blog/2004/11/15/windows-user-finally-you-will-be-able-to-use-konfabulator"},"nextItem":{"title":"First contact with Oracle Database XE","permalink":"/blog/2004/10/31/first-contact-with-oracle-database-xe"}},"content":"I was discussing with a friend about the creation of a monitoring dashboard in HTML. As any monitoring tool you want to be able to see the information in *real-time*, so you need to refresh the content....\\n\\nOne way of doing it is to refresh the whole page and this is easy, just use the meta tag:\\n`<META HTTP-EQUIV=\\"Refresh\\" CONTENT=\\"5; URL=http://www.grallandco.com/\\" >`\\nwhere 5 is the number of seconds between each refresh.\\n\\nJavascript allows you to easily refresh a specific image of your page (a chart for example):\\n\\n``` html\\n<img src=\\"myChartServlet\\" name=\\"chart1\\"/>\\n<script language=\\"JavaScript\\">\\n  function loadImage() {\\n    var now = new Date();\\n    if (document.images) {\\n      document.images.chart1.src = \'myChartServlet?time=\'+now.getTime(); // add the time to avoid caching\\n    }\\n    setTimeout(\'loadImage()\',1000);\\n  }\\n  setTimeout(\'loadImage()\',1000);\\n<\/script>\\n```\\nYou can obviously make the whole think dynamic and configurable..."},{"id":"/2004/10/31/first-contact-with-oracle-database-xe","metadata":{"permalink":"/blog/2004/10/31/first-contact-with-oracle-database-xe","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-31-first-contact-with-oracle-database-xe.md","source":"@site/blog/2004-10-31-first-contact-with-oracle-database-xe.md","title":"First contact with Oracle Database XE","description":"Oracle Database 10g Express Edition Beta Release is now available for download, from OTN:","date":"2004-10-31T00:00:00.000Z","formattedDate":"October 31, 2004","tags":[],"readingTime":1.065,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"First contact with Oracle Database XE"},"prevItem":{"title":"HTML/Javascript tip: Refreshing an image not the full page","permalink":"/blog/2004/11/08/html-javascript-tip-refreshing-an-image-not-the-full-page"},"nextItem":{"title":"Oracle Application Server and OC4J Certification","permalink":"/blog/2004/10/28/oracle-application-server-oc4j-certification"}},"content":"Oracle Database 10g Express Edition Beta Release is now available for download, from OTN:\\n\\n*   [Oracle 10g Express Edition (Oracle Database XE)](http://www.oracle.com/technology/software/products/database/xe/index.html).\\n\\n\x3c!-- truncate --\x3e\\n\\nWhat is this new release, you can find more information on the Oracle XE page, that I quote:\\n\\n> Oracle Database XE is a great starter database for:\\n>\\n> *   **Developers** working on PHP, Java, .NET, and Open Source applications\\n>\\n> *   **DBAs** who need a free, starter database for training and deployment\\n>\\n> *   **Independent Software Vendors (ISVs) and hardware vendors** who want a starter database to distribute free of charge\\n>\\n> *   **Educational institutions and students** who need a free database for their curriculum\\n\\nThis first contact with the latest release of the Oracle database is really great, the 150Mb download is really quick...\\n\\nThe installation is really easy... I have created a [viewlet driving you through the installation](http://www.grallandco.com/blog/archives/oracle-xe/oracle-xe-install.swf).\\n\\nAs you can see in the viewlet, at the end of the installation you have\\na running database that you can fully administer from your browser, you\\ncan also create CRUD application using HTMLDB... And obviously you can\\nconnect your J2EE application server to it and start creating\\napplications...\\n\\n\\n#### Resources:\\n\\n*   [Oracle XE site](http://www.oracle.com/technology/products/database/xe/index.html)\\n*   [Oracle XE forum](http://www.oracle.com/technology/products/database/xe/forum.html) (monitored by Tom Kyte!)"},{"id":"/2004/10/28/oracle-application-server-oc4j-certification","metadata":{"permalink":"/blog/2004/10/28/oracle-application-server-oc4j-certification","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-28-oracle-application-server-oc4j-certification.md","source":"@site/blog/2004-10-28-oracle-application-server-oc4j-certification.md","title":"Oracle Application Server and OC4J Certification","description":"If you ask yourself on which Operating Systems, JDK, Browser, etc... you can use Oracle Application Server 10g just consult OTN or Oracle Metalink:","date":"2004-10-28T00:00:00.000Z","formattedDate":"October 28, 2004","tags":[],"readingTime":0.165,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Application Server and OC4J Certification"},"prevItem":{"title":"First contact with Oracle Database XE","permalink":"/blog/2004/10/31/first-contact-with-oracle-database-xe"},"nextItem":{"title":"Mike Keith about Java2 5.0 Annotations","permalink":"/blog/2004/10/26/mike-keith-about-java2-5-dot-0-annotations"}},"content":"If you ask yourself on which Operating Systems, JDK, Browser, etc... you can use Oracle Application Server 10g just consult OTN or [Oracle Metalink](http://metalink.oracle.com/):\\n\\n* [OTN Oracle Application Server 10g (9.0.4) Certification Matrix](http://www.oracle.com/technology/software/products/ias/files/as-certification-904.html)"},{"id":"/2004/10/26/mike-keith-about-java2-5-dot-0-annotations","metadata":{"permalink":"/blog/2004/10/26/mike-keith-about-java2-5-dot-0-annotations","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-26-mike-keith-about-java2-5-dot-0-annotations.md","source":"@site/blog/2004-10-26-mike-keith-about-java2-5-dot-0-annotations.md","title":"Mike Keith about Java2 5.0 Annotations","description":"Mike Keith,architect for Oracle Containers for J2EE (OC4J) and TopLink products and currently represents Oracle in the EJB 3.0 expert group, has published an interesting article about Annotations in Java 5.0:.","date":"2004-10-26T00:00:00.000Z","formattedDate":"October 26, 2004","tags":[],"readingTime":0.31,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Mike Keith about Java2 5.0 Annotations"},"prevItem":{"title":"Oracle Application Server and OC4J Certification","permalink":"/blog/2004/10/28/oracle-application-server-oc4j-certification"},"nextItem":{"title":"New Dynamic Menu for OracleAS Portal 10g","permalink":"/blog/2004/10/07/new-dynamic-menu-for-oracleas-portal-10g"}},"content":"Mike Keith,architect for Oracle Containers for J2EE (OC4J) and TopLink products and currently represents Oracle in the EJB 3.0 expert group, has published an interesting article about Annotations in Java 5.0:.\\n\\n[To Annotate or Not?](http://www.oracle.com/technology/pub/articles/annotations_xml.html) focused on the usage of Annotation or XML as metadata language. Mike provides sample code and explain the pros and cons of the annotation using use cases."},{"id":"/2004/10/07/new-dynamic-menu-for-oracleas-portal-10g","metadata":{"permalink":"/blog/2004/10/07/new-dynamic-menu-for-oracleas-portal-10g","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-07-new-dynamic-menu-for-oracleas-portal-10g.md","source":"@site/blog/2004-10-07-new-dynamic-menu-for-oracleas-portal-10g.md","title":"New Dynamic Menu for OracleAS Portal 10g","description":"Ronaldo Viscuso has just released a new version of his DHTML Menu Portlet. This portlet generates a DHTML menu for site navigation, with links to pages and subpages from a selected page group.","date":"2004-10-07T00:00:00.000Z","formattedDate":"October 7, 2004","tags":[],"readingTime":0.37,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"New Dynamic Menu for OracleAS Portal 10g"},"prevItem":{"title":"Mike Keith about Java2 5.0 Annotations","permalink":"/blog/2004/10/26/mike-keith-about-java2-5-dot-0-annotations"},"nextItem":{"title":"Jive portlets for OracleAS Portal","permalink":"/blog/2004/10/06/jive-portlets-for-oracleas-portal"}},"content":"Ronaldo Viscuso has just released a new version of his DHTML Menu Portlet. This portlet generates a DHTML menu for site navigation, with links to pages and subpages from a selected page group.\\n\\nStylesheet, height, width, mouse-over behavior, etc. are all configurable via the \\"Edit Defaults\\" page, which allows for complete customization for pretty much any desired look & feel....\\n\\nThe portlet, screen shots and installation document are available on the [Portal Knowledge Exchange](http://portalstudio.oracle.com/pls/ops/opstudio.wwv_main.main?p_siteid=233&p_cornerid=497710)."},{"id":"/2004/10/06/jive-portlets-for-oracleas-portal","metadata":{"permalink":"/blog/2004/10/06/jive-portlets-for-oracleas-portal","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-06-jive-portlets-for-oracleas-portal.md","source":"@site/blog/2004-10-06-jive-portlets-for-oracleas-portal.md","title":"Jive portlets for OracleAS Portal","description":"An Oracle Application Server (OracleAS) Provider for Jive Forums is now available to download from the Portal Integration Solutions page on OTN.","date":"2004-10-06T00:00:00.000Z","formattedDate":"October 6, 2004","tags":[],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Jive portlets for OracleAS Portal"},"prevItem":{"title":"New Dynamic Menu for OracleAS Portal 10g","permalink":"/blog/2004/10/07/new-dynamic-menu-for-oracleas-portal-10g"},"nextItem":{"title":"Looking for articles: use JavaPro magazine collection","permalink":"/blog/2004/10/06/looking-for-articles-use-javapro-magazine-collection"}},"content":"An Oracle Application Server (OracleAS) Provider for [Jive Forums](http://www.jivesoftware.com/products/forums/) is now available to download from the [Portal Integration Solutions](http://www.oracle.com/technology/products/ias/portal/point_downloads.html#op4j) page on OTN.\\n\\nThe Oracle Application Server (OracleAS) Provider for Jive Forums contains portlets that allow users and administrators to:\\n\\n* View topics posted on their favorite forums\\n* Create a new topic or post a reply to one\\n* Search the forums\\n* View the hot topics across all forums\\n* View all topics, forums, categories and users being watched by the user\\n* View top reward points earners\\n* Administer the Jive Forums Application"},{"id":"/2004/10/06/looking-for-articles-use-javapro-magazine-collection","metadata":{"permalink":"/blog/2004/10/06/looking-for-articles-use-javapro-magazine-collection","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-06-looking-for-articles-use-javapro-magazine-collection.md","source":"@site/blog/2004-10-06-looking-for-articles-use-javapro-magazine-collection.md","title":"Looking for articles: use JavaPro magazine collection","description":"You are looking for a articles about J2EE, JavaPro magazine put online a nice tool to search and view all the published articles:","date":"2004-10-06T00:00:00.000Z","formattedDate":"October 6, 2004","tags":[],"readingTime":0.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Looking for articles: use JavaPro magazine collection"},"prevItem":{"title":"Jive portlets for OracleAS Portal","permalink":"/blog/2004/10/06/jive-portlets-for-oracleas-portal"},"nextItem":{"title":"Groovy, Java\'s New Scripting Language","permalink":"/blog/2004/10/04/groovy"}},"content":"You are looking for a articles about J2EE, [JavaPro magazine](http://www.ftponline.com/javapro/) put online a nice tool to search and view all the published articles:\\n\\n* [J2EE](http://www.ftponline.com/resources/spcollections/j2ee/)\\n* [Web Services](http://www.ftponline.com/resources/spcollections/webservices/)\\n* ..."},{"id":"/2004/10/04/groovy","metadata":{"permalink":"/blog/2004/10/04/groovy","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-04-groovy.md","source":"@site/blog/2004-10-04-groovy.md","title":"Groovy, Java\'s New Scripting Language","description":"Groovy, Java\'s New Scripting Language by Ian F. Darwin -- When some Java developers hear about Groovy, their first reaction is often, \\"Oh, no, not another scripting language for Java.\\" Ian Darwin had the same reaction, until he took a good look at Groovy.","date":"2004-10-04T00:00:00.000Z","formattedDate":"October 4, 2004","tags":[],"readingTime":0.22,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Groovy, Java\'s New Scripting Language"},"prevItem":{"title":"Looking for articles: use JavaPro magazine collection","permalink":"/blog/2004/10/06/looking-for-articles-use-javapro-magazine-collection"},"nextItem":{"title":"Good taste tool: del.icio.us","permalink":"/blog/2004/10/03/good-taste-tool-del-dot-icio-dot-us"}},"content":"[Groovy, Java\'s New Scripting Language](http://www.onjava.com/pub/a/onjava/2004/09/29/groovy.html) by Ian F. Darwin -- When some Java developers hear about Groovy, their first reaction is often, \\"Oh, no, not another scripting language for Java.\\" Ian Darwin had the same reaction, until he took a good look at Groovy."},{"id":"/2004/10/03/good-taste-tool-del-dot-icio-dot-us","metadata":{"permalink":"/blog/2004/10/03/good-taste-tool-del-dot-icio-dot-us","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-03-good-taste-tool-del-dot-icio-dot-us.md","source":"@site/blog/2004-10-03-good-taste-tool-del-dot-icio-dot-us.md","title":"Good taste tool: del.icio.us","description":"I am starting to use del.icio.us. A great way to manage your own bookmarks but also to share them with others...","date":"2004-10-03T00:00:00.000Z","formattedDate":"October 3, 2004","tags":[],"readingTime":0.42,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Good taste tool: del.icio.us"},"prevItem":{"title":"Groovy, Java\'s New Scripting Language","permalink":"/blog/2004/10/04/groovy"},"nextItem":{"title":"Sample Code: BLOB insertion in Oracle10g","permalink":"/blog/2004/10/01/sample-code-blob-insertion-in-oracle10g"}},"content":"I am starting to use [del.icio.us](http://del.icio.us/). A great way to manage your own bookmarks but also to share them with others...\\n\\nIt is fun to see how many people are sharing the same bookmarks as you, and help you to find information related to the different categories...\\n\\nAlso the integration with Firefox using the [delicious plugin](http://delicious.mozdev.org/) make the usage of the bookmark manager very easy. You can also use the [Apple Mac OS X client for del.icio.us](http://www.scifihifi.com/cocoalicious/).\\n\\n[Here my bookmarks... still working on it](http://del.icio.us/tgrall)."},{"id":"/2004/10/01/sample-code-blob-insertion-in-oracle10g","metadata":{"permalink":"/blog/2004/10/01/sample-code-blob-insertion-in-oracle10g","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-10-01-sample-code-blob-insertion-in-oracle10g.md","source":"@site/blog/2004-10-01-sample-code-blob-insertion-in-oracle10g.md","title":"Sample Code: BLOB insertion in Oracle10g","description":"Responding to a customer question about Blob insertion in Oracle 10g DB using the JDBC 3.0 driver. Very Simple application. Download Java Source See insertBlob method.","date":"2004-10-01T00:00:00.000Z","formattedDate":"October 1, 2004","tags":[],"readingTime":2.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Sample Code: BLOB insertion in Oracle10g","categories":["java","jdbc","oracle"]},"prevItem":{"title":"Good taste tool: del.icio.us","permalink":"/blog/2004/10/03/good-taste-tool-del-dot-icio-dot-us"},"nextItem":{"title":"Struts or JSF by Craig McClanahan","permalink":"/blog/2004/09/29/struts-or-jsf-by-craig-mcclanahan"}},"content":"Responding to a customer question about Blob insertion in Oracle 10g DB using the JDBC 3.0 driver. Very Simple application. [Download Java Source](http://www.grallandco.com/blog/archives/BlobOracle.java) See insertBlob method.\\n\\n``` java\\npackage demo;\\n\\nimport java.sql.*;\\nimport java.io.*;\\nimport java.sql.PreparedStatement;\\nimport java.util.*;\\n\\nimport oracle.jdbc.driver.*;\\nimport oracle.sql.BLOB;\\n\\n\\n\\n/**\\n* Insert record in the MEDIA table\\n*   MEDIA (file_name varchar2(256), file_content BLOB);\\n*/\\npublic class BlobOracle\\n{\\n  private final static String hostname = \\"localhost\\";\\n  private final static String port = \\"1521\\";\\n  private final static String sid = \\"ORCL\\";\\n  private final static String username = \\"scott\\";\\n  private final static String password = \\"tiger\\";\\n  private static String fileLocation;\\n  private static Connection connection;\\n\\n  public BlobOracle()\\n  {\\n  }\\n\\n  /**\\n  *\\n  * @param args\\n  */\\n  public static void main(String[] args)\\n  {\\n    try\\n    {\\n      if (args.length == 0)\\n      {\\n        System.out.println(\\"\\\\n\\\\n  Usage demo.BlobOracle \\");\\n        System.exit(0);\\n      }\\n\\n      fileLocation = args[0];\\n\\n      setConnection();\\n      insertBLOB();\\n\\n    } catch (Exception ex)\\n    {\\n      ex.printStackTrace();\\n      } finally\\n      {\\n      }\\n    }\\n\\n\\n    private static void setConnection() throws SQLException\\n    {\\n      DriverManager.registerDriver(new oracle.jdbc.driver.OracleDriver());\\n      connection = DriverManager.getConnection(\\"jdbc:oracle:thin:@\\"+hostname+ \\":\\"+ port +\\":\\"+ sid , username , password);\\n      connection.setAutoCommit(false); // we must control the commit\\n    }\\n\\n    private static void insertBLOB() throws SQLException, Exception\\n    {\\n      BLOB blob;\\n      File file ;\\n      FileInputStream is;\\n      OutputStream os;\\n\\n      long ts1 = System.currentTimeMillis();\\n\\n\\n      //Create a statement.\\n      PreparedStatement pstmt = connection.prepareStatement(\\"insert into media (file_name, file_content) values (?,empty_blob())\\");\\n      file = new File(fileLocation);\\n      String fileName = file.getName();\\n      //Set the file name and execute the query\\n      pstmt.setString(1, fileName);\\n      pstmt.execute();\\n\\n      //Take back the record for update (we will insert the blob)\\n      //supposely the file name is the PK\\n      pstmt = connection.prepareStatement(\\"select file_content from media where file_name = ? for update\\");\\n      pstmt.setString(1, fileName);\\n\\n      //Execute the query, and we must have one record so take it\\n      ResultSet rset = pstmt.executeQuery();\\n      rset.next();\\n\\n      //Use the OracleDriver resultset, we take the blob locator\\n      blob = ((OracleResultSet)rset).getBLOB(\\"file_content\\");\\n\\n      is = new FileInputStream(file); //Create a stream from the file\\n      // JDBC 2.0\\n      //os = blob.getBinaryOutputStream(); //get the output stream from the Blob to insert it\\n      // JDBC 3.0\\n      os = blob.setBinaryStream(0); //get the output stream from the Blob to insert it\\n\\n      //Read the file by chuncks and insert them in the Blob. The chunk size come from the blob\\n      byte[] chunk = new byte[blob.getChunkSize()];\\n      int i=-1;\\n      System.out.println(\\"Inserting the Blob\\");\\n      while((i = is.read(chunk))!=-1)\\n      {\\n        os.write(chunk,0,i); //Write the chunk\\n        System.out.print(\'.\'); // print progression\\n      }\\n\\n      // When done close the streams\\n      is.close();\\n      os.close();\\n\\n      //Close the statement and commit\\n      pstmt.close();\\n      long ts2 = System.currentTimeMillis();\\n\\n      connection.commit();\\n      connection.close();\\n\\n      System.out.println(\\"\\\\n\\"+ (ts2 - ts1) +\\" ms\\" );\\n\\n\\n    }\\n\\n\\n  }\\n```"},{"id":"/2004/09/29/struts-or-jsf-by-craig-mcclanahan","metadata":{"permalink":"/blog/2004/09/29/struts-or-jsf-by-craig-mcclanahan","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-09-29-struts-or-jsf-by-craig-mcclanahan.md","source":"@site/blog/2004-09-29-struts-or-jsf-by-craig-mcclanahan.md","title":"Struts or JSF by Craig McClanahan","description":"Craig McClanahan has started a blog... and his second post -after a quick introduction- is about the \'hot\' subject of the usage of Struts or JSF.","date":"2004-09-29T00:00:00.000Z","formattedDate":"September 29, 2004","tags":[],"readingTime":0.605,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Struts or JSF by Craig McClanahan"},"prevItem":{"title":"Sample Code: BLOB insertion in Oracle10g","permalink":"/blog/2004/10/01/sample-code-blob-insertion-in-oracle10g"},"nextItem":{"title":"Persistence reconciliation for POJO....","permalink":"/blog/2004/09/27/persistence-reconciliation-for-pojo-dot-dot-dot"}},"content":"Craig McClanahan has started a blog... and his second post -after a quick introduction- is about the \'hot\' subject of the usage of [Struts or JSF](http://blogs.sun.com/roller/page/craigmcc/20040927#struts_or_jsf_struts_and).\\n\\nQuote:\\n>For new development, here\'s the best strategy for determining what to do:\\n\\n> * Evaluate the two technologies individually, to see if they satisfy your requirements.\\n> * If one or the other technology is sufficient, go ahead and use it (it\'s easier to learn and use one technology rather than two where possible); keeping in mind, however, the caveats about Struts HTML tags mentioned above.\\n> * If your requirements include unique features supported only by Struts (such as Tiles or client side validation support), feel free to use the two frameworks together."},{"id":"/2004/09/27/persistence-reconciliation-for-pojo-dot-dot-dot","metadata":{"permalink":"/blog/2004/09/27/persistence-reconciliation-for-pojo-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-09-27-persistence-reconciliation-for-pojo-dot-dot-dot.md","source":"@site/blog/2004-09-27-persistence-reconciliation-for-pojo-dot-dot-dot.md","title":"Persistence reconciliation for POJO....","description":"Sun has sent a letter to the Java Community to start a single POJO persistence model, this will be done under the EJB 3.0 specification (JSR-220).","date":"2004-09-27T00:00:00.000Z","formattedDate":"September 27, 2004","tags":[],"readingTime":0.185,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Persistence reconciliation for POJO...."},"prevItem":{"title":"Struts or JSF by Craig McClanahan","permalink":"/blog/2004/09/29/struts-or-jsf-by-craig-mcclanahan"},"nextItem":{"title":"Oracle Application Server 10g Sets World Record SPECjAppServer 2002 Performance Result on Fujitsu PRIMEPOWER Servers","permalink":"/blog/2004/09/14/oracle-application-server-10g-sets-world-record-specjappserver-2002-performance-result-on-fujitsu-primepower-servers"}},"content":"Sun has sent a [letter to the Java Community](http://java.sun.com/j2ee/letter/persistence.html) to start a single POJO persistence model, this will be done under the EJB 3.0 specification ([JSR-220](http://jcp.org/en/jsr/detail?id=220)).\\n\\nYou can see comments from the community on [TheServerSide Web site](http://www.theserverside.com/news/thread.tss?thread_id=28995)."},{"id":"/2004/09/14/oracle-application-server-10g-sets-world-record-specjappserver-2002-performance-result-on-fujitsu-primepower-servers","metadata":{"permalink":"/blog/2004/09/14/oracle-application-server-10g-sets-world-record-specjappserver-2002-performance-result-on-fujitsu-primepower-servers","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-09-14-oracle-application-server-10g-sets-world-record-specjappserver-2002-performance-result-on-fujitsu-primepower-servers.md","source":"@site/blog/2004-09-14-oracle-application-server-10g-sets-world-record-specjappserver-2002-performance-result-on-fujitsu-primepower-servers.md","title":"Oracle Application Server 10g Sets World Record SPECjAppServer 2002 Performance Result on Fujitsu PRIMEPOWER Servers","description":"Oracle and Fujistu just announced new SpecJ2002 results, establishing a new record of 5,991.73 TOPS@MultipleNode (total operations per second) with a price-performance of 654.20 Euros/TOPS@MultipleNode. This was realized using Oracle Application Server 10g running on Fujitsu servers.","date":"2004-09-14T00:00:00.000Z","formattedDate":"September 14, 2004","tags":[],"readingTime":0.25,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle Application Server 10g Sets World Record SPECjAppServer 2002 Performance Result on Fujitsu PRIMEPOWER Servers"},"prevItem":{"title":"Persistence reconciliation for POJO....","permalink":"/blog/2004/09/27/persistence-reconciliation-for-pojo-dot-dot-dot"},"nextItem":{"title":"O/R Mapping Benchmark: new Oracle Toplink results","permalink":"/blog/2004/09/07/o-slash-r-mapping-benchmark-new-oracle-toplink-results"}},"content":"Oracle and Fujistu just announced new SpecJ2002 results, establishing a new record of 5,991.73 TOPS@MultipleNode (total operations per second) with a price-performance of 654.20 Euros/TOPS@MultipleNode. This was realized using [Oracle Application Server 10*g*](http://www.oracle.com/technology/products/ias/index.html) running on [Fujitsu servers](http://fujitsu.com).\\n\\nThe results are available on the [Spec Web site](http://www.specbench.org/jAppServer2002/results/jAppServer2002.html#MultipleNode), the Press Release [here](http://www.prnewswire.com/news/index_mail.shtml?ACCT=104&STORY=/www/story/09-14-2004/0002249793&EDATE=)."},{"id":"/2004/09/07/o-slash-r-mapping-benchmark-new-oracle-toplink-results","metadata":{"permalink":"/blog/2004/09/07/o-slash-r-mapping-benchmark-new-oracle-toplink-results","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-09-07-o-slash-r-mapping-benchmark-new-oracle-toplink-results.md","source":"@site/blog/2004-09-07-o-slash-r-mapping-benchmark-new-oracle-toplink-results.md","title":"O/R Mapping Benchmark: new Oracle Toplink results","description":"The Middleware company has created an initiative to test the different Object-Relational mapping tools, this test is named Torpedo. You can find the announcement of this O-R benchmark initiative on TSS Web site.","date":"2004-09-07T00:00:00.000Z","formattedDate":"September 7, 2004","tags":[],"readingTime":0.23,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"O/R Mapping Benchmark: new Oracle Toplink results"},"prevItem":{"title":"Oracle Application Server 10g Sets World Record SPECjAppServer 2002 Performance Result on Fujitsu PRIMEPOWER Servers","permalink":"/blog/2004/09/14/oracle-application-server-10g-sets-world-record-specjappserver-2002-performance-result-on-fujitsu-primepower-servers"},"nextItem":{"title":"Portlet Development Book and Web Site","permalink":"/blog/2004/08/27/portlet-development-book-and-web-site"}},"content":"The Middleware company has created an initiative to test the different Object-Relational mapping tools, this test is named [Torpedo](http://www.middlewareresearch.com/torpedo/). You can find the announcement of this O-R benchmark initiative on [TSS Web site](http://www.theserverside.com/news/thread.tss?thread_id=27761).\\n\\nOracle Toplink just posted news results that you can read and comment [here](http://theserverside.com/news/thread.tss?thread_id=28518)."},{"id":"/2004/08/27/portlet-development-book-and-web-site","metadata":{"permalink":"/blog/2004/08/27/portlet-development-book-and-web-site","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-08-27-portlet-development-book-and-web-site.md","source":"@site/blog/2004-08-27-portlet-development-book-and-web-site.md","title":"Portlet Development Book and Web Site","description":"Portalbook.com is the site of Jeff Linwood and Dave Minter, authors of the book Building Portals with the Java Portlet API.","date":"2004-08-27T00:00:00.000Z","formattedDate":"August 27, 2004","tags":[],"readingTime":0.285,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Portlet Development Book and Web Site"},"prevItem":{"title":"O/R Mapping Benchmark: new Oracle Toplink results","permalink":"/blog/2004/09/07/o-slash-r-mapping-benchmark-new-oracle-toplink-results"},"nextItem":{"title":"Have we lost the Groove ?","permalink":"/blog/2004/08/22/have-we-lost-the-groove"}},"content":"[Portalbook.com](http://portalbook.com/) is the site of Jeff Linwood and Dave Minter, authors of the book *[Building Portals with the Java Portlet API](http://apress.com/book/bookDisplay.html?bID=362)*.\\n\\nIf you are interested in Portlet development, bookmark this site and buy the [aPress book](http://apress.com/book/bookDisplay.html?bID=362) !\\n\\nI did not have a chance to read the book yet, so feel free to give your feedback as comment."},{"id":"/2004/08/22/have-we-lost-the-groove","metadata":{"permalink":"/blog/2004/08/22/have-we-lost-the-groove","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-08-22-have-we-lost-the-groove.md","source":"@site/blog/2004-08-22-have-we-lost-the-groove.md","title":"Have we lost the Groove ?","description":"Like lot of Java developers I was very excited about the announcement of the creation of the JSR-241... But where are we now ?","date":"2004-08-22T00:00:00.000Z","formattedDate":"August 22, 2004","tags":[],"readingTime":0.62,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Have we lost the Groove ?"},"prevItem":{"title":"Portlet Development Book and Web Site","permalink":"/blog/2004/08/27/portlet-development-book-and-web-site"},"nextItem":{"title":"Oracle releases early version of JavaServer Faces Components","permalink":"/blog/2004/08/17/oracle-releases-early-version-of-javaserver-faces-components"}},"content":"Like lot of Java developers I was very excited about the announcement of the creation of the JSR-241... But where are we now ?\\n\\nThe JSR mailing list ([http://news.gmane.org/gmane.comp.lang.groovy.jsr](http://news.gmane.org/gmane.comp.lang.groovy.jsr) ) is very quiet... No meeting, not a lot of question/response. And like Patrick M. I have friends that have asked to be part of the Expert Group and never had any response.\\n\\nI think that the Groovy Language is really a nice way of putting Java to the next level, easier to learn, to use and still powerful... I will not go in an enumeration of the benefits of Groovy here... but just asking what the Expert Group is doing ? What is the next step for the Groovy Language and the JCP ?"},{"id":"/2004/08/17/oracle-releases-early-version-of-javaserver-faces-components","metadata":{"permalink":"/blog/2004/08/17/oracle-releases-early-version-of-javaserver-faces-components","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-08-17-oracle-releases-early-version-of-javaserver-faces-components.md","source":"@site/blog/2004-08-17-oracle-releases-early-version-of-javaserver-faces-components.md","title":"Oracle releases early version of JavaServer Faces Components","description":"Oracle has been involved in the JavaServer Faces (JSR 127) since its creation, and released yesterday a set of components. The list of the available component is available here.","date":"2004-08-17T00:00:00.000Z","formattedDate":"August 17, 2004","tags":[],"readingTime":0.185,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Oracle releases early version of JavaServer Faces Components"},"prevItem":{"title":"Have we lost the Groove ?","permalink":"/blog/2004/08/22/have-we-lost-the-groove"},"nextItem":{"title":"Weather syndication","permalink":"/blog/2004/07/29/weather-syndication"}},"content":"Oracle has been involved in the JavaServer Faces (JSR 127) since its creation, and released yesterday a set of components. The list of the available component is available [here](http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/jsf/doc/tagdoc/core/index.html).\\n\\n[The software is available for download on OTN](http://www.oracle.com/technology/products/jdev/htdocs/partners/addins/exchange/jsf/index.html)."},{"id":"/2004/07/29/weather-syndication","metadata":{"permalink":"/blog/2004/07/29/weather-syndication","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-07-29-weather-syndication.md","source":"@site/blog/2004-07-29-weather-syndication.md","title":"Weather syndication","description":"Lot of us want to create portlets (or publish content) based on weather information. The best solution that I found is the Weather Channel XML.","date":"2004-07-29T00:00:00.000Z","formattedDate":"July 29, 2004","tags":[],"readingTime":0.23,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Weather syndication"},"prevItem":{"title":"Oracle releases early version of JavaServer Faces Components","permalink":"/blog/2004/08/17/oracle-releases-early-version-of-javaserver-faces-components"},"nextItem":{"title":"AspectJ Tool for JDeveloper 10g","permalink":"/blog/2004/07/14/aspectj-tool-for-jdeveloper-10g"}},"content":"Lot of us want to create portlets (or publish content) based on weather information. The best solution that I found is the [Weather Channel XML](http://www.weather.com/services/xmloap.html).\\n\\nAnd I was really impressed by [xaopWeather](http://www.xoapweather.com) (PHP based)!\\nLet me know if you find an equivalent but Java based !"},{"id":"/2004/07/14/aspectj-tool-for-jdeveloper-10g","metadata":{"permalink":"/blog/2004/07/14/aspectj-tool-for-jdeveloper-10g","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-07-14-aspectj-tool-for-jdeveloper-10g.md","source":"@site/blog/2004-07-14-aspectj-tool-for-jdeveloper-10g.md","title":"AspectJ Tool for JDeveloper 10g","description":"Gerard Davidson has created a java.net project to integrate AOP Framework to JDeveloper 10g, starting with an extension for AspectJ!","date":"2004-07-14T00:00:00.000Z","formattedDate":"July 14, 2004","tags":[],"readingTime":0.1,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"AspectJ Tool for JDeveloper 10g"},"prevItem":{"title":"Weather syndication","permalink":"/blog/2004/07/29/weather-syndication"},"nextItem":{"title":"Struts 1.2.1 Beta available /  JDeveloper 10g How-To","permalink":"/blog/2004/07/12/struts-1-dot-2-1-beta-available-slash-jdeveloper-10g-how-to"}},"content":"Gerard Davidson has created a java.net project to integrate AOP Framework to [JDeveloper 10*g*](http://jdeveloperaop.dev.java.net/), starting with an extension for [AspectJ](http://eclipse.org/aspectj/)!"},{"id":"/2004/07/12/struts-1-dot-2-1-beta-available-slash-jdeveloper-10g-how-to","metadata":{"permalink":"/blog/2004/07/12/struts-1-dot-2-1-beta-available-slash-jdeveloper-10g-how-to","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-07-12-struts-1-dot-2-1-beta-available-slash-jdeveloper-10g-how-to.md","source":"@site/blog/2004-07-12-struts-1-dot-2-1-beta-available-slash-jdeveloper-10g-how-to.md","title":"Struts 1.2.1 Beta available /  JDeveloper 10g How-To","description":"Struts 1.2.1 Beta is now available for download.","date":"2004-07-12T00:00:00.000Z","formattedDate":"July 12, 2004","tags":[],"readingTime":0.125,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Struts 1.2.1 Beta available /  JDeveloper 10g How-To"},"prevItem":{"title":"AspectJ Tool for JDeveloper 10g","permalink":"/blog/2004/07/14/aspectj-tool-for-jdeveloper-10g"},"nextItem":{"title":"Which Version I am running ?","permalink":"/blog/2004/07/12/which-version-i-am-running"}},"content":"[Struts 1.2.1 Beta](http://struts.apache.org/announce.html) is now available for download.\\n\\nDuncan Mills published on OTN an article that explains how-to use Struts 1.2.1 with [Oracle JDeveloper 10*g*](http://otn.oracle.com/products/jdev/tips/mills/struts1_2_1.html)."},{"id":"/2004/07/12/which-version-i-am-running","metadata":{"permalink":"/blog/2004/07/12/which-version-i-am-running","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-07-12-which-version-i-am-running.md","source":"@site/blog/2004-07-12-which-version-i-am-running.md","title":"Which Version I am running ?","description":"Today I was helping a customer on a deployment issue...","date":"2004-07-12T00:00:00.000Z","formattedDate":"July 12, 2004","tags":[],"readingTime":1.2,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Which Version I am running ?"},"prevItem":{"title":"Struts 1.2.1 Beta available /  JDeveloper 10g How-To","permalink":"/blog/2004/07/12/struts-1-dot-2-1-beta-available-slash-jdeveloper-10g-how-to"},"nextItem":{"title":"Opening the Black Box of Integration","permalink":"/blog/2004/07/09/opening-the-black-box-of-integration"}},"content":"Today I was helping a customer on a deployment issue...\\n\\n\\nThe first question that I have asked is:\\n\\n\\"*Which version of OC4J are you using ?*\\".\\n\\nAnd he has no idea !\\n\\n\\nSo let me give you some tips to know the version of Oracle components that you are using...\\n\\n\x3c!-- truncate --\x3e\\n\\n====\\nOC4J\\n\\nIn the j2ee/home directory:\\n\\n>`java -jar oc4j.jar -version`\\n\\n====\\nJPDK\\n\\nGo in the provider test page\\n\\n>`http://server:port/context/providers/your-provider`\\n\\nYou can see the version of the in the bottom of the page. In the JPDK you can also use the `HttpCommonConstants.COMPONENT_VERSIONS` to access it programmatically.\\n\\n\\nAlso if you  are looking for a version of a specific component, Oracle development often use a \'version file\' into the jar file of this component.This is the case for example for:\\n\\n\\n* Portals: ptlshare.jar, wsrp-container.jar, wsrp-container.jar, pdkjava.jar, portaltools.jar\\n* Oracle XML: xmlparserv2.jar,oraclexsql.jar, xsqlserializers.jar\\n* UIX: uix2.jar, jewt4.jar\\n* BI Bean: bigraphbean.jar\\n\\n*Update following Markus comments:*\\n\\nMarkus made me realized that I forgot to mention the standard way of checking the version of a product. It is also possible, and done by most of the Oracle components, to put the version number into the Manifest file of any archive. So take a look to the manifest, and if it is not available you can look for a version file into the archive.\\n\\n\\nSo next time somebody ask you which version of the product you are using you will be all set !"},{"id":"/2004/07/09/opening-the-black-box-of-integration","metadata":{"permalink":"/blog/2004/07/09/opening-the-black-box-of-integration","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-07-09-opening-the-black-box-of-integration.md","source":"@site/blog/2004-07-09-opening-the-black-box-of-integration.md","title":"Opening the Black Box of Integration","description":"Web Service Journal just publish an article \'Opening the Black Box of Integration\' written by Mike Lehmann OracleAS 10g Product Manager.","date":"2004-07-09T00:00:00.000Z","formattedDate":"July 9, 2004","tags":[],"readingTime":0.44,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Opening the Black Box of Integration"},"prevItem":{"title":"Which Version I am running ?","permalink":"/blog/2004/07/12/which-version-i-am-running"},"nextItem":{"title":"Shirky.com: In Praise of Evolvable Systems","permalink":"/blog/2004/04/08/shirky-dot-com-in-praise-of-evolvable-systems"}},"content":"Web Service Journal just publish [an article \'Opening the Black Box of Integration\'](http://sys-con.com/story/?storyid=45525&DE=1_) written by [Mike Lehmann](http://radio.weblogs.com/0132036/) OracleAS 10*g* Product Manager.\\n\\nSummary:\\n >If you\'ve been working with integration technologies for any length of time, you\'re well aware of the freight train of standards that has been careening through the industry during the last five years. These standards, particularly in the Web services space, are on the verge of doing to proprietary integration servers what SQL and J2EE standards did to database and middle-tier servers of days gone by."},{"id":"/2004/04/08/shirky-dot-com-in-praise-of-evolvable-systems","metadata":{"permalink":"/blog/2004/04/08/shirky-dot-com-in-praise-of-evolvable-systems","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-04-08-shirky-dot-com-in-praise-of-evolvable-systems.md","source":"@site/blog/2004-04-08-shirky-dot-com-in-praise-of-evolvable-systems.md","title":"Shirky.com: In Praise of Evolvable Systems","description":"Rob,colleague of mine, pointed me to an article about \\"Evolvable Systems\\":","date":"2004-04-08T00:00:00.000Z","formattedDate":"April 8, 2004","tags":[],"readingTime":0.36,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Shirky.com: In Praise of Evolvable Systems"},"prevItem":{"title":"Opening the Black Box of Integration","permalink":"/blog/2004/07/09/opening-the-black-box-of-integration"},"nextItem":{"title":"JavaOne and Apple WWDC 2004","permalink":"/blog/2004/04/07/javaone-and-apple-wwdc-2004"}},"content":"Rob,colleague of mine, pointed me to an article about \\"[Evolvable Systems](http://www.shirky.com/writings/evolve.html)\\":\\n\\n*Why something as poorly designed as the Web became The Next Big Thing, and what that means for the future*.\\n\\nFirst of all this article is very interesting, but the whole site -that I did not know- is full of truth, and fun ! ( I love the one about blogs)\\n\\nI invite you to bookmark [http://www.shirky.com](http://www.shirky.com) !\\n\\nThanks again Rob!"},{"id":"/2004/04/07/javaone-and-apple-wwdc-2004","metadata":{"permalink":"/blog/2004/04/07/javaone-and-apple-wwdc-2004","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-04-07-javaone-and-apple-wwdc-2004.md","source":"@site/blog/2004-04-07-javaone-and-apple-wwdc-2004.md","title":"JavaOne and Apple WWDC 2004","description":"I have been working on the JavaOne Oracle Demo Ground where I was demonstrating OracleAS Portal 10g. I also attend several sessions (EJB 3.0 & CMP Persistence, J2EE 1.4 Web Services, AOP) and Thomas Kurian and James Gosling keynotes.","date":"2004-04-07T00:00:00.000Z","formattedDate":"April 7, 2004","tags":[{"label":"java","permalink":"/blog/tags/java"},{"label":"javaEE","permalink":"/blog/tags/java-ee"}],"readingTime":3.935,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"JavaOne and Apple WWDC 2004","tags":["java","javaEE"]},"prevItem":{"title":"Shirky.com: In Praise of Evolvable Systems","permalink":"/blog/2004/04/08/shirky-dot-com-in-praise-of-evolvable-systems"},"nextItem":{"title":"Do not remember the default password... here a list...","permalink":"/blog/2004/01/26/do-not-remember-the-default-password-dot-dot-dot-here-a-list-dot-dot-dot"}},"content":"I have been working on the JavaOne Oracle Demo Ground where I was demonstrating OracleAS Portal 10*g*. I also attend several sessions (EJB 3.0 & CMP Persistence, J2EE 1.4 Web Services, AOP) and Thomas Kurian and James Gosling keynotes.\\n\\nAlso Tuesday afternoon I have been on duty on the Oracle Pod in the Apple developer conference, where I was demonstrating Oracle RDMS and JDeveloper 10g.  \\n\\n*JavaOne*\\n\\nOracle had a very nice and big demo ground, with the best location: in the front of the main entrance. The main theme was Racing Cars. Oracle organized racing contest using Sony Play Station with Gran Turismo 3.\\n\\nThe 10 stations demonstrated the same application that recall the main theme since user can create easily a new racing team. To summarize, we have developed   a J2EE 1.3/1.4 application using ADF and POJO (Toplink) but also we integrated  JSF for the view, Web Service call and integration with business process using the new [Oracle BPEL Process Manager](http://otn.oracle.com/bpel). (Oracle announced the acquisition of Collaxa).\\n\\n\\nOn the Portal station, the demo showed how easy it is to integrate existing MVC application(Struts, ADF, ...) using the Oracle JPDK. Portal teal also built a nice demonstration that shows the same portlet -business logic and view- developed using proprietary APIs (JPDK) and standard based implementation (WSRP & JSR-168). Lot of people stop by the portal pod, and I was pleased with the quality of the questions:\\n\\n\\n* Portlet Standards: Oracle provides a JSR-168/WSRP container but also a [Developer Preview of the OracleAS 10g Portal](http://otn.oracle.com/software/products/ias/preview.html) that allows you to register and executes WSRP based portlet.\\n* Struts, ADF, ad JSF integration: The [Oracle PDK provides a Struts integration](http://portalstudio.oracle.com/pls/ops/docs/FOLDER/COMMUNITY/PDK/ARTICLES/pdkstruts/portal-struts-wp.html) based on extension of the Struts Tags and has a specific Portlet Renderer.\\n\\n* Web Services & Portal: OmniPortlet and Coded Portlets allow developer to easily integrate Web Services into Portal.. and for example it is easy to create a portlet that will kick-off a Business Process that is running inside the Oracle BPEL Process Manager.\\n\\nLinda DiMichel\'s session about EJB 3.0 was a big one, the room was packed. EJB is taking the good path. Simplification of the objects no need to multiply the artifacts and deployment descriptors: EJB 3.0 will use Java annotations. 3.0 will use POJO -Plain Old Java Objects- and allow native SQL. This is just a very short summary of the new spec that looks very very good ! (and similar to what Toplink is doing for years)\\n\\nAOP was also on of the think I wanted to see during this event. I was really pleased by the AOP Panel of Wednesday afternoon. James Gosling, Graham Hamilton, Cedric Beust and Gregor Kiczales presented and discussed AOP in general and AOP in J2EE/Java in particular. My interpretation of the whole discussion is:\\n\\nEverybody thinks that AOP is interesting, and will make code cleaner; but we have a lot to do to simplify the development. Like Cedric and James said, I am \\"conflicted\\" about AOP, and I think the reason is because we see the benefits of AOP in simple case such as logging... But harder to implement cleanly when the aspects are complex such as authentication/authorization or persistence. The Java community still need to work on this to be sure that developers will develop better code; AOP will give it, just have to find the good way ! I have also watched Alex Vasseur and Jonas Boner\'s session about AOP and J2EE, mainly focus on [AspectWerkz](http://aspectwerkz.codehaus.org/). I really liked the paper with lot of demonstration that show simple and concrete example of AOP within J2EE container.\\n\\nLike lot of us I will continue to invest into AOP and experiment it within the J2EE world. James Strachan - JSR 241 Lead- session was also a very popular one. This session introduced the Groovy Programming Language that leverage the Java Language but using a concise and object oriented scripting language. I will not detail the content of groovy here, it will be better for you to take a quick look to the [Groovy Web Site](http://groovy.codehaus.org/) knowing that you can download the language and start to develop script with it.\\n\\n\\n*Apple WWDC 2004*\\n\\n\\nI was not able to attend any session... not event Steve Job\'s keynote. But I worked on the Oracle demo pod Tuesday afternoon.\\n\\nWe had 2 dual G5 with \'old\' cinema displays -hope that next year we will have the [30 inches display!](http://www.apple.com/displays/)-, these machines were used to run Oracle 10*g* database and JDeveloper, currently available to developer, you can download them from [OTN](http://otn.oracle.com/macos).\\n\\nBut the big news is the fact that these products will be production on Mac OS X soon:\\n\\n  * Oracle 10*g* server production in fall\\n\\n  * Oracle 10*g* JDeveloper production in September"},{"id":"/2004/01/26/do-not-remember-the-default-password-dot-dot-dot-here-a-list-dot-dot-dot","metadata":{"permalink":"/blog/2004/01/26/do-not-remember-the-default-password-dot-dot-dot-here-a-list-dot-dot-dot","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2004-01-26-do-not-remember-the-default-password-dot-dot-dot-here-a-list-dot-dot-dot.md","source":"@site/blog/2004-01-26-do-not-remember-the-default-password-dot-dot-dot-here-a-list-dot-dot-dot.md","title":"Do not remember the default password... here a list...","description":"I am sure that like me sometimes when you are testing product you do not know, or remember the default password.","date":"2004-01-26T00:00:00.000Z","formattedDate":"January 26, 2004","tags":[],"readingTime":0.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Do not remember the default password... here a list..."},"prevItem":{"title":"JavaOne and Apple WWDC 2004","permalink":"/blog/2004/04/07/javaone-and-apple-wwdc-2004"},"nextItem":{"title":"OmniPortlet tip: use stored procedure with the SQL data source","permalink":"/blog/2003/11/19/omniportlet-tip-use-stored-procedure-with-the-sql-data-source"}},"content":"I am sure that like me sometimes when you are testing product you do not know, or remember the default password.\\n\\nTake a look the the [CIRT Web Site](http://www.cirt.net/cgi-bin/passwd.pl), not necessary up-to-date, but better than nothing!"},{"id":"/2003/11/19/omniportlet-tip-use-stored-procedure-with-the-sql-data-source","metadata":{"permalink":"/blog/2003/11/19/omniportlet-tip-use-stored-procedure-with-the-sql-data-source","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2003-11-19-omniportlet-tip-use-stored-procedure-with-the-sql-data-source.md","source":"@site/blog/2003-11-19-omniportlet-tip-use-stored-procedure-with-the-sql-data-source.md","title":"OmniPortlet tip: use stored procedure with the SQL data source","description":"OmniPortlet provides to page designer an easy and powerful way to publish","date":"2003-11-19T00:00:00.000Z","formattedDate":"November 19, 2003","tags":[],"readingTime":1.195,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OmniPortlet tip: use stored procedure with the SQL data source"},"prevItem":{"title":"Do not remember the default password... here a list...","permalink":"/blog/2004/01/26/do-not-remember-the-default-password-dot-dot-dot-here-a-list-dot-dot-dot"},"nextItem":{"title":"How to configure Web Cache to cache remote portlet content","permalink":"/blog/2003/11/16/how-to-configure-web-cache-to-cache-remote-portlet-content"}},"content":"OmniPortlet provides to page designer an easy and powerful way to publish\\ncontent from different data sources. One of the data source is a SQL data source\\nthat allow you to connect to a relational database using JDBC. Obviously you can\\nenter any SQL statement, but you can also consume a REF CURSOT returned by a\\nprocedure.\\n\\nThat is really interesting if you want to add business logic to your data,or\\nhave to set some specific code before the execution of the query.\\n\\nTo to it you have to create a <b>procedure</b> that has the first parameter the\\nreturn a ref cursor:\\n`procedure get_employee_for_dept(p_ref_cursor out ref_cursor, p_dept in number);`\\n\\n\\nHere a complete package based on the SCOTT sample schema:\\n\\n\\n\\n```\\n create or replace package EMPLOYEE_API\\n as\\n -- create a ref cursor type that will be return to the consumer\\n type ref_cursor is ref cursor;\\n\\n -- return in the p_ref_cursor the list of employees for the department p_dept\\n\\n procedure get_employee_for_dept(p_ref_cursor out ref_cursor, p_dept in number);\\n end;\\n create or replace package body EMPLOYEE_API\\n as\\n -- return in the p_ref_cursor the list of employees for the department p_dept\\n procedure get_employee_for_dept(p_ref_cursor out ref_cursor, p_dept in number)\\n as\\n begin\\n  -- open the cusor based on the emp table\\n\\n  OPEN p_ref_cursor FOR\\n  SELECT * from emp WHERE deptno = p_dept;\\n end;\\n\\n end;\\n```\\n\\n\\nIn the Statement field of the OmniPortlet SQL data source you can now enter:\\n<span class=\\"code\\">\\ncall EMPLOYEE_API.get_employee_for_dept(\'10\')\\n\\n</span>\\n\\n<p>Enjoy OmniPortlet !</p>"},{"id":"/2003/11/16/how-to-configure-web-cache-to-cache-remote-portlet-content","metadata":{"permalink":"/blog/2003/11/16/how-to-configure-web-cache-to-cache-remote-portlet-content","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2003-11-16-how-to-configure-web-cache-to-cache-remote-portlet-content.md","source":"@site/blog/2003-11-16-how-to-configure-web-cache-to-cache-remote-portlet-content.md","title":"How to configure Web Cache to cache remote portlet content","description":"Some portlet developers have hard time to set up a correct environment to use","date":"2003-11-16T00:00:00.000Z","formattedDate":"November 16, 2003","tags":[],"readingTime":1.79,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"How to configure Web Cache to cache remote portlet content"},"prevItem":{"title":"OmniPortlet tip: use stored procedure with the SQL data source","permalink":"/blog/2003/11/19/omniportlet-tip-use-stored-procedure-with-the-sql-data-source"},"nextItem":{"title":"Add columns to OmniPortlet","permalink":"/blog/2003/11/03/add-columns-to-omniportlet"}},"content":"Some portlet developers have hard time to set up a correct environment to use\\ninvalidation based cache with portlets. The summary is often:<br/>\\n*\\"it is working with the PDK Example provided in Oracle 9iAS but every\\ntime that I want to do it myself from Oracle JDeveloper or a stand alone OC4J\\nthe portlet is not cached !\\"*\\n\\n\x3c!-- truncate --\x3e\\n\\nThis is most of the case due to a bad understanding of the different components\\nthat are involved.\\n\\nWhen we talk about invalidation based caching for Remote Portlet, we talk about\\na portlet that caches its content in a Web Cache that __is between the\\nPortal Midtier and the Web Provider Midtier__. This is a first important\\npoint. That also means that you have to register your remote Web provider using\\na URL that points to this Web Cache.\\n\\n![](http://www.grallandco.com/images/blog/webcache.png 461 287 PPE, Web Cache and Web Provider)\\n\\nSo when you are developing a portlet within Oracle JDeveloper or deploying\\na portlet to an OC4J or an AS and you want to use invalidation based caching\\nyou have to put a Web Cache between the PPE and the Application Server that\\nyou are using. Let\'s take an example based on a execution of the Web Provider\\ninside JDeveloper -embedded OC4J-, a URL like: `http://dev-machine:port/application/providers`.\\n\\nSo to cache the content of the portlets that are running in\\nthis OC4J you have to configure Web Cache using Web Cache Manager and:\\n\\n1. create an Application Web Server that is the embedded OC4J\\n1. create a new Site Definition, this will create a new HTTP port, that you\'ll\\nuse to register your provider.\\n1. finally you have to create a \\"Site to server mapping\\", where you\\nassociate the OC4J to the new HTTP port.\\n\\nAfter the bounce of Web Cache you can use the HTTP port on the Web Cache server\\nto access your provider, and if the portlet is correctly configured -[see this Oracle article](http://portalstudio.oracle.com/pls/ops/docs/FOLDER/COMMUNITY/PDK/JPDK/V2/DOC/WEBCACHE/INSTALLINGINVALIDATIONCACHESAMPLE.HTML) -to support invalidation based caching; its content\\nwill be automatically cached.\\n\\nNote: *this tip is not limited to Portlet but to any application that you want to develop and enhance using invalidation based caching.*"},{"id":"/2003/11/03/add-columns-to-omniportlet","metadata":{"permalink":"/blog/2003/11/03/add-columns-to-omniportlet","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2003-11-03-add-columns-to-omniportlet.md","source":"@site/blog/2003-11-03-add-columns-to-omniportlet.md","title":"Add columns to OmniPortlet","description":"I had some questions about the limitation to 5 columns of the","date":"2003-11-03T00:00:00.000Z","formattedDate":"November 3, 2003","tags":[],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Add columns to OmniPortlet"},"prevItem":{"title":"How to configure Web Cache to cache remote portlet content","permalink":"/blog/2003/11/16/how-to-configure-web-cache-to-cache-remote-portlet-content"},"nextItem":{"title":"Looking for Portlets ?","permalink":"/blog/2003/10/27/looking-for-portlets"}},"content":"I had some questions about the limitation to 5 columns of the\\nOmniPortlet tabular layout. Here is a tip to add more columns to this\\nlayout:\\n\\n1. backup the current OmniPortlet provider.xml\\n`/OC4J_HOME/applications/portalTools/omniPortlet/WEB-INF/providers/omniPortlet`\\n\\n2. You can now open the file, and look for the tag `<dataField`\\n\\n\\n3. You can add new fields, by copying the existing `<dataField>` tag and\\nchange the value of the `<name>` and `<displayName>`\\n\\n\x3c!-- Here is a complete example --\x3e\\n\\n``` xml\\n<dataField class=\\"oracle.webdb.reformlet.definition.DataFieldDefinition\\">\\n<name>Field6</name>\\n<displayName>Column6</displayName>\\n<description>Field6</description>\\n<text>##column##</text>\\n\\n<alignment>left</alignment>\\n<displayAs>hidden</displayAs>\\n<type>linebreak</type>\\n<font>Arial.3.Plain.None</font>\\n\\n<color>#000000</color>\\n<style>none</style>\\n<styleType>custom</styleType>\\n</dataField>\\n```\\n\\nThe same logic could be used to add parameters or events to the OmniPortlet."},{"id":"/2003/10/27/looking-for-portlets","metadata":{"permalink":"/blog/2003/10/27/looking-for-portlets","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2003-10-27-looking-for-portlets.md","source":"@site/blog/2003-10-27-looking-for-portlets.md","title":"Looking for Portlets ?","description":"You are implementing a Portal based on Oracle 9iAS Portal, do not reinvent the wheel !","date":"2003-10-27T00:00:00.000Z","formattedDate":"October 27, 2003","tags":[],"readingTime":0.245,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Looking for Portlets ?"},"prevItem":{"title":"Add columns to OmniPortlet","permalink":"/blog/2003/11/03/add-columns-to-omniportlet"},"nextItem":{"title":"Portlet with a vertical scroll bar","permalink":"/blog/2003/10/27/portlet-with-a-vertical-scroll-bar"}},"content":"You are implementing a Portal based on Oracle 9*i*AS Portal, do not reinvent the wheel !\\n\\nThe [Portal Center Knowledge Exchange](http://portalstudio.oracle.com/servlet/page?_pageid=3050&_dad=ops&amp;_schema=OPSTUDIO&_mode=3) is a great place to find portlets, articles, tips and technics from the Portlet Developer Community ! You just have to suscribe for free to the\\n[Developer Services](http://portalstudio.oracle.com/servlet/page?_pageid=2241&amp;_dad=ops&_schema=OPSTUDIO&amp;_mode=3)"},{"id":"/2003/10/27/portlet-with-a-vertical-scroll-bar","metadata":{"permalink":"/blog/2003/10/27/portlet-with-a-vertical-scroll-bar","editUrl":"https://github.dev/tgrall/tgrall.github.io/blob/main/blog/2003-10-27-portlet-with-a-vertical-scroll-bar.md","source":"@site/blog/2003-10-27-portlet-with-a-vertical-scroll-bar.md","title":"Portlet with a vertical scroll bar","description":"To have a better control of the layout of your Portal Pages, you can use a","date":"2003-10-27T00:00:00.000Z","formattedDate":"October 27, 2003","tags":[],"readingTime":0.415,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Portlet with a vertical scroll bar"},"prevItem":{"title":"Looking for Portlets ?","permalink":"/blog/2003/10/27/looking-for-portlets"}},"content":"To have a better control of the layout of your Portal Pages, you can use a\\nvertical scroll bar in portlet wihout having to use iFrame technology.\\nCSS provide us lof of power to do it.\\n\\n\\n1. Create a style in your portlet (or in the portlet container code):\\n\\n``` html\\n<style>\\n.scroll<i>portletId</i> {\\n  height:200px;\\n  overflow: auto;\\n}\\n</style>\\n```\\n\\n\\n2. Use this style in a `DIV` section:\\n\\n``` html\\n<div class=\\"scrollportletId\\">\\nThe content of this section will be vertically scrollable.\\n</div>\\n```"}]}')}}]);